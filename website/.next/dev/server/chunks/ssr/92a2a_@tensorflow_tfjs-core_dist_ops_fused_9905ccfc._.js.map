{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 4, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-core/dist/ops/fused/conv2d.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-core/src/ops/fused/conv2d.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../../engine';\nimport {customGrad} from '../../gradients';\nimport {FusedConv2D, FusedConv2DAttrs, FusedConv2DInputs} from '../../kernel_names';\nimport {NamedAttrMap} from '../../kernel_registry';\nimport {Tensor, Tensor3D, Tensor4D} from '../../tensor';\nimport {GradSaveFunc, NamedTensorMap} from '../../tensor_types';\nimport {makeTypesMatch} from '../../tensor_util';\nimport {convertToTensor} from '../../tensor_util_env';\nimport {TensorLike} from '../../types';\nimport * as util from '../../util';\nimport {add} from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport {conv2d as unfusedConv2d} from '../conv2d';\nimport {conv2DBackpropFilter} from '../conv2d_backprop_filter';\nimport {conv2DBackpropInput} from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport {Activation} from '../fused_types';\nimport {applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse} from '../fused_util';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\n\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedConv2d_<T extends Tensor3D|Tensor4D>({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha\n}: {\n  x: T|TensorLike,\n  filter: Tensor4D|TensorLike,\n  strides: [number, number]|number,\n  pad: 'valid'|'same'|number|conv_util.ExplicitPadding,\n  dataFormat?: 'NHWC'|'NCHW',\n  dilations?: [number, number]|number,\n  dimRoundingMode?: 'floor'|'round'|'ceil',\n  bias?: Tensor|TensorLike,\n  activation?: Activation,\n  preluActivationWeights?: Tensor,\n  leakyreluAlpha?: number\n}): T {\n  activation = activation || 'linear';\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    // TODO: Transpose bias and preluActivationWeights properly for NCHW\n    // format before computation.\n    util.assert(\n        dataFormat === 'NHWC',\n        () => `Error in fused conv2d: got dataFormat of ${dataFormat} but ` +\n            `only NHWC is currently supported for the case of gradient depth ` +\n            `is 0 and the activation is not linear.`);\n\n    let result = unfusedConv2d(\n        x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(\n               result, activation, preluActivationWeights, leakyreluAlpha) as T;\n  }\n\n  const $x = convertToTensor(x, 'x', 'conv2d', 'float32');\n  const $filter = convertToTensor(filter, 'filter', 'conv2d', 'float32');\n\n  let x4D = $x as Tensor4D;\n  let reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n  util.assert(\n      x4D.rank === 4,\n      () => `Error in fused conv2d: input must be rank 4, but got rank ` +\n          `${x4D.rank}.`);\n  util.assert(\n      $filter.rank === 4,\n      () => `Error in fused conv2d: filter must be rank 4, but got rank ` +\n          `${$filter.rank}.`);\n  conv_util.checkPadOnDimRoundingMode('fused conv2d', pad, dimRoundingMode);\n  const inputChannels = dataFormat === 'NHWC' ? x4D.shape[3] : x4D.shape[1];\n  util.assert(\n      $filter.shape[2] === inputChannels,\n      () => `Error in conv2d: depth of input (${inputChannels}) must match ` +\n          `input depth for filter ${$filter.shape[2]}.`);\n  util.assert(\n      conv_util.eitherStridesOrDilationsAreOne(strides, dilations),\n      () => 'Error in conv2D: Either strides or dilations must be 1. ' +\n          `Got strides ${strides} and dilations '${dilations}'`);\n\n  const convInfo = conv_util.computeConv2DInfo(\n      x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n\n  let $bias: Tensor;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n\n    // According to TensorFlow, the bias is supposed be a 1-D tensor or a\n    // scalar.\n    //\n    // 3-D or 4-D bias is not disabled for NHWC format, because they are\n    // currently being used in some cases. For examplem in our code base,\n    // https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/fused_conv2d_test.ts#L1972.\n    if (dataFormat === 'NHWC') {\n      broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    } else {\n      util.assert(\n          $bias.shape.length <= 1,\n          () => `Error in fused conv2d: only supports scalar or 1-D Tensor ` +\n              `bias for NCHW format but got the bias of ` +\n              `rank-${$bias.shape.length}.`);\n\n      util.assert(\n          $bias.shape.length === 0 || $bias.shape[0] === convInfo.outChannels ||\n              $bias.shape[0] === 1,\n          () => `Error in fused conv2d: bias shape (${$bias.shape}) is not ` +\n              `compatible with the number of output channels ` +\n              `(${convInfo.outChannels})`);\n    }\n  }\n\n  let $preluActivationWeights: Tensor;\n  if (preluActivationWeights != null) {\n    // PReLU's activation weights could be a scalar, a 1-D tensor or a 3-D\n    // tensor.\n    const alphaShape = preluActivationWeights.shape;\n    util.assert(\n        alphaShape.length <= 1 || alphaShape.length === 3,\n        () => `Error in fused conv2d: only supports scalar, 1-D Tensor or ` +\n            `3-D Tensor PReLU activation weights but got a tensor of ` +\n            `rank-${alphaShape.length}.`);\n\n    if (alphaShape.length === 1) {\n      // Whether the data format is NCHW or NHWC, the 1-D PReLU activation\n      // weights tensor should be aligned with the output channels of conv2d\n      // result.\n      util.assert(\n          alphaShape[0] === 1 || alphaShape[0] === convInfo.outChannels,\n          () => `Error in fused conv2d: PReLU activation weights ` +\n              `(${alphaShape}) is not compatible with the number of output ` +\n              `channels (${convInfo.outChannels}).`);\n    } else if (alphaShape.length === 3) {\n      // Whether the data format is NCHW or NHWC, the PReLU activation weights\n      // tensor should has the compatible shape with the result of conv2d.\n      try {\n        broadcast_util.assertAndGetBroadcastShape(\n            alphaShape, convInfo.outShape);\n      } catch (e) {\n        const errMsg =\n            `Error in fused conv2d: PReLU activation weights (${alphaShape}) ` +\n            `is not compatible with the output shape of the conv2d ` +\n            `(${convInfo.outShape}).`;\n        throw Error(errMsg);\n      }\n    }\n\n    $preluActivationWeights = convertToTensor(\n        preluActivationWeights, 'prelu weights', 'fused conv2d');\n  }\n\n  const grad = (dy: Tensor4D, saved: Tensor[]) => {\n    util.assert(\n        dataFormat === 'NHWC',\n        () => `Error in gradient of fused conv2D: got dataFormat of ${\n            dataFormat} but only NHWC is currently supported.`);\n\n    const [$filter, x4D, y, $bias] =\n        saved as [Tensor4D, Tensor4D, Tensor4D, Tensor];\n\n    const dyActivation = getFusedDyActivation(dy, y, activation) as Tensor4D;\n\n    util.assert(\n        conv_util.tupleValuesAreOne(dilations),\n        () => 'Error in gradient of fused conv2D: ' +\n            `dilation rates greater than 1 ` +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n\n    const xDer =\n        conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n    const filterDer =\n        conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n    const der: Tensor[] = [xDer, filterDer];\n\n    if ($bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      der.push(biasDer);\n    }\n    return der;\n  };\n\n  const inputs: FusedConv2DInputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n\n  const attrs: FusedConv2DAttrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  };\n\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    const customOp =\n        customGrad((x4D: Tensor4D, filter: Tensor4D, save: GradSaveFunc) => {\n          let res: Tensor4D|Tensor3D =\n              // tslint:disable-next-line: no-unnecessary-type-assertion\n              ENGINE.runKernel(\n                  FusedConv2D, inputs as unknown as NamedTensorMap,\n                  attrs as unknown as NamedAttrMap);\n\n          save([filter, x4D, res]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n    return customOp(x4D, $filter) as T;\n  } else {\n    const customOpWithBias = customGrad(\n        (x4D: Tensor4D, filter: Tensor4D, bias: Tensor, save: GradSaveFunc) => {\n          let res: Tensor4D|Tensor3D = ENGINE.runKernel(\n              FusedConv2D, inputs as unknown as NamedTensorMap,\n              attrs as unknown as NamedAttrMap);\n\n          save([filter, x4D, res, bias]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n\n    return customOpWithBias(x4D, $filter, $bias) as T;\n  }\n}\nexport const conv2d = /* @__PURE__ */ op({fusedConv2d_});\n"],"names":[],"mappings":";;;;AAAA;;;;;;;;;;;;;;;GAeG,CAEH,OAAO,EAAC,MAAM,EAAC,MAAM,cAAc,CAAC;AACpC,OAAO,EAAC,UAAU,EAAC,MAAM,iBAAiB,CAAC;AAC3C,OAAO,EAAC,WAAW,EAAsC,MAAM,oBAAoB,CAAC;AAIpF,OAAO,EAAC,cAAc,EAAC,MAAM,mBAAmB,CAAC;AACjD,OAAO,EAAC,eAAe,EAAC,MAAM,uBAAuB,CAAC;AAEtD,OAAO,KAAK,IAAI,MAAM,YAAY,CAAC;AACnC,OAAO,EAAC,GAAG,EAAC,MAAM,QAAQ,CAAC;AAC3B,OAAO,KAAK,cAAc,MAAM,mBAAmB,CAAC;AACpD,OAAO,EAAC,MAAM,IAAI,aAAa,EAAC,MAAM,WAAW,CAAC;AAClD,OAAO,EAAC,oBAAoB,EAAC,MAAM,2BAA2B,CAAC;AAC/D,OAAO,EAAC,mBAAmB,EAAC,MAAM,0BAA0B,CAAC;AAC7D,OAAO,KAAK,SAAS,MAAM,cAAc,CAAC;AAE1C,OAAO,EAAC,eAAe,EAAE,oBAAoB,EAAE,oBAAoB,EAAE,UAAU,EAAC,MAAM,eAAe,CAAC;AACtG,OAAO,EAAC,EAAE,EAAC,MAAM,cAAc,CAAC;AAChC,OAAO,EAAC,OAAO,EAAC,MAAM,YAAY,CAAC;;;;;;;;;;;;;;;;AAEnC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAwDG,CACH,SAAS,YAAY,CAA8B,EACjD,CAAC,EACD,MAAM,EACN,OAAO,EACP,GAAG,EACH,UAAU,GAAG,MAAM,EACnB,SAAS,GAAG;IAAC,CAAC;IAAE,CAAC;CAAC,EAClB,eAAe,EACf,IAAI,EACJ,UAAU,GAAG,QAAQ,EACrB,sBAAsB,EACtB,cAAc,EAaf;IACC,UAAU,GAAG,UAAU,IAAI,QAAQ,CAAC;IAEpC,QAAI,iQAAU,EAAC,kPAAM,CAAC,KAAK,CAAC,aAAa,EAAE,UAAU,CAAC,KAAK,KAAK,EAAE;QAChE,oEAAoE;QACpE,6BAA6B;QAC7B,IAAI,CAAC,gPAAM,CACP,UAAU,KAAK,MAAM,EACrB,GAAG,CAAG,CAAD,AAAC,yCAAA,EAA4C,UAAU,CAAA,KAAA,CAAO,GAC/D,CAAA,gEAAA,CAAkE,GAClE,CAAA,sCAAA,CAAwC,CAAC,CAAC;QAElD,IAAI,MAAM,OAAG,yPAAa,EACtB,CAAC,EAAE,MAAM,EAAE,OAAO,EAAE,GAAG,EAAE,UAAU,EAAE,SAAS,EAAE,eAAe,CAAC,CAAC;QACrE,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,MAAM,OAAG,mPAAG,EAAC,MAAM,EAAE,IAAI,CAAC,CAAC;SAC5B;QAED,WAAO,sQAAe,EACX,MAAM,EAAE,UAAU,EAAE,sBAAsB,EAAE,cAAc,CAAM,CAAC;KAC7E;IAED,MAAM,EAAE,OAAG,oQAAe,EAAC,CAAC,EAAE,GAAG,EAAE,QAAQ,EAAE,SAAS,CAAC,CAAC;IACxD,MAAM,OAAO,OAAG,oQAAe,EAAC,MAAM,EAAE,QAAQ,EAAE,QAAQ,EAAE,SAAS,CAAC,CAAC;IAEvE,IAAI,GAAG,GAAG,EAAc,CAAC;IACzB,IAAI,YAAY,GAAG,KAAK,CAAC;IAEzB,IAAI,EAAE,CAAC,IAAI,KAAK,CAAC,EAAE;QACjB,YAAY,GAAG,IAAI,CAAC;QACpB,GAAG,OAAG,2PAAO,EAAC,EAAE,EAAE;YAAC,CAAC;YAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC;YAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC;YAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC;SAAC,CAAC,CAAC;KAC/D;IACD,IAAI,CAAC,gPAAM,CACP,GAAG,CAAC,IAAI,KAAK,CAAC,EACd,GAAG,CAAG,CAAD,AAAC,0DAAA,CAA4D,GAC9D,GAAG,GAAG,CAAC,IAAI,CAAA,CAAA,CAAG,CAAC,CAAC;IACxB,IAAI,CAAC,gPAAM,CACP,OAAO,CAAC,IAAI,KAAK,CAAC,EAClB,GAAG,CAAG,CAAD,AAAC,2DAAA,CAA6D,GAC/D,GAAG,OAAO,CAAC,IAAI,CAAA,CAAA,CAAG,CAAC,CAAC;IAC5B,SAAS,CAAC,qQAAyB,CAAC,cAAc,EAAE,GAAG,EAAE,eAAe,CAAC,CAAC;IAC1E,MAAM,aAAa,GAAG,UAAU,KAAK,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;IAC1E,IAAI,CAAC,gPAAM,CACP,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,aAAa,EAClC,GAAG,CAAG,CAAD,AAAC,iCAAA,EAAoC,aAAa,CAAA,aAAA,CAAe,GAClE,CAAA,uBAAA,EAA0B,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,CAAA,CAAA,CAAG,CAAC,CAAC;IACvD,IAAI,CAAC,gPAAM,CACP,SAAS,CAAC,0QAA8B,CAAC,OAAO,EAAE,SAAS,CAAC,EAC5D,GAAG,CAAG,CAAD,yDAA2D,GAC5D,CAAA,YAAA,EAAe,OAAO,CAAA,gBAAA,EAAmB,SAAS,CAAA,CAAA,CAAG,CAAC,CAAC;IAE/D,MAAM,QAAQ,GAAG,SAAS,CAAC,6PAAiB,CACxC,GAAG,CAAC,KAAK,EAAE,OAAO,CAAC,KAAK,EAAE,OAAO,EAAE,SAAS,EAAE,GAAG,EAAE,eAAe,CAAC,CAAC;IAExE,IAAI,KAAa,CAAC;IAClB,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,KAAK,OAAG,oQAAe,EAAC,IAAI,EAAE,MAAM,EAAE,cAAc,CAAC,CAAC;QACtD,CAAC,KAAK,CAAC,OAAG,+PAAc,EAAC,KAAK,EAAE,EAAE,CAAC,CAAC;QAEpC,qEAAqE;QACrE,UAAU;QACV,EAAE;QACF,oEAAoE;QACpE,qEAAqE;QACrE,uIAAuI;QACvI,IAAI,UAAU,KAAK,MAAM,EAAE;YACzB,cAAc,CAAC,sQAA0B,CAAC,QAAQ,CAAC,QAAQ,EAAE,KAAK,CAAC,KAAK,CAAC,CAAC;SAC3E,MAAM;YACL,IAAI,CAAC,gPAAM,CACP,KAAK,CAAC,KAAK,CAAC,MAAM,IAAI,CAAC,EACvB,GAAG,CAAG,CAAA,AAAD,0DAAC,CAA4D,GAC9D,CAAA,yCAAA,CAA2C,GAC3C,CAAA,KAAA,EAAQ,KAAK,CAAC,KAAK,CAAC,MAAM,CAAA,CAAA,CAAG,CAAC,CAAC;YAEvC,IAAI,CAAC,gPAAM,CACP,KAAK,CAAC,KAAK,CAAC,MAAM,KAAK,CAAC,IAAI,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,QAAQ,CAAC,WAAW,IAC/D,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC,EACxB,GAAG,CAAG,CAAD,AAAC,mCAAA,EAAsC,KAAK,CAAC,KAAK,CAAA,SAAA,CAAW,GAC9D,CAAA,8CAAA,CAAgD,GAChD,CAAA,CAAA,EAAI,QAAQ,CAAC,WAAW,CAAA,CAAA,CAAG,CAAC,CAAC;SACtC;KACF;IAED,IAAI,uBAA+B,CAAC;IACpC,IAAI,sBAAsB,IAAI,IAAI,EAAE;QAClC,sEAAsE;QACtE,UAAU;QACV,MAAM,UAAU,GAAG,sBAAsB,CAAC,KAAK,CAAC;QAChD,IAAI,CAAC,gPAAM,CACP,UAAU,CAAC,MAAM,IAAI,CAAC,IAAI,UAAU,CAAC,MAAM,KAAK,CAAC,EACjD,GAAG,CAAG,CAAD,AAAC,2DAAA,CAA6D,GAC/D,CAAA,wDAAA,CAA0D,GAC1D,CAAA,KAAA,EAAQ,UAAU,CAAC,MAAM,CAAA,CAAA,CAAG,CAAC,CAAC;QAEtC,IAAI,UAAU,CAAC,MAAM,KAAK,CAAC,EAAE;YAC3B,oEAAoE;YACpE,sEAAsE;YACtE,UAAU;YACV,IAAI,CAAC,gPAAM,CACP,UAAU,CAAC,CAAC,CAAC,KAAK,CAAC,IAAI,UAAU,CAAC,CAAC,CAAC,KAAK,QAAQ,CAAC,WAAW,EAC7D,GAAG,CAAG,CAAD,AAAC,gDAAA,CAAkD,GACpD,CAAA,CAAA,EAAI,UAAU,CAAA,8CAAA,CAAgD,GAC9D,CAAA,UAAA,EAAa,QAAQ,CAAC,WAAW,CAAA,EAAA,CAAI,CAAC,CAAC;SAChD,MAAM,IAAI,UAAU,CAAC,MAAM,KAAK,CAAC,EAAE;YAClC,wEAAwE;YACxE,oEAAoE;YACpE,IAAI;gBACF,cAAc,CAAC,sQAA0B,CACrC,UAAU,EAAE,QAAQ,CAAC,QAAQ,CAAC,CAAC;aACpC,CAAC,OAAO,CAAC,EAAE;gBACV,MAAM,MAAM,GACR,CAAA,iDAAA,EAAoD,UAAU,CAAA,EAAA,CAAI,GAClE,CAAA,sDAAA,CAAwD,GACxD,CAAA,CAAA,EAAI,QAAQ,CAAC,QAAQ,CAAA,EAAA,CAAI,CAAC;gBAC9B,MAAM,KAAK,CAAC,MAAM,CAAC,CAAC;aACrB;SACF;QAED,uBAAuB,OAAG,oQAAe,EACrC,sBAAsB,EAAE,eAAe,EAAE,cAAc,CAAC,CAAC;KAC9D;IAED,MAAM,IAAI,GAAG,CAAC,EAAY,EAAE,KAAe,EAAE,EAAE;QAC7C,IAAI,CAAC,gPAAM,CACP,UAAU,KAAK,MAAM,EACrB,GAAG,CAAG,CAAD,AAAC,qDAAA,EACF,UAAU,CAAA,sCAAA,CAAwC,CAAC,CAAC;QAE5D,MAAM,CAAC,OAAO,EAAE,GAAG,EAAE,CAAC,EAAE,KAAK,CAAC,GAC1B,KAA+C,CAAC;QAEpD,MAAM,YAAY,OAAG,2QAAoB,EAAC,EAAE,EAAE,CAAC,EAAE,UAAU,CAAa,CAAC;QAEzE,IAAI,CAAC,gPAAM,CACP,SAAS,CAAC,6PAAiB,CAAC,SAAS,CAAC,EACtC,GAAG,CAAG,CAAD,oCAAsC,GACvC,CAAA,8BAAA,CAAgC,GAChC,CAAA,mDAAA,EAAsD,SAAS,CAAA,CAAA,CAAG,CAAC,CAAC;QAE5E,MAAM,IAAI,OACN,qRAAmB,EAAC,GAAG,CAAC,KAAK,EAAE,YAAY,EAAE,OAAO,EAAE,OAAO,EAAE,GAAG,CAAC,CAAC;QACxE,MAAM,SAAS,OACX,uRAAoB,EAAC,GAAG,EAAE,YAAY,EAAE,OAAO,CAAC,KAAK,EAAE,OAAO,EAAE,GAAG,CAAC,CAAC;QACzE,MAAM,GAAG,GAAa;YAAC,IAAI;YAAE,SAAS;SAAC,CAAC;QAExC,IAAI,KAAK,IAAI,IAAI,EAAE;YACjB,MAAM,OAAO,OAAG,2QAAoB,EAAC,KAAK,EAAE,YAAY,CAAC,CAAC;YAC1D,GAAG,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;SACnB;QACD,OAAO,GAAG,CAAC;IACb,CAAC,CAAC;IAEF,MAAM,MAAM,GAAsB;QAChC,CAAC,EAAE,GAAG;QACN,MAAM,EAAE,OAAO;QACf,IAAI,EAAE,KAAK;QACX,sBAAsB,EAAE,uBAAuB;KAChD,CAAC;IAEF,MAAM,KAAK,GAAqB;QAC9B,OAAO;QACP,GAAG;QACH,UAAU;QACV,SAAS;QACT,eAAe;QACf,UAAU;QACV,cAAc;KACf,CAAC;IAEF,yEAAyE;IACzE,oEAAoE;IACpE,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,MAAM,QAAQ,OACV,yPAAU,EAAC,CAAC,GAAa,EAAE,MAAgB,EAAE,IAAkB,EAAE,EAAE;YACjE,IAAI,GAAG,GACH,0DAA0D;YAC1D,kPAAM,CAAC,SAAS,CACZ,6PAAW,EAAE,MAAmC,EAChD,KAAgC,CAAC,CAAC;YAE1C,IAAI,CAAC;gBAAC,MAAM;gBAAE,GAAG;gBAAE,GAAG;aAAC,CAAC,CAAC;YAEzB,IAAI,YAAY,EAAE;gBAChB,0DAA0D;gBAC1D,GAAG,OAAG,2PAAO,EAAC,GAAG,EAAE;oBAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;oBAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;oBAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;iBAAC,CACjD,CAAC;aACd;YAED,OAAO;gBAAC,KAAK,EAAE,GAAG;gBAAE,QAAQ,EAAE,IAAI;YAAA,CAAC,CAAC;QACtC,CAAC,CAAC,CAAC;QACP,OAAO,QAAQ,CAAC,GAAG,EAAE,OAAO,CAAM,CAAC;KACpC,MAAM;QACL,MAAM,gBAAgB,OAAG,yPAAU,EAC/B,CAAC,GAAa,EAAE,MAAgB,EAAE,IAAY,EAAE,IAAkB,EAAE,EAAE;YACpE,IAAI,GAAG,GAAsB,kPAAM,CAAC,SAAS,CACzC,6PAAW,EAAE,MAAmC,EAChD,KAAgC,CAAC,CAAC;YAEtC,IAAI,CAAC;gBAAC,MAAM;gBAAE,GAAG;gBAAE,GAAG;gBAAE,IAAI;aAAC,CAAC,CAAC;YAE/B,IAAI,YAAY,EAAE;gBAChB,0DAA0D;gBAC1D,GAAG,OAAG,2PAAO,EAAC,GAAG,EAAE;oBAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;oBAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;oBAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;iBAAC,CACjD,CAAC;aACd;YAED,OAAO;gBAAC,KAAK,EAAE,GAAG;gBAAE,QAAQ,EAAE,IAAI;YAAA,CAAC,CAAC;QACtC,CAAC,CAAC,CAAC;QAEP,OAAO,gBAAgB,CAAC,GAAG,EAAE,OAAO,EAAE,KAAK,CAAM,CAAC;KACnD;AACH,CAAC;AACM,MAAM,MAAM,GAAG,aAAA,EAAe,KAAC,wPAAE,EAAC;IAAC,YAAY;AAAA,CAAC,CAAC,CAAC"}},
    {"offset": {"line": 273, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-core/dist/ops/fused/depthwise_conv2d.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-core/src/ops/fused/depthwise_conv2d.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../../engine';\nimport {customGrad} from '../../gradients';\nimport {FusedDepthwiseConv2D, FusedDepthwiseConv2DAttrs, FusedDepthwiseConv2DInputs} from '../../kernel_names';\nimport {NamedAttrMap} from '../../kernel_registry';\nimport {Tensor, Tensor3D, Tensor4D} from '../../tensor';\nimport {GradSaveFunc, NamedTensorMap} from '../../tensor_types';\nimport {makeTypesMatch} from '../../tensor_util';\nimport {convertToTensor} from '../../tensor_util_env';\nimport {TensorLike} from '../../types';\nimport * as util from '../../util';\nimport {add} from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport {depthwiseConv2d as unfusedDepthwiseConv2d} from '../depthwise_conv2d';\nimport {depthwiseConv2dNativeBackpropFilter} from '../depthwise_conv2d_native_backprop_filter';\nimport {depthwiseConv2dNativeBackpropInput} from '../depthwise_conv2d_native_backprop_input';\nimport {Activation} from '../fused_types';\nimport {applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse} from '../fused_util';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\n\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedDepthwiseConv2d_<T extends Tensor3D|Tensor4D>({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha\n}: {\n  x: T|TensorLike,\n  filter: Tensor4D|TensorLike,\n  strides: [number, number]|number,\n  pad: 'valid'|'same'|number,\n  dataFormat?: 'NHWC'|'NCHW',\n  dilations?: [number, number]|number,\n  dimRoundingMode?: 'floor'|'round'|'ceil',\n  bias?: Tensor|TensorLike,\n  activation?: Activation,\n  preluActivationWeights?: Tensor,\n  leakyreluAlpha?: number\n}): T {\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedDepthwiseConv2d(\n        x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(\n               result, activation, preluActivationWeights, leakyreluAlpha) as T;\n  }\n\n  const $x = convertToTensor(x, 'x', 'depthwiseConv2d', 'float32');\n  const $filter =\n      convertToTensor(filter, 'filter', 'depthwiseConv2d', 'float32');\n\n  let x4D = $x as Tensor4D;\n  let reshapedTo4D = false;\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n  util.assert(\n      x4D.rank === 4,\n      () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` +\n          `rank ${x4D.rank}.`);\n  util.assert(\n      $filter.rank === 4,\n      () => `Error in fused depthwiseConv2d: filter must be rank 4, ` +\n          `but got rank ${$filter.rank}.`);\n  util.assert(\n      x4D.shape[3] === $filter.shape[2],\n      () => `Error in fused depthwiseConv2d: number of input channels ` +\n          `(${x4D.shape[3]}) must match the inChannels dimension in ` +\n          `filter ${$filter.shape[2]}.`);\n  if (dilations == null) {\n    dilations = [1, 1];\n  }\n  util.assert(\n      conv_util.eitherStridesOrDilationsAreOne(strides, dilations),\n      () =>\n          'Error in fused depthwiseConv2d: Either strides or dilations must ' +\n          `be 1. Got strides ${strides} and dilations '${dilations}'`);\n  conv_util.checkPadOnDimRoundingMode(\n      'fused depthwiseConv2d', pad, dimRoundingMode);\n  const convInfo = conv_util.computeConv2DInfo(\n      x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode,\n      true /* depthwise */);\n\n  let $bias: Tensor;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights: Tensor;\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(\n        preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n  }\n\n  const grad = (dy: Tensor4D, saved: Tensor[]) => {\n    util.assert(\n        conv_util.tupleValuesAreOne(dilations),\n        () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${dilations}'`);\n    const [$filter, x4D, y, bias] = saved;\n\n    const dyActivation = getFusedDyActivation(dy, y, activation) as Tensor4D;\n\n    const xDer = depthwiseConv2dNativeBackpropInput(\n        (x4D as Tensor4D).shape, dyActivation, $filter as Tensor4D, strides,\n        pad, dilations, dimRoundingMode);\n    const filterDer = depthwiseConv2dNativeBackpropFilter(\n        x4D as Tensor4D, dyActivation, ($filter as Tensor4D).shape, strides,\n        pad, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [xDer, filterDer, biasDer];\n    }\n    return [xDer, filterDer];\n  };\n\n  const inputs: FusedDepthwiseConv2DInputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs: FusedDepthwiseConv2DAttrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  };\n\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    const customOp =\n        customGrad((x4D: Tensor4D, filter: Tensor4D, save: GradSaveFunc) => {\n          // tslint:disable-next-line: no-unnecessary-type-assertion\n          let res: Tensor4D|Tensor3D = ENGINE.runKernel(\n              FusedDepthwiseConv2D, inputs as unknown as NamedTensorMap,\n              attrs as unknown as NamedAttrMap);\n\n          save([filter, x4D, res]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n    return customOp(x4D, $filter) as T;\n  } else {\n    const customOpWithBias = customGrad(\n        (x4D: Tensor4D, filter: Tensor4D, bias: Tensor, save: GradSaveFunc) => {\n          // tslint:disable-next-line: no-unnecessary-type-assertion\n          let res: Tensor4D|Tensor3D = ENGINE.runKernel(\n              FusedDepthwiseConv2D, inputs as unknown as NamedTensorMap,\n              attrs as unknown as NamedAttrMap);\n\n          save([filter, x4D, res, bias]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n\n    return customOpWithBias(x4D, $filter, $bias) as T;\n  }\n}\nexport const depthwiseConv2d = /* @__PURE__ */ op({fusedDepthwiseConv2d_});\n"],"names":[],"mappings":";;;;AAAA;;;;;;;;;;;;;;;GAeG,CAEH,OAAO,EAAC,MAAM,EAAC,MAAM,cAAc,CAAC;AACpC,OAAO,EAAC,UAAU,EAAC,MAAM,iBAAiB,CAAC;AAC3C,OAAO,EAAC,oBAAoB,EAAwD,MAAM,oBAAoB,CAAC;AAI/G,OAAO,EAAC,cAAc,EAAC,MAAM,mBAAmB,CAAC;AACjD,OAAO,EAAC,eAAe,EAAC,MAAM,uBAAuB,CAAC;AAEtD,OAAO,KAAK,IAAI,MAAM,YAAY,CAAC;AACnC,OAAO,EAAC,GAAG,EAAC,MAAM,QAAQ,CAAC;AAC3B,OAAO,KAAK,cAAc,MAAM,mBAAmB,CAAC;AACpD,OAAO,KAAK,SAAS,MAAM,cAAc,CAAC;AAC1C,OAAO,EAAC,eAAe,IAAI,sBAAsB,EAAC,MAAM,qBAAqB,CAAC;AAC9E,OAAO,EAAC,mCAAmC,EAAC,MAAM,4CAA4C,CAAC;AAC/F,OAAO,EAAC,kCAAkC,EAAC,MAAM,2CAA2C,CAAC;AAE7F,OAAO,EAAC,eAAe,EAAE,oBAAoB,EAAE,oBAAoB,EAAE,UAAU,EAAC,MAAM,eAAe,CAAC;AACtG,OAAO,EAAC,EAAE,EAAC,MAAM,cAAc,CAAC;AAChC,OAAO,EAAC,OAAO,EAAC,MAAM,YAAY,CAAC;;;;;;;;;;;;;;;;AAEnC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAkDG,CACH,SAAS,qBAAqB,CAA8B,EAC1D,CAAC,EACD,MAAM,EACN,OAAO,EACP,GAAG,EACH,UAAU,GAAG,MAAM,EACnB,SAAS,GAAG;IAAC,CAAC;IAAE,CAAC;CAAC,EAClB,eAAe,EACf,IAAI,EACJ,UAAU,GAAG,QAAQ,EACrB,sBAAsB,EACtB,cAAc,EAaf;IACC,QAAI,iQAAU,EAAC,kPAAM,CAAC,KAAK,CAAC,aAAa,EAAE,UAAU,CAAC,KAAK,KAAK,EAAE;QAChE,IAAI,MAAM,OAAG,4QAAsB,EAC/B,CAAC,EAAE,MAAM,EAAE,OAAO,EAAE,GAAG,EAAE,UAAU,EAAE,SAAS,EAAE,eAAe,CAAC,CAAC;QACrE,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,MAAM,OAAG,mPAAG,EAAC,MAAM,EAAE,IAAI,CAAC,CAAC;SAC5B;QAED,WAAO,sQAAe,EACX,MAAM,EAAE,UAAU,EAAE,sBAAsB,EAAE,cAAc,CAAM,CAAC;KAC7E;IAED,MAAM,EAAE,OAAG,oQAAe,EAAC,CAAC,EAAE,GAAG,EAAE,iBAAiB,EAAE,SAAS,CAAC,CAAC;IACjE,MAAM,OAAO,OACT,oQAAe,EAAC,MAAM,EAAE,QAAQ,EAAE,iBAAiB,EAAE,SAAS,CAAC,CAAC;IAEpE,IAAI,GAAG,GAAG,EAAc,CAAC;IACzB,IAAI,YAAY,GAAG,KAAK,CAAC;IACzB,IAAI,EAAE,CAAC,IAAI,KAAK,CAAC,EAAE;QACjB,YAAY,GAAG,IAAI,CAAC;QACpB,GAAG,OAAG,2PAAO,EAAC,EAAE,EAAE;YAAC,CAAC;YAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC;YAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC;YAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC;SAAC,CAAC,CAAC;KAC/D;IACD,IAAI,CAAC,gPAAM,CACP,GAAG,CAAC,IAAI,KAAK,CAAC,EACd,GAAG,CAAG,CAAA,AAAD,8DAAC,CAAgE,GAClE,CAAA,KAAA,EAAQ,GAAG,CAAC,IAAI,CAAA,CAAA,CAAG,CAAC,CAAC;IAC7B,IAAI,CAAC,gPAAM,CACP,OAAO,CAAC,IAAI,KAAK,CAAC,EAClB,GAAG,CAAG,CAAD,AAAC,uDAAA,CAAyD,GAC3D,CAAA,aAAA,EAAgB,OAAO,CAAC,IAAI,CAAA,CAAA,CAAG,CAAC,CAAC;IACzC,IAAI,CAAC,gPAAM,CACP,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EACjC,GAAG,CAAG,CAAA,AAAD,yDAAC,CAA2D,GAC7D,CAAA,CAAA,EAAI,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,CAAA,yCAAA,CAA2C,GAC3D,CAAA,OAAA,EAAU,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,CAAA,CAAA,CAAG,CAAC,CAAC;IACvC,IAAI,SAAS,IAAI,IAAI,EAAE;QACrB,SAAS,GAAG;YAAC,CAAC;YAAE,CAAC;SAAC,CAAC;KACpB;IACD,IAAI,CAAC,gPAAM,CACP,SAAS,CAAC,0QAA8B,CAAC,OAAO,EAAE,SAAS,CAAC,EAC5D,GAAG,CACC,CADC,kEACkE,GACnE,CAAA,kBAAA,EAAqB,OAAO,CAAA,gBAAA,EAAmB,SAAS,CAAA,CAAA,CAAG,CAAC,CAAC;IACrE,SAAS,CAAC,qQAAyB,CAC/B,uBAAuB,EAAE,GAAG,EAAE,eAAe,CAAC,CAAC;IACnD,MAAM,QAAQ,GAAG,SAAS,CAAC,6PAAiB,CACxC,GAAG,CAAC,KAAK,EAAE,OAAO,CAAC,KAAK,EAAE,OAAO,EAAE,SAAS,EAAE,GAAG,EAAE,eAAe,EAClE,IAAI,CAAC,eAAe,CAAC,CAAC;IAE1B,IAAI,KAAa,CAAC;IAClB,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,KAAK,OAAG,oQAAe,EAAC,IAAI,EAAE,MAAM,EAAE,cAAc,CAAC,CAAC;QACtD,CAAC,KAAK,CAAC,OAAG,+PAAc,EAAC,KAAK,EAAE,EAAE,CAAC,CAAC;QAEpC,cAAc,CAAC,sQAA0B,CAAC,QAAQ,CAAC,QAAQ,EAAE,KAAK,CAAC,KAAK,CAAC,CAAC;KAC3E;IAED,IAAI,uBAA+B,CAAC;IACpC,IAAI,sBAAsB,IAAI,IAAI,EAAE;QAClC,uBAAuB,OAAG,oQAAe,EACrC,sBAAsB,EAAE,eAAe,EAAE,uBAAuB,CAAC,CAAC;KACvE;IAED,MAAM,IAAI,GAAG,CAAC,EAAY,EAAE,KAAe,EAAE,EAAE;QAC7C,IAAI,CAAC,gPAAM,CACP,SAAS,CAAC,6PAAiB,CAAC,SAAS,CAAC,EACtC,GAAG,CAAG,CAAD,4DAA8D,GAC/D,CAAA,oDAAA,CAAsD,GACtD,CAAA,CAAA,EAAI,SAAS,CAAA,CAAA,CAAG,CAAC,CAAC;QAC1B,MAAM,CAAC,OAAO,EAAE,GAAG,EAAE,CAAC,EAAE,IAAI,CAAC,GAAG,KAAK,CAAC;QAEtC,MAAM,YAAY,OAAG,2QAAoB,EAAC,EAAE,EAAE,CAAC,EAAE,UAAU,CAAa,CAAC;QAEzE,MAAM,IAAI,OAAG,qTAAkC,EAC1C,GAAgB,CAAC,KAAK,EAAE,YAAY,EAAE,OAAmB,EAAE,OAAO,EACnE,GAAG,EAAE,SAAS,EAAE,eAAe,CAAC,CAAC;QACrC,MAAM,SAAS,OAAG,uTAAmC,EACjD,GAAe,EAAE,YAAY,EAAG,OAAoB,CAAC,KAAK,EAAE,OAAO,EACnE,GAAG,EAAE,SAAS,EAAE,eAAe,CAAC,CAAC;QAErC,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,MAAM,OAAO,OAAG,2QAAoB,EAAC,KAAK,EAAE,YAAY,CAAC,CAAC;YAC1D,OAAO;gBAAC,IAAI;gBAAE,SAAS;gBAAE,OAAO;aAAC,CAAC;SACnC;QACD,OAAO;YAAC,IAAI;YAAE,SAAS;SAAC,CAAC;IAC3B,CAAC,CAAC;IAEF,MAAM,MAAM,GAA+B;QACzC,CAAC,EAAE,GAAG;QACN,MAAM,EAAE,OAAO;QACf,IAAI,EAAE,KAAK;QACX,sBAAsB,EAAE,uBAAuB;KAChD,CAAC;IACF,MAAM,KAAK,GAA8B;QACvC,OAAO;QACP,GAAG;QACH,UAAU;QACV,SAAS;QACT,eAAe;QACf,UAAU;QACV,cAAc;KACf,CAAC;IAEF,yEAAyE;IACzE,oEAAoE;IACpE,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,MAAM,QAAQ,OACV,yPAAU,EAAC,CAAC,GAAa,EAAE,MAAgB,EAAE,IAAkB,EAAE,EAAE;YACjE,0DAA0D;YAC1D,IAAI,GAAG,GAAsB,kPAAM,CAAC,SAAS,CACzC,sQAAoB,EAAE,MAAmC,EACzD,KAAgC,CAAC,CAAC;YAEtC,IAAI,CAAC;gBAAC,MAAM;gBAAE,GAAG;gBAAE,GAAG;aAAC,CAAC,CAAC;YAEzB,IAAI,YAAY,EAAE;gBAChB,0DAA0D;gBAC1D,GAAG,OAAG,2PAAO,EAAC,GAAG,EAAE;oBAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;oBAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;oBAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;iBAAC,CACjD,CAAC;aACd;YAED,OAAO;gBAAC,KAAK,EAAE,GAAG;gBAAE,QAAQ,EAAE,IAAI;YAAA,CAAC,CAAC;QACtC,CAAC,CAAC,CAAC;QACP,OAAO,QAAQ,CAAC,GAAG,EAAE,OAAO,CAAM,CAAC;KACpC,MAAM;QACL,MAAM,gBAAgB,OAAG,yPAAU,EAC/B,CAAC,GAAa,EAAE,MAAgB,EAAE,IAAY,EAAE,IAAkB,EAAE,EAAE;YACpE,0DAA0D;YAC1D,IAAI,GAAG,GAAsB,kPAAM,CAAC,SAAS,CACzC,sQAAoB,EAAE,MAAmC,EACzD,KAAgC,CAAC,CAAC;YAEtC,IAAI,CAAC;gBAAC,MAAM;gBAAE,GAAG;gBAAE,GAAG;gBAAE,IAAI;aAAC,CAAC,CAAC;YAE/B,IAAI,YAAY,EAAE;gBAChB,0DAA0D;gBAC1D,GAAG,OAAG,2PAAO,EAAC,GAAG,EAAE;oBAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;oBAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;oBAAE,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC;iBAAC,CACjD,CAAC;aACd;YAED,OAAO;gBAAC,KAAK,EAAE,GAAG;gBAAE,QAAQ,EAAE,IAAI;YAAA,CAAC,CAAC;QACtC,CAAC,CAAC,CAAC;QAEP,OAAO,gBAAgB,CAAC,GAAG,EAAE,OAAO,EAAE,KAAK,CAAM,CAAC;KACnD;AACH,CAAC;AACM,MAAM,eAAe,GAAG,aAAA,EAAe,KAAC,wPAAE,EAAC;IAAC,qBAAqB;AAAA,CAAC,CAAC,CAAC"}},
    {"offset": {"line": 510, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-core/dist/ops/fused/mat_mul.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-core/src/ops/fused/mat_mul.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../../engine';\nimport {customGrad} from '../../gradients';\nimport {_FusedMatMul, _FusedMatMulAttrs, _FusedMatMulInputs} from '../../kernel_names';\nimport {NamedAttrMap} from '../../kernel_registry';\nimport {Tensor, Tensor3D} from '../../tensor';\nimport {GradSaveFunc, NamedTensorMap} from '../../tensor_types';\nimport {makeTypesMatch} from '../../tensor_util';\nimport {convertToTensor} from '../../tensor_util_env';\nimport {TensorLike} from '../../types';\nimport * as util from '../../util';\n\nimport {add} from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport {Activation} from '../fused_types';\nimport {applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse} from '../fused_util';\nimport {matMul as unfusedMatMul} from '../mat_mul';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\n\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\nfunction fusedMatMul_({\n  a,\n  b,\n  transposeA = false,\n  transposeB = false,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha = 0.2,\n}: {\n  a: Tensor|TensorLike,\n  b: Tensor|TensorLike,\n  transposeA?: boolean,\n  transposeB?: boolean,\n  bias?: Tensor|TensorLike,\n  activation?: Activation,\n  preluActivationWeights?: Tensor\n  leakyreluAlpha?: number\n}): Tensor {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n      let result = unfusedMatMul(a, b, transposeA, transposeB);\n      if (bias != null) {\n        result = add(result, bias);\n      }\n\n      return applyActivation(\n                 result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n\n    let $a = convertToTensor(a, 'a', 'fused matMul');\n    let $b = convertToTensor(b, 'b', 'fused matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n\n    const innerShapeA =\n        transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB =\n        transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n\n    const outerShapeA =\n        transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB =\n        transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n\n    util.assert(\n        innerShapeA === innerShapeB,\n        () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\n            `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n            `${$b.shape} and transposeA=${transposeA}` +\n            ` and transposeB=${transposeB} must match.`);\n\n    const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(\n        $a.shape.slice(0, -2), $b.shape.slice(0, -2));\n    const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n\n    const a3D: Tensor3D = transposeA ?\n        reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n    const b3D: Tensor3D = transposeB ?\n        reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n\n    let $bias: Tensor;\n    if (bias != null) {\n      $bias = convertToTensor(bias, 'bias', 'fused matMul');\n      [$bias] = makeTypesMatch($bias, $a);\n\n      broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n    }\n\n    let $preluActivationWeights: Tensor;\n    if (preluActivationWeights != null) {\n      $preluActivationWeights = convertToTensor(\n          preluActivationWeights, 'prelu weights', 'fused matMul');\n    }\n\n    const grad = (dy: Tensor3D, saved: Tensor[]) => {\n      const [a3D, b3D, y, $bias] = saved;\n      // we reshape dy because the result of the forward is not\n      // necessarily going to be a 3d tensor due to a reshape done at the end of\n      // the customOp.\n      const dyActivation =\n          getFusedDyActivation(reshape(dy, y.shape), y, activation);\n      let aDer: Tensor;\n      let bDer: Tensor;\n\n      if (!transposeA && !transposeB) {\n        aDer = unfusedMatMul(dyActivation, b3D, false, true);\n        bDer = unfusedMatMul(a3D, dyActivation, true, false);\n      } else if (!transposeA && transposeB) {\n        aDer = unfusedMatMul(dyActivation, b3D, false, false);\n        bDer = unfusedMatMul(dyActivation, a3D, true, false);\n      } else if (transposeA && !transposeB) {\n        aDer = unfusedMatMul(b3D, dyActivation, false, true);\n        bDer = unfusedMatMul(a3D, dyActivation, false, false);\n      } else {\n        aDer = unfusedMatMul(b3D, dyActivation, true, true);\n        bDer = unfusedMatMul(dyActivation, a3D, true, true);\n      }\n\n      if (bias != null) {\n        const biasDer = getFusedBiasGradient($bias, dyActivation);\n        return [aDer, bDer, biasDer];\n      } else {\n        return [aDer, bDer];\n      }\n    };\n\n    const inputs: _FusedMatMulInputs = {\n      a: a3D,\n      b: b3D,\n      bias: $bias,\n      preluActivationWeights: $preluActivationWeights\n    };\n    const attrs: _FusedMatMulAttrs =\n        {transposeA, transposeB, activation, leakyreluAlpha};\n\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n      const customOp =\n          customGrad((a3D: Tensor3D, b3D: Tensor3D, save: GradSaveFunc) => {\n            const res =\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                ENGINE.runKernel(\n                    _FusedMatMul, inputs as unknown as NamedTensorMap,\n                    attrs as unknown as NamedAttrMap) as Tensor;\n\n            save([a3D, b3D, res]);\n\n            return {value: reshape(res, outShape), gradFunc: grad};\n          });\n      return customOp(a3D, b3D);\n    } else {\n      const customOpWithBias = customGrad(\n          (a3D: Tensor3D, b3D: Tensor3D, $bias: Tensor, save: GradSaveFunc) => {\n            const res =\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                ENGINE.runKernel(\n                    _FusedMatMul, inputs as unknown as NamedTensorMap,\n                    attrs as unknown as NamedAttrMap) as Tensor;\n\n            save([a3D, b3D, res, $bias]);\n\n            return {value: reshape(res, outShape), gradFunc: grad};\n          });\n\n      return customOpWithBias(a3D, b3D, $bias);\n    }\n  }\n\n  export const matMul = /* @__PURE__ */ op({fusedMatMul_});\n"],"names":[],"mappings":";;;;AAAA;;;;;;;;;;;;;;;GAeG,CAEH,OAAO,EAAC,MAAM,EAAC,MAAM,cAAc,CAAC;AACpC,OAAO,EAAC,UAAU,EAAC,MAAM,iBAAiB,CAAC;AAC3C,OAAO,EAAC,YAAY,EAAwC,MAAM,oBAAoB,CAAC;AAIvF,OAAO,EAAC,cAAc,EAAC,MAAM,mBAAmB,CAAC;AACjD,OAAO,EAAC,eAAe,EAAC,MAAM,uBAAuB,CAAC;AAEtD,OAAO,KAAK,IAAI,MAAM,YAAY,CAAC;AAEnC,OAAO,EAAC,GAAG,EAAC,MAAM,QAAQ,CAAC;AAC3B,OAAO,KAAK,cAAc,MAAM,mBAAmB,CAAC;AAEpD,OAAO,EAAC,eAAe,EAAE,oBAAoB,EAAE,oBAAoB,EAAE,UAAU,EAAC,MAAM,eAAe,CAAC;AACtG,OAAO,EAAC,MAAM,IAAI,aAAa,EAAC,MAAM,YAAY,CAAC;AACnD,OAAO,EAAC,EAAE,EAAC,MAAM,cAAc,CAAC;AAChC,OAAO,EAAC,OAAO,EAAC,MAAM,YAAY,CAAC;;;;;;;;;;;;;AAEnC;;;;;;;;;;;;;;;;;;;;GAoBG,CACH,SAAS,YAAY,CAAC,EACpB,CAAC,EACD,CAAC,EACD,UAAU,GAAG,KAAK,EAClB,UAAU,GAAG,KAAK,EAClB,IAAI,EACJ,UAAU,GAAG,QAAQ,EACrB,sBAAsB,EACtB,cAAc,GAAG,GAAG,EAUrB;IACG,QAAI,iQAAU,EAAC,kPAAM,CAAC,KAAK,CAAC,aAAa,EAAE,UAAU,CAAC,KAAK,KAAK,EAAE;QAChE,IAAI,MAAM,OAAG,0PAAa,EAAC,CAAC,EAAE,CAAC,EAAE,UAAU,EAAE,UAAU,CAAC,CAAC;QACzD,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,MAAM,OAAG,mPAAG,EAAC,MAAM,EAAE,IAAI,CAAC,CAAC;SAC5B;QAED,WAAO,sQAAe,EACX,MAAM,EAAE,UAAU,EAAE,sBAAsB,EAAE,cAAc,CAAC,CAAC;KACxE;IAED,IAAI,EAAE,OAAG,oQAAe,EAAC,CAAC,EAAE,GAAG,EAAE,cAAc,CAAC,CAAC;IACjD,IAAI,EAAE,OAAG,oQAAe,EAAC,CAAC,EAAE,GAAG,EAAE,cAAc,CAAC,CAAC;IACjD,CAAC,EAAE,EAAE,EAAE,CAAC,OAAG,+PAAc,EAAC,EAAE,EAAE,EAAE,CAAC,CAAC;IAElC,MAAM,WAAW,GACb,UAAU,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC;IAC/D,MAAM,WAAW,GACb,UAAU,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC;IAE/D,MAAM,WAAW,GACb,UAAU,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC;IAC/D,MAAM,WAAW,GACb,UAAU,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,CAAC,CAAC,CAAC;IAE/D,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IACzC,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IACzC,MAAM,SAAS,GAAG,IAAI,CAAC,uPAAa,CAAC,UAAU,CAAC,CAAC;IACjD,MAAM,SAAS,GAAG,IAAI,CAAC,uPAAa,CAAC,UAAU,CAAC,CAAC;IAEjD,IAAI,CAAC,gPAAM,CACP,WAAW,KAAK,WAAW,EAC3B,GAAG,CAAG,CAAA,AAAD,qCAAC,EAAwC,WAAW,CAAA,OAAA,CAAS,GAC9D,GAAG,WAAW,CAAA,yBAAA,EAA4B,EAAE,CAAC,KAAK,CAAA,KAAA,CAAO,GACzD,GAAG,EAAE,CAAC,KAAK,CAAA,gBAAA,EAAmB,UAAU,EAAE,GAC1C,CAAA,gBAAA,EAAmB,UAAU,CAAA,YAAA,CAAc,CAAC,CAAC;IAErD,MAAM,iBAAiB,GAAG,cAAc,CAAC,sQAA0B,CAC/D,EAAE,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAClD,MAAM,QAAQ,GAAG,iBAAiB,CAAC,MAAM,CAAC;QAAC,WAAW;QAAE,WAAW;KAAC,CAAC,CAAC;IAEtE,MAAM,GAAG,GAAa,UAAU,CAAC,CAAC,KAC9B,2PAAO,EAAC,EAAE,EAAE;QAAC,SAAS;QAAE,WAAW;QAAE,WAAW;KAAC,CAAC,CAAC,CAAC,KACpD,2PAAO,EAAC,EAAE,EAAE;QAAC,SAAS;QAAE,WAAW;QAAE,WAAW;KAAC,CAAC,CAAC;IACvD,MAAM,GAAG,GAAa,UAAU,CAAC,CAAC,KAC9B,2PAAO,EAAC,EAAE,EAAE;QAAC,SAAS;QAAE,WAAW;QAAE,WAAW;KAAC,CAAC,CAAC,CAAC,KACpD,2PAAO,EAAC,EAAE,EAAE;QAAC,SAAS;QAAE,WAAW;QAAE,WAAW;KAAC,CAAC,CAAC;IAEvD,IAAI,KAAa,CAAC;IAClB,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,KAAK,OAAG,oQAAe,EAAC,IAAI,EAAE,MAAM,EAAE,cAAc,CAAC,CAAC;QACtD,CAAC,KAAK,CAAC,OAAG,+PAAc,EAAC,KAAK,EAAE,EAAE,CAAC,CAAC;QAEpC,cAAc,CAAC,sQAA0B,CAAC,QAAQ,EAAE,KAAK,CAAC,KAAK,CAAC,CAAC;KAClE;IAED,IAAI,uBAA+B,CAAC;IACpC,IAAI,sBAAsB,IAAI,IAAI,EAAE;QAClC,uBAAuB,OAAG,oQAAe,EACrC,sBAAsB,EAAE,eAAe,EAAE,cAAc,CAAC,CAAC;KAC9D;IAED,MAAM,IAAI,GAAG,CAAC,EAAY,EAAE,KAAe,EAAE,EAAE;QAC7C,MAAM,CAAC,GAAG,EAAE,GAAG,EAAE,CAAC,EAAE,KAAK,CAAC,GAAG,KAAK,CAAC;QACnC,yDAAyD;QACzD,0EAA0E;QAC1E,gBAAgB;QAChB,MAAM,YAAY,OACd,2QAAoB,MAAC,2PAAO,EAAC,EAAE,EAAE,CAAC,CAAC,KAAK,CAAC,EAAE,CAAC,EAAE,UAAU,CAAC,CAAC;QAC9D,IAAI,IAAY,CAAC;QACjB,IAAI,IAAY,CAAC;QAEjB,IAAI,CAAC,UAAU,IAAI,CAAC,UAAU,EAAE;YAC9B,IAAI,OAAG,0PAAa,EAAC,YAAY,EAAE,GAAG,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC;YACrD,IAAI,OAAG,0PAAa,EAAC,GAAG,EAAE,YAAY,EAAE,IAAI,EAAE,KAAK,CAAC,CAAC;SACtD,MAAM,IAAI,CAAC,UAAU,IAAI,UAAU,EAAE;YACpC,IAAI,OAAG,0PAAa,EAAC,YAAY,EAAE,GAAG,EAAE,KAAK,EAAE,KAAK,CAAC,CAAC;YACtD,IAAI,OAAG,0PAAa,EAAC,YAAY,EAAE,GAAG,EAAE,IAAI,EAAE,KAAK,CAAC,CAAC;SACtD,MAAM,IAAI,UAAU,IAAI,CAAC,UAAU,EAAE;YACpC,IAAI,OAAG,0PAAa,EAAC,GAAG,EAAE,YAAY,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC;YACrD,IAAI,OAAG,0PAAa,EAAC,GAAG,EAAE,YAAY,EAAE,KAAK,EAAE,KAAK,CAAC,CAAC;SACvD,MAAM;YACL,IAAI,OAAG,0PAAa,EAAC,GAAG,EAAE,YAAY,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;YACpD,IAAI,OAAG,0PAAa,EAAC,YAAY,EAAE,GAAG,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;SACrD;QAED,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,MAAM,OAAO,OAAG,2QAAoB,EAAC,KAAK,EAAE,YAAY,CAAC,CAAC;YAC1D,OAAO;gBAAC,IAAI;gBAAE,IAAI;gBAAE,OAAO;aAAC,CAAC;SAC9B,MAAM;YACL,OAAO;gBAAC,IAAI;gBAAE,IAAI;aAAC,CAAC;SACrB;IACH,CAAC,CAAC;IAEF,MAAM,MAAM,GAAuB;QACjC,CAAC,EAAE,GAAG;QACN,CAAC,EAAE,GAAG;QACN,IAAI,EAAE,KAAK;QACX,sBAAsB,EAAE,uBAAuB;KAChD,CAAC;IACF,MAAM,KAAK,GACP;QAAC,UAAU;QAAE,UAAU;QAAE,UAAU;QAAE,cAAc;IAAA,CAAC,CAAC;IAEzD,yEAAyE;IACzE,oEAAoE;IACpE,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,MAAM,QAAQ,OACV,yPAAU,EAAC,CAAC,GAAa,EAAE,GAAa,EAAE,IAAkB,EAAE,EAAE;YAC9D,MAAM,GAAG,GACL,0DAA0D;YAC1D,kPAAM,CAAC,SAAS,CACZ,8PAAY,EAAE,MAAmC,EACjD,KAAgC,CAAW,CAAC;YAEpD,IAAI,CAAC;gBAAC,GAAG;gBAAE,GAAG;gBAAE,GAAG;aAAC,CAAC,CAAC;YAEtB,OAAO;gBAAC,KAAK,MAAE,2PAAO,EAAC,GAAG,EAAE,QAAQ,CAAC;gBAAE,QAAQ,EAAE,IAAI;YAAA,CAAC,CAAC;QACzD,CAAC,CAAC,CAAC;QACP,OAAO,QAAQ,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC;KAC3B,MAAM;QACL,MAAM,gBAAgB,OAAG,yPAAU,EAC/B,CAAC,GAAa,EAAE,GAAa,EAAE,KAAa,EAAE,IAAkB,EAAE,EAAE;YAClE,MAAM,GAAG,GACL,0DAA0D;YAC1D,kPAAM,CAAC,SAAS,CACZ,8PAAY,EAAE,MAAmC,EACjD,KAAgC,CAAW,CAAC;YAEpD,IAAI,CAAC;gBAAC,GAAG;gBAAE,GAAG;gBAAE,GAAG;gBAAE,KAAK;aAAC,CAAC,CAAC;YAE7B,OAAO;gBAAC,KAAK,MAAE,2PAAO,EAAC,GAAG,EAAE,QAAQ,CAAC;gBAAE,QAAQ,EAAE,IAAI;YAAA,CAAC,CAAC;QACzD,CAAC,CAAC,CAAC;QAEP,OAAO,gBAAgB,CAAC,GAAG,EAAE,GAAG,EAAE,KAAK,CAAC,CAAC;KAC1C;AACH,CAAC;AAEM,MAAM,MAAM,GAAG,aAAA,EAAe,KAAC,wPAAE,EAAC;IAAC,YAAY;AAAA,CAAC,CAAC,CAAC"}}]
}