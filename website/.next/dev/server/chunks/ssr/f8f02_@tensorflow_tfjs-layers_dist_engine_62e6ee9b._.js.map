{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 4, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-layers@4.22.0_@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-layers/dist/engine/topology.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-layers/src/engine/topology.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original source: keras/engine/topology.py */\n\nimport {DataType, Scalar, serialization, Tensor, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {getNextUniqueTensorId, getUid} from '../backend/state';\nimport {getScopedTensorName, getUniqueTensorName, nameScope} from '../common';\nimport {Constraint} from '../constraints';\nimport {AttributeError, NotImplementedError, RuntimeError, ValueError} from '../errors';\nimport {getInitializer, Initializer} from '../initializers';\nimport {Shape} from '../keras_format/common';\nimport {Regularizer} from '../regularizers';\nimport {Kwargs, RegularizerFn} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as types_utils from '../utils/types_utils';\nimport * as variable_utils from '../utils/variable_utils';\nimport {batchGetValue, batchSetValue, LayerVariable} from '../variables';\n\n// TODO(michaelterry): This is a stub until it's defined.\nexport type Op = (x: LayerVariable) => LayerVariable;\n\n/**\n * Constructor arguments for InputSpec.\n */\nexport interface InputSpecArgs {\n  /** Expected datatype of the input. */\n  dtype?: DataType;\n  /** Expected shape of the input (may include null for unchecked axes). */\n  shape?: Shape;\n  /** Expected rank of the input. */\n  ndim?: number;\n  /** Maximum rank of the input. */\n  maxNDim?: number;\n  /** Minimum rank of the input. */\n  minNDim?: number;\n  /** Dictionary mapping integer axes to a specific dimension value. */\n  axes?: {[axis: number]: number};\n}\n\n/**\n * Specifies the ndim, dtype and shape of every input to a layer.\n *\n * Every layer should expose (if appropriate) an `inputSpec` attribute:\n * a list of instances of InputSpec (one per input tensor).\n *\n * A null entry in a shape is compatible with any dimension,\n * a null shape is compatible with any shape.\n */\nexport class InputSpec {\n  /** Expected datatype of the input. */\n  dtype?: DataType;\n  /** Expected shape of the input (may include null for unchecked axes). */\n  shape?: Shape;\n  /** Expected rank of the input. */\n  ndim?: number;\n  /** Maximum rank of the input. */\n  maxNDim?: number;\n  /** Minimum rank of the input. */\n  minNDim?: number;\n  /** Dictionary mapping integer axes to a specific dimension value. */\n  axes?: {[axis: number]: number};\n\n  constructor(args: InputSpecArgs) {\n    this.dtype = args.dtype;\n    this.shape = args.shape;\n    /*\n      TODO(michaelterry): Could throw error if ndim and shape are both defined\n        (then backport).\n    */\n    if (args.shape != null) {\n      this.ndim = args.shape.length;\n    } else {\n      this.ndim = args.ndim;\n    }\n    this.maxNDim = args.maxNDim;\n    this.minNDim = args.minNDim;\n    this.axes = args.axes || {};\n  }\n}\n\n/**\n * `tf.SymbolicTensor` is a placeholder for a Tensor without any concrete value.\n *\n * They are most often encountered when building a graph of `Layer`s for a\n * `tf.LayersModel` and the input data's shape, but not values are known.\n *\n * @doc {heading: 'Models', 'subheading': 'Classes'}\n */\nexport class SymbolicTensor {\n  /* A unique ID for the tensor to be able to differentiate tensors. */\n  readonly id: number;\n  // The fully scoped name of this Variable, including a unique suffix if needed\n  readonly name: string;\n  // The originally requested fully scoped name of this Variable, not including\n  // any unique suffix.  This may be needed when restoring weights because this\n  // original name is used as a key.\n  readonly originalName?: string;\n  /**\n   * Rank/dimensionality of the tensor.\n   */\n  readonly rank: number;\n  /**\n   * Replacement for _keras_history.\n   */\n  nodeIndex: number;\n  /**\n   * Replacement for _keras_history.\n   */\n  tensorIndex: number;\n\n  /**\n   *\n   * @param dtype\n   * @param shape\n   * @param sourceLayer The Layer that produced this symbolic tensor.\n   * @param inputs The inputs passed to sourceLayer's __call__() method.\n   * @param nodeIndex\n   * @param tensorIndex\n   * @param callArgs The keyword arguments passed to the __call__() method.\n   * @param name\n   * @param outputTensorIndex The index of this tensor in the list of outputs\n   *   returned by apply().\n   */\n  constructor(\n      readonly dtype: DataType, readonly shape: Shape,\n      public sourceLayer: Layer, readonly inputs: SymbolicTensor[],\n      readonly callArgs: Kwargs, name?: string,\n      readonly outputTensorIndex?: number) {\n    this.id = getNextUniqueTensorId();\n    if (name != null) {\n      this.originalName = getScopedTensorName(name);\n      this.name = getUniqueTensorName(this.originalName);\n    }\n    this.rank = shape.length;\n  }\n}\n\n/**\n * Constructor arguments for Node.\n */\nexport interface NodeArgs {\n  /**\n   * The layer that takes `inputTensors` and turns them into `outputTensors`.\n   * (the node gets created when the `call` method of the layer is called).\n   */\n  outboundLayer: Layer;\n  /**\n   * A list of layers, the same length as `inputTensors`, the layers from where\n   * `inputTensors` originate.\n   */\n  inboundLayers: Layer[];\n  /**\n   * A list of integers, the same length as `inboundLayers`. `nodeIndices[i]` is\n   * the origin node of `inputTensors[i]` (necessary since each inbound layer\n   * might have several nodes, e.g. if the layer is being shared with a\n   * different data stream).\n   */\n  nodeIndices: number[];\n  /**\n   * A list of integers, the same length as `inboundLayers`. `tensorIndices[i]`\n   * is the index of `inputTensors[i]` within the output of the inbound layer\n   * (necessary since each inbound layer might have multiple tensor outputs,\n   * with each one being independently manipulable).\n   */\n  tensorIndices: number[];\n  /** List of input tensors. */\n  inputTensors: SymbolicTensor[];\n  /** List of output tensors. */\n  outputTensors: SymbolicTensor[];\n  /** List of input masks (a mask can be a tensor, or null). */\n  inputMasks: Tensor[];\n  /** List of output masks (a mask can be a tensor, or null). */\n  outputMasks: Tensor[];\n  /** List of input shape tuples. */\n  inputShapes: Shape|Shape[];\n  /** List of output shape tuples. */\n  outputShapes: Shape|Shape[];\n}\n\n/**\n * The type of the return value of Layer.dispose() and Container.dispose().\n */\nexport interface DisposeResult {\n  /**\n   * Reference count after the dispose call.\n   */\n  refCountAfterDispose: number;\n\n  /**\n   * Number of variables dispose in this dispose call.\n   */\n  numDisposedVariables: number;\n}\n\nlet _nextNodeID = 0;\n\n/**\n * A `Node` describes the connectivity between two layers.\n *\n * Each time a layer is connected to some new input,\n * a node is added to `layer.inboundNodes`.\n *\n * Each time the output of a layer is used by another layer,\n * a node is added to `layer.outboundNodes`.\n *\n * `nodeIndices` and `tensorIndices` are basically fine-grained coordinates\n * describing the origin of the `inputTensors`, verifying the following:\n *\n * `inputTensors[i] ==\n * inboundLayers[i].inboundNodes[nodeIndices[i]].outputTensors[\n *   tensorIndices[i]]`\n *\n * A node from layer A to layer B is added to:\n *     A.outboundNodes\n *     B.inboundNodes\n */\nexport class Node {\n  /**\n   * The layer that takes `inputTensors` and turns them into `outputTensors`\n   * (the node gets created when the `call` method of the layer is called).\n   */\n  outboundLayer: Layer;\n  /**\n   * A list of layers, the same length as `inputTensors`, the layers from where\n   * `inputTensors` originate.\n   */\n  inboundLayers: Layer[];\n  /**\n   * A list of integers, the same length as `inboundLayers`. `nodeIndices[i]` is\n   * the origin node of `inputTensors[i]` (necessary since each inbound layer\n   * might have several nodes, e.g. if the layer is being shared with a\n   * different data stream).\n   */\n  nodeIndices: number[];\n  /**\n   * A list of integers, the same length as `inboundLayers`. `tensorIndices[i]`\n   * is the index of `inputTensors[i]` within the output of the inbound layer\n   * (necessary since each inbound layer might have multiple tensor outputs,\n   * with each one being independently manipulable).\n   */\n  tensorIndices: number[];\n  /** List of input tensors. */\n  inputTensors: SymbolicTensor[];\n  /** List of output tensors. */\n  outputTensors: SymbolicTensor[];\n  /** List of input masks (a mask can be a tensor, or null). */\n  inputMasks: Tensor[];\n  /** List of output masks (a mask can be a tensor, or null). */\n  outputMasks: Tensor[];\n  /** List of input shape tuples. */\n  inputShapes: Shape|Shape[];\n  /** List of output shape tuples. */\n  outputShapes: Shape|Shape[];\n\n  readonly id: number;\n\n  constructor(\n      args: NodeArgs,\n      // TODO(michaelterry): Define actual type for this.\n      public callArgs?: Kwargs) {\n    this.id = _nextNodeID++;\n    /*\n      Layer instance (NOT a list).\n      this is the layer that takes a list of input tensors\n      and turns them into a list of output tensors.\n      the current node will be added to\n      the inboundNodes of outboundLayer.\n    */\n    this.outboundLayer = args.outboundLayer;\n\n    /*\n        The following 3 properties describe where\n        the input tensors come from: which layers,\n        and for each layer, which node and which\n        tensor output of each node.\n    */\n\n    // List of layer instances.\n    this.inboundLayers = args.inboundLayers;\n    // List of integers, 1:1 mapping with inboundLayers.\n    this.nodeIndices = args.nodeIndices;\n    // List of integers, 1:1 mapping with inboundLayers.\n    this.tensorIndices = args.tensorIndices;\n\n    /*\n        Following 2 properties:\n        tensor inputs and outputs of outboundLayer.\n    */\n\n    // List of tensors. 1:1 mapping with inboundLayers.\n    this.inputTensors = args.inputTensors;\n    // List of tensors, created by outboundLayer.call().\n    this.outputTensors = args.outputTensors;\n\n    /*\n        Following 2 properties: input and output masks.\n        List of tensors, 1:1 mapping with inputTensor.\n    */\n    this.inputMasks = args.inputMasks;\n    // List of tensors, created by outboundLayer.computeMask().\n    this.outputMasks = args.outputMasks;\n\n    // Following 2 properties: input and output shapes.\n\n    // List of shape tuples, shapes of inputTensors.\n    this.inputShapes = args.inputShapes;\n    // List of shape tuples, shapes of outputTensors.\n    this.outputShapes = args.outputShapes;\n\n    // Add nodes to all layers involved.\n    for (const layer of args.inboundLayers) {\n      if (layer != null) {\n        layer.outboundNodes.push(this);\n      }\n    }\n    args.outboundLayer.inboundNodes.push(this);\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const inboundNames: string[] = [];\n    for (const layer of this.inboundLayers) {\n      if (layer != null) {\n        inboundNames.push(layer.name);\n      } else {\n        inboundNames.push(null);\n      }\n    }\n    return {\n      outboundLayer: this.outboundLayer ? this.outboundLayer.name : null,\n      inboundLayers: inboundNames,\n      nodeIndices: this.nodeIndices,\n      tensorIndices: this.tensorIndices\n    };\n  }\n}\n\n/** Constructor arguments for Layer. */\nexport declare interface LayerArgs {\n  /**\n   * If defined, will be used to create an input layer to insert before this\n   * layer. If both `inputShape` and `batchInputShape` are defined,\n   * `batchInputShape` will be used. This argument is only applicable to input\n   * layers (the first layer of a model).\n   */\n  inputShape?: Shape;\n  /**\n   * If defined, will be used to create an input layer to insert before this\n   * layer. If both `inputShape` and `batchInputShape` are defined,\n   * `batchInputShape` will be used. This argument is only applicable to input\n   * layers (the first layer of a model).\n   */\n  batchInputShape?: Shape;\n  /**\n   * If `inputShape` is specified and `batchInputShape` is *not* specified,\n   * `batchSize` is used to construct the `batchInputShape`: `[batchSize,\n   * ...inputShape]`\n   */\n  batchSize?: number;\n  /**\n   * The data-type for this layer. Defaults to 'float32'.\n   * This argument is only applicable to input layers (the first layer of a\n   * model).\n   */\n  dtype?: DataType;\n  /** Name for this layer. */\n  name?: string;\n  /**\n   * Whether the weights of this layer are updatable by `fit`.\n   * Defaults to true.\n   */\n  trainable?: boolean;\n  /**\n   * Initial weight values of the layer.\n   */\n  weights?: Tensor[];\n  /** Legacy support. Do not use for new code. */\n  inputDType?: DataType;\n}\n\n// If necessary, add `output` arguments to the CallHook function.\n// This is currently used for testing only, but may be used for debugger-related\n// purposes in the future.\nexport type CallHook = (inputs: Tensor|Tensor[], kwargs: Kwargs) => void;\n\nlet _nextLayerID = 0;\n\n/**\n * A layer is a grouping of operations and weights that can be composed to\n * create a `tf.LayersModel`.\n *\n * Layers are constructed by using the functions under the\n * [tf.layers](#Layers-Basic) namespace.\n *\n * @doc {heading: 'Layers', subheading: 'Classes', namespace: 'layers'}\n */\nexport abstract class Layer extends serialization.Serializable {\n  /** Name for this layer. Must be unique within a model. */\n  name: string;\n  /**\n   * List of InputSpec class instances.\n   *\n   * Each entry describes one required input:\n   * - ndim\n   * - dtype\n   * A layer with `n` input tensors must have an `inputSpec` of length `n`.\n   */\n  inputSpec: InputSpec[];\n  supportsMasking: boolean;\n  /** Whether the layer weights will be updated during training. */\n  protected trainable_: boolean;\n  batchInputShape: Shape;\n  dtype: DataType;\n  initialWeights: Tensor[];\n\n  inboundNodes: Node[];\n  outboundNodes: Node[];\n\n  activityRegularizer: Regularizer;\n\n  protected _trainableWeights: LayerVariable[];\n  private _nonTrainableWeights: LayerVariable[];\n  private _losses: RegularizerFn[];\n  // TODO(cais): _updates is currently unused.\n  private _updates: Tensor[];\n  private _built: boolean;\n  private _callHook: CallHook = null;\n\n  private _addedWeightNames: string[] = [];\n\n  readonly id: number;\n\n  // Porting Notes: PyKeras does not have this property in this base Layer\n  //   class. Instead lets Layer subclass set it dynamically and checks the\n  //   value with `hasattr`. In tfjs-layers, we let this be a member of this\n  //   base class.\n  protected _stateful = false;\n\n  protected _refCount: number|null;\n\n  // A flag for whether fast (i.e., all-zero) weight initialization is to\n  // be used during `build()` call. This speeds up weight initialization\n  // by saving unnecessary calls to expensive initializers in cases where\n  // the initialized values will be overwritten by loaded weight values\n  // during model loading.\n  private fastWeightInitDuringBuild: boolean;\n\n  constructor(args: LayerArgs = {}) {\n    super();\n    this.id = _nextLayerID++;\n\n    this.activityRegularizer = null;\n\n    this.inputSpec = null;\n    this.supportsMasking = false;\n\n    // These properties will be set upon call of this.build()\n    this._trainableWeights = [];\n    this._nonTrainableWeights = [];\n    this._losses = [];\n    this._updates = [];\n    this._built = false;\n\n    /*\n      These lists will be filled via successive calls\n      to this.addInboundNode().\n     */\n    this.inboundNodes = [];\n    this.outboundNodes = [];\n\n    let name = args.name;\n    if (!name) {\n      const prefix = this.getClassName();\n      name = generic_utils.toSnakeCase(prefix) + '_' + getUid(prefix);\n    }\n    this.name = name;\n\n    this.trainable_ = args.trainable == null ? true : args.trainable;\n\n    if (args.inputShape != null || args.batchInputShape != null) {\n      /*\n        In this case we will later create an input layer\n        to insert before the current layer\n       */\n      let batchInputShape: Shape;\n      if (args.batchInputShape != null) {\n        batchInputShape = args.batchInputShape;\n      } else if (args.inputShape != null) {\n        let batchSize: number = null;\n        if (args.batchSize != null) {\n          batchSize = args.batchSize;\n        }\n        batchInputShape = [batchSize].concat(args.inputShape);\n      }\n      this.batchInputShape = batchInputShape;\n\n      // Set dtype.\n      let dtype = args.dtype;\n      if (dtype == null) {\n        dtype = args.inputDType;\n      }\n      if (dtype == null) {\n        dtype = 'float32';\n      }\n      this.dtype = dtype;\n    }\n\n    if (args.weights != null) {\n      this.initialWeights = args.weights;\n    } else {\n      this.initialWeights = null;\n    }\n\n    // The value of `_refCount` is initialized to null. When the layer is used\n    // in a symbolic way for the first time, it will be set to 1.\n    this._refCount = null;\n\n    this.fastWeightInitDuringBuild = false;\n  }\n\n  /**\n   * Converts a layer and its index to a unique (immutable type) name.\n   * This function is used internally with `this.containerNodes`.\n   * @param layer The layer.\n   * @param nodeIndex The layer's position (e.g. via enumerate) in a list of\n   *   nodes.\n   *\n   * @returns The unique name.\n   */\n  protected static nodeKey(layer: Layer, nodeIndex: number) {\n    return layer.name + '_ib-' + nodeIndex.toString();\n  }\n\n  /**\n   * Returns this.inboundNode at index nodeIndex.\n   *\n   * Porting note: This is a replacement for _get_node_attribute_at_index()\n   * @param nodeIndex\n   * @param attrName The name of the attribute related to request for this node.\n   */\n  private getNodeAtIndex(nodeIndex: number, attrName: string): Node {\n    if (this.inboundNodes.length === 0) {\n      throw new RuntimeError(\n          'The layer has never been called ' +\n          `and thus has no defined ${attrName}.`);\n    }\n    if (this.inboundNodes.length <= nodeIndex) {\n      throw new ValueError(\n          `Asked to get ${attrName} at node ${nodeIndex}, ` +\n          `but the layer has only ${this.inboundNodes.length} inbound nodes.`);\n    }\n    return this.inboundNodes[nodeIndex];\n  }\n\n  /**\n   * Retrieves the input tensor(s) of a layer at a given node.\n   *\n   * @param nodeIndex Integer, index of the node from which to retrieve the\n   *   attribute. E.g. `nodeIndex=0` will correspond to the first time the layer\n   *   was called.\n   *\n   * @return A tensor (or list of tensors if the layer has multiple inputs).\n   */\n  getInputAt(nodeIndex: number): SymbolicTensor|SymbolicTensor[] {\n    return generic_utils.singletonOrArray(\n        this.getNodeAtIndex(nodeIndex, 'input').inputTensors);\n  }\n\n  /**\n   * Retrieves the output tensor(s) of a layer at a given node.\n   *\n   * @param nodeIndex Integer, index of the node from which to retrieve the\n   *   attribute. E.g. `nodeIndex=0` will correspond to the first time the layer\n   *   was called.\n   *\n   * @return A tensor (or list of tensors if the layer has multiple outputs).\n   */\n  getOutputAt(nodeIndex: number): SymbolicTensor|SymbolicTensor[] {\n    return generic_utils.singletonOrArray(\n        this.getNodeAtIndex(nodeIndex, 'output').outputTensors);\n  }\n\n  // Properties\n\n  /**\n   * Retrieves the input tensor(s) of a layer.\n   *\n   * Only applicable if the layer has exactly one inbound node,\n   * i.e. if it is connected to one incoming layer.\n   *\n   * @return Input tensor or list of input tensors.\n   *\n   * @exception AttributeError if the layer is connected to more than one\n   *   incoming layers.\n   */\n  get input(): SymbolicTensor|SymbolicTensor[] {\n    if (this.inboundNodes.length > 1) {\n      throw new AttributeError(\n          `Layer ${this.name}` +\n          ' has multiple inbound nodes, ' +\n          'hence the notion of \"layer input\" ' +\n          'is ill-defined. ' +\n          'Use `getInputAt(nodeIndex)` instead.');\n    } else if (this.inboundNodes.length === 0) {\n      throw new AttributeError(\n          `Layer ${this.name}` +\n          ' is not connected, no input to return.');\n    }\n    return generic_utils.singletonOrArray(\n        this.getNodeAtIndex(0, 'input').inputTensors);\n  }\n\n  /**\n   * Retrieves the output tensor(s) of a layer.\n   *\n   * Only applicable if the layer has exactly one inbound node,\n   * i.e. if it is connected to one incoming layer.\n   *\n   * @return Output tensor or list of output tensors.\n   *\n   * @exception AttributeError if the layer is connected to more than one\n   *   incoming layers.\n   */\n  get output(): SymbolicTensor|SymbolicTensor[] {\n    if (this.inboundNodes.length === 0) {\n      throw new AttributeError(\n          `Layer ${this.name}` +\n          ' has no inbound nodes.');\n    }\n    if (this.inboundNodes.length > 1) {\n      throw new AttributeError(\n          `Layer ${this.name}` +\n          ' has multiple inbound nodes, ' +\n          'hence the notion of \"layer output\" ' +\n          'is ill-defined. ' +\n          'Use `getOutputAt(nodeIndex)` instead.');\n    }\n    return generic_utils.singletonOrArray(\n        this.getNodeAtIndex(0, 'output').outputTensors);\n  }\n\n  get losses(): RegularizerFn[] {\n    return this._losses;\n  }\n\n  /**\n   * Retrieves the Layer's current loss values.\n   *\n   * Used for regularizers during training.\n   */\n  calculateLosses(): Scalar[] {\n    // Porting Node: This is an augmentation to Layer.loss in PyKeras.\n    //   In PyKeras, Layer.loss returns symbolic tensors. Here a concrete\n    //   Tensor (specifically Scalar) values are returned. This is due to the\n    //   imperative backend.\n    return this.losses.map(lossFn => lossFn());\n  }\n\n  get updates(): Tensor[] {\n    return this._updates;\n  }\n\n  get built(): boolean {\n    return this._built;\n  }\n\n  set built(built: boolean) {\n    this._built = built;\n  }\n\n  get trainable(): boolean {\n    return this.trainable_;\n  }\n\n  set trainable(trainable: boolean) {\n    this._trainableWeights.forEach(w => w.trainable = trainable);\n    this.trainable_ = trainable;\n  }\n\n  get trainableWeights(): LayerVariable[] {\n    if (this.trainable_) {\n      return this._trainableWeights.filter(w => w.trainable);\n    } else {\n      return [];\n    }\n  }\n\n  set trainableWeights(weights: LayerVariable[]) {\n    this._trainableWeights = weights;\n  }\n\n  get nonTrainableWeights(): LayerVariable[] {\n    if (this.trainable) {\n      return this._trainableWeights.filter(w => !w.trainable)\n          .concat(this._nonTrainableWeights);\n    } else {\n      return this._trainableWeights.concat(this._nonTrainableWeights);\n    }\n  }\n\n  set nonTrainableWeights(weights: LayerVariable[]) {\n    this._nonTrainableWeights = weights;\n  }\n\n  /**\n   * The concatenation of the lists trainableWeights and nonTrainableWeights\n   * (in this order).\n   */\n  get weights(): LayerVariable[] {\n    return this.trainableWeights.concat(this.nonTrainableWeights);\n  }\n\n  get stateful(): boolean {\n    return this._stateful;\n  }\n\n  /**\n   * Reset the states of the layer.\n   *\n   * This method of the base Layer class is essentially a no-op.\n   * Subclasses that are stateful (e.g., stateful RNNs) should override this\n   * method.\n   */\n  resetStates(): void {\n    if (!this.stateful) {\n      throw new Error(\n          'Cannot call the resetStates() method of a non-stateful Layer ' +\n          'object.');\n    }\n  }\n\n  /**\n   * Checks compatibility between the layer and provided inputs.\n   *\n   * This checks that the tensor(s) `input`\n   * verify the input assumptions of the layer\n   * (if any). If not, exceptions are raised.\n   *\n   * @param inputs Input tensor or list of input tensors.\n   *\n   * @exception ValueError in case of mismatch between\n   *   the provided inputs and the expectations of the layer.\n   */\n  protected assertInputCompatibility(inputs: Tensor|Tensor[]|SymbolicTensor|\n                                     SymbolicTensor[]): void {\n    const inputsList = generic_utils.toList(inputs);\n    if (this.inputSpec == null || this.inputSpec.length === 0) {\n      return;\n    }\n    const inputSpec = generic_utils.toList(this.inputSpec);\n    if (inputsList.length !== inputSpec.length) {\n      throw new ValueError(\n          `Layer ${this.name} expects ${inputSpec.length} inputs, ` +\n          `but it received ${inputsList.length} input tensors. ` +\n          `Input received: ${inputs}`);\n    }\n    for (let inputIndex = 0; inputIndex < inputsList.length; inputIndex++) {\n      const x = inputsList[inputIndex];\n      const spec: InputSpec = inputSpec[inputIndex];\n      if (spec == null) {\n        continue;\n      }\n\n      // Check ndim.\n      const ndim = x.rank;\n      if (spec.ndim != null) {\n        if (ndim !== spec.ndim) {\n          throw new ValueError(\n              `Input ${inputIndex} is incompatible with layer ${this.name}: ` +\n              `expected ndim=${spec.ndim}, found ndim=${ndim}`);\n        }\n      }\n      if (spec.maxNDim != null) {\n        if (ndim > spec.maxNDim) {\n          throw new ValueError(\n              `Input ${inputIndex} is incompatible with layer ${this.name}` +\n              `: expected max_ndim=${spec.maxNDim}, found ndim=${ndim}`);\n        }\n      }\n      if (spec.minNDim != null) {\n        if (ndim < spec.minNDim) {\n          throw new ValueError(\n              `Input ${inputIndex} is incompatible with layer ${this.name}` +\n              `: expected min_ndim=${spec.minNDim}, found ndim=${ndim}.`);\n        }\n      }\n\n      // Check dtype.\n      if (spec.dtype != null) {\n        if (x.dtype !== spec.dtype) {\n          throw new ValueError(\n              `Input ${inputIndex} is incompatible with layer ${this.name} ` +\n              `: expected dtype=${spec.dtype}, found dtype=${x.dtype}.`);\n        }\n      }\n\n      // Check specific shape axes.\n      if (spec.axes) {\n        const xShape = x.shape;\n        for (const key in spec.axes) {\n          const axis = Number(key);\n          const value = spec.axes[key];\n          // Perform Python-style slicing in case axis < 0;\n          // TODO(cais): Use https://github.com/alvivi/typescript-underscore to\n          // ensure type safety through Underscore calls.\n          const xShapeAtAxis =\n              axis >= 0 ? xShape[axis] : xShape[xShape.length + axis];\n          if (value != null && [value, null].indexOf(xShapeAtAxis) === -1) {\n            throw new ValueError(\n                `Input ${inputIndex} is incompatible with layer ` +\n                `${this.name}: expected axis ${axis} of input shape to ` +\n                `have value ${value} but got shape ${xShape}.`);\n          }\n        }\n      }\n\n      // Check shape.\n      if (spec.shape != null) {\n        for (let i = 0; i < spec.shape.length; ++i) {\n          const specDim = spec.shape[i];\n          const dim = x.shape[i];\n          if (specDim != null && dim != null) {\n            if (specDim !== dim) {\n              throw new ValueError(\n                  `Input ${inputIndex} is incompatible with layer ` +\n                  `${this.name}: expected shape=${spec.shape}, ` +\n                  `found shape=${x.shape}.`);\n            }\n          }\n        }\n      }\n    }\n  }\n\n  /**\n   * This is where the layer's logic lives.\n   *\n   * @param inputs Input tensor, or list/tuple of input tensors.\n   * @param kwargs Additional keyword arguments.\n   *\n   * @return A tensor or list/tuple of tensors.\n   */\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return inputs;\n  }\n\n  protected invokeCallHook(inputs: Tensor|Tensor[], kwargs: Kwargs) {\n    if (this._callHook != null) {\n      this._callHook(inputs, kwargs);\n    }\n  }\n\n  /**\n   * Set call hook.\n   * This is currently used for testing only.\n   * @param callHook\n   */\n  setCallHook(callHook: CallHook) {\n    this._callHook = callHook;\n  }\n\n  /**\n   * Clear call hook.\n   * This is currently used for testing only.\n   */\n  clearCallHook() {\n    this._callHook = null;\n  }\n\n  /**\n   * Builds or executes a `Layer`'s logic.\n   *\n   * When called with `tf.Tensor`(s), execute the `Layer`'s computation and\n   * return Tensor(s). For example:\n   *\n   * ```js\n   * const denseLayer = tf.layers.dense({\n   *   units: 1,\n   *   kernelInitializer: 'zeros',\n   *   useBias: false\n   * });\n   *\n   * // Invoke the layer's apply() method with a `tf.Tensor` (with concrete\n   * // numeric values).\n   * const input = tf.ones([2, 2]);\n   * const output = denseLayer.apply(input);\n   *\n   * // The output's value is expected to be [[0], [0]], due to the fact that\n   * // the dense layer has a kernel initialized to all-zeros and does not have\n   * // a bias.\n   * output.print();\n   * ```\n   *\n   * When called with `tf.SymbolicTensor`(s), this will prepare the layer for\n   * future execution.  This entails internal book-keeping on shapes of\n   * expected Tensors, wiring layers together, and initializing weights.\n   *\n   * Calling `apply` with `tf.SymbolicTensor`s are typically used during the\n   * building of non-`tf.Sequential` models. For example:\n   *\n   * ```js\n   * const flattenLayer = tf.layers.flatten();\n   * const denseLayer = tf.layers.dense({units: 1});\n   *\n   * // Use tf.layers.input() to obtain a SymbolicTensor as input to apply().\n   * const input = tf.input({shape: [2, 2]});\n   * const output1 = flattenLayer.apply(input);\n   *\n   * // output1.shape is [null, 4]. The first dimension is the undetermined\n   * // batch size. The second dimension comes from flattening the [2, 2]\n   * // shape.\n   * console.log(JSON.stringify(output1.shape));\n   *\n   * // The output SymbolicTensor of the flatten layer can be used to call\n   * // the apply() of the dense layer:\n   * const output2 = denseLayer.apply(output1);\n   *\n   * // output2.shape is [null, 1]. The first dimension is the undetermined\n   * // batch size. The second dimension matches the number of units of the\n   * // dense layer.\n   * console.log(JSON.stringify(output2.shape));\n   *\n   * // The input and output can be used to construct a model that consists\n   * // of the flatten and dense layers.\n   * const model = tf.model({inputs: input, outputs: output2});\n   * ```\n   *\n   * @param inputs a `tf.Tensor` or `tf.SymbolicTensor` or an Array of them.\n   * @param kwargs Additional keyword arguments to be passed to `call()`.\n   *\n   * @return Output of the layer's `call` method.\n   *\n   * @exception ValueError error in case the layer is missing shape information\n   *   for its `build` call.\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  // Porting Note: This is a replacement for __call__() in Python.\n  apply(\n      inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n      kwargs?: Kwargs): Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[] {\n    kwargs = kwargs || {};\n\n    this.assertNotDisposed();\n\n    // Ensure inputs are all the same type.\n    const inputsList = generic_utils.toList(inputs);\n\n    const allAreSymbolic = checkAllSymbolic(inputs);\n    const noneAreSymbolic = checkNoneSymbolic(inputs);\n\n    if (allAreSymbolic === noneAreSymbolic) {\n      throw new ValueError(\n          'Arguments to apply() must be all ' +\n          'SymbolicTensors or all Tensors');\n    }\n\n    // TODO(michaelterry): nameScope() may not be necessary.\n    return nameScope(this.name, () => {\n      // Handle laying building (weight creating, input spec locking).\n      if (!this.built) {\n        /*\n          Throw exceptions in case the input is not compatible\n          with the inputSpec specified in the layer constructor.\n         */\n        this.assertInputCompatibility(inputs);\n\n        // Collect input shapes to build layer.\n        const inputShapes: Shape[] = [];\n        for (const xElem of generic_utils.toList(inputs)) {\n          inputShapes.push(xElem.shape);\n        }\n        this.build(generic_utils.singletonOrArray(inputShapes));\n        this.built = true;\n\n        // Load weights that were specified at layer instantiation.\n        if (this.initialWeights) {\n          this.setWeights(this.initialWeights);\n        }\n\n        if (this._refCount === null && noneAreSymbolic) {\n          // The first use of this layer is a non-symbolic call, set ref count\n          // to 1 so the Layer can be properly disposed if its dispose() method\n          // is called.\n          this._refCount = 1;\n        }\n      }\n\n      /*\n        Throw exceptions in case the input is not compatible\n        with the inputSpec set at build time.\n      */\n      this.assertInputCompatibility(inputs);\n\n      // Handle mask propagation.\n      // TODO(michaelterry): Mask propagation not currently implemented.\n\n      // Actually call the layer, collecting output(s), mask(s), and shape(s).\n      if (noneAreSymbolic) {\n        let output = this.call(inputs, kwargs);\n\n        // Apply masks to the output tensors if the layer supports it.\n        if (this.supportsMasking) {\n          // TODO(mattsoulanille): pass the input tensors' masks to computeMask\n          this.setMaskMetadata(inputs, output);\n        }\n\n        // If the layer returns tensors from its inputs, unmodified,\n        // we copy them to avoid loss of tensor metadata.\n        const outputList: Tensor[] = generic_utils.toList(output);\n        const outputListCopy: Tensor[] = [];\n        // TODO(michaelterry): This copying may not be necessary given our eager\n        // backend.\n        for (let x of outputList) {\n          if (inputsList.indexOf(x) !== -1) {\n            x = x.clone();\n          }\n          outputListCopy.push(x);\n        }\n        output = generic_utils.singletonOrArray(outputListCopy);\n\n        if (this.activityRegularizer != null) {\n          throw new NotImplementedError(\n              'Layer invocation in the presence of activity ' +\n              'regularizer(s) is not supported yet.');\n        }\n\n        // TODO(michaelterry): Call addInboundNode()?\n        return output;\n      } else {\n        const inputShape = collectInputShape(inputs);\n        const outputShape = this.computeOutputShape(inputShape);\n        let output: SymbolicTensor|SymbolicTensor[];\n        const outputDType = guessOutputDType(inputs);\n        this.warnOnIncompatibleInputShape(\n            Array.isArray(inputs) ? inputShape[0] as Shape :\n                                    inputShape as Shape);\n\n        if (outputShape != null && outputShape.length > 0 &&\n            Array.isArray(outputShape[0])) {\n          // We have multiple output shapes. Create multiple output tensors.\n          output = (outputShape as Shape[])\n                       .map(\n                           (shape, index) => new SymbolicTensor(\n                               outputDType, shape, this,\n                               generic_utils.toList(inputs), kwargs, this.name,\n                               index));\n        } else {\n          output = new SymbolicTensor(\n              outputDType, outputShape as Shape, this,\n              generic_utils.toList(inputs), kwargs, this.name);\n        }\n\n        /*\n          Add an inbound node to the layer, so that it keeps track\n          of the call and of all new variables created during the call.\n          This also updates the layer history of the output tensor(s).\n          If the input tensor(s) had no previous history,\n          this does nothing.\n        */\n        this.addInboundNode(\n            inputs, output, null, null, inputShape, outputShape, kwargs);\n        this._refCount++;\n\n        if (this.activityRegularizer != null) {\n          throw new NotImplementedError(\n              'Layer invocation in the presence of activity ' +\n              'regularizer(s) is not supported yet.');\n        }\n\n        return output;\n      }\n    });\n  }\n\n  /**\n   * Check compatibility between input shape and this layer's batchInputShape.\n   *\n   * Print warning if any incompatibility is found.\n   *\n   * @param inputShape Input shape to be checked.\n   */\n  protected warnOnIncompatibleInputShape(inputShape: Shape) {\n    if (this.batchInputShape == null) {\n      return;\n    } else if (inputShape.length !== this.batchInputShape.length) {\n      console.warn(\n          `The rank of the input tensor provided (shape: ` +\n          `${JSON.stringify(inputShape)}) does not match that of the ` +\n          `batchInputShape (${JSON.stringify(this.batchInputShape)}) ` +\n          `of the layer ${this.name}`);\n    } else {\n      let dimMismatch = false;\n      this.batchInputShape.forEach((dimension, i) => {\n        if (dimension != null && inputShape[i] != null &&\n            inputShape[i] !== dimension) {\n          dimMismatch = true;\n        }\n      });\n      if (dimMismatch) {\n        console.warn(\n            `The shape of the input tensor ` +\n            `(${JSON.stringify(inputShape)}) does not ` +\n            `match the expectation of layer ${this.name}: ` +\n            `${JSON.stringify(this.batchInputShape)}`);\n      }\n    }\n  }\n\n  /**\n   * Retrieves the output shape(s) of a layer.\n   *\n   * Only applicable if the layer has only one inbound node, or if all inbound\n   * nodes have the same output shape.\n   *\n   * @returns Output shape or shapes.\n   * @throws AttributeError: if the layer is connected to more than one incoming\n   *   nodes.\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  get outputShape(): Shape|Shape[] {\n    if (this.inboundNodes == null || this.inboundNodes.length === 0) {\n      throw new AttributeError(\n          `The layer ${this.name} has never been called and thus has no ` +\n          `defined output shape.`);\n    }\n    const allOutputShapes: string[] = [];\n    for (const node of this.inboundNodes) {\n      const shapeString = JSON.stringify(node.outputShapes);\n      if (allOutputShapes.indexOf(shapeString) === -1) {\n        allOutputShapes.push(shapeString);\n      }\n    }\n    if (allOutputShapes.length === 1) {\n      const outputShapes = this.inboundNodes[0].outputShapes;\n      if (Array.isArray(outputShapes) && Array.isArray(outputShapes[0]) &&\n          outputShapes.length === 1) {\n        return (outputShapes as Shape[])[0];\n      } else {\n        return outputShapes;\n      }\n\n    } else {\n      throw new AttributeError(\n          `The layer ${this.name} has multiple inbound nodes with different ` +\n          `output shapes. Hence the notion of \"output shape\" is ill-defined ` +\n          `for the layer.`);\n      // TODO(cais): Implement getOutputShapeAt().\n    }\n  }\n\n  /**\n   * Counts the total number of numbers (e.g., float32, int32) in the\n   * weights.\n   *\n   * @returns An integer count.\n   * @throws RuntimeError: If the layer is not built yet (in which case its\n   *   weights are not defined yet.)\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  countParams(): number {\n    if (!this.built) {\n      throw new RuntimeError(\n          `You tried to call countParams() on ${this.name}, ` +\n          `but the layer is not built yet. Build it first by calling ` +\n          `build(batchInputShape).`);\n    }\n    return variable_utils.countParamsInWeights(this.weights);\n  }\n\n  /**\n   * Creates the layer weights.\n   *\n   * Must be implemented on all layers that have weights.\n   *\n   * Called when apply() is called to construct the weights.\n   *\n   * @param inputShape A `Shape` or array of `Shape` (unused).\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  build(inputShape: Shape|Shape[]) {\n    this.built = true;\n  }\n\n  /**\n   * Returns the current values of the weights of the layer.\n   *\n   * @param trainableOnly Whether to get the values of only trainable weights.\n   * @returns Weight values as an `Array` of `tf.Tensor`s.\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  getWeights(trainableOnly = false): Tensor[] {\n    return batchGetValue(trainableOnly ? this.trainableWeights : this.weights);\n  }\n\n  /**\n   * Sets the weights of the layer, from Tensors.\n   *\n   * @param weights a list of Tensors. The number of arrays and their shape\n   *   must match number of the dimensions of the weights of the layer (i.e.\n   *   it should match the output of `getWeights`).\n   *\n   * @exception ValueError If the provided weights list does not match the\n   *   layer's specifications.\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  setWeights(weights: Tensor[]): void {\n    tidy(() => {\n      const params = this.weights;\n      if (params.length !== weights.length) {\n        // TODO(cais): Restore the following and use `providedWeights`, instead\n        // of `weights` in the error message, once the deeplearn.js bug is\n        // fixed: https://github.com/PAIR-code/deeplearnjs/issues/498 const\n        // providedWeights = JSON.stringify(weights).slice(0, 50);\n        throw new ValueError(\n            `You called setWeights(weights) on layer \"${this.name}\" ` +\n            `with a weight list of length ${weights.length}, ` +\n            `but the layer was expecting ${params.length} weights. ` +\n            `Provided weights: ${weights}...`);\n      }\n      if (params.length === 0) {\n        return;\n      }\n      const weightValueTuples: Array<[LayerVariable, Tensor]> = [];\n      const paramValues = batchGetValue(params);\n      for (let i = 0; i < paramValues.length; ++i) {\n        const pv = paramValues[i];\n        const p = params[i];\n        const w = weights[i];\n        if (!util.arraysEqual(pv.shape, w.shape)) {\n          throw new ValueError(\n              `Layer weight shape ${pv.shape} ` +\n              `not compatible with provided weight shape ${w.shape}`);\n        }\n        weightValueTuples.push([p, w]);\n      }\n      batchSetValue(weightValueTuples);\n    });\n  }\n\n  /**\n   * Adds a weight variable to the layer.\n   *\n   * @param name Name of the new weight variable.\n   * @param shape The shape of the weight.\n   * @param dtype The dtype of the weight.\n   * @param initializer An initializer instance.\n   * @param regularizer A regularizer instance.\n   * @param trainable Whether the weight should be trained via backprop or not\n   *   (assuming that the layer itself is also trainable).\n   * @param constraint An optional trainable.\n   * @return The created weight variable.\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  protected addWeight(\n      name: string, shape: Shape, dtype?: DataType, initializer?: Initializer,\n      regularizer?: Regularizer, trainable?: boolean, constraint?: Constraint,\n      getInitializerFunc?: Function): LayerVariable {\n    // Reject duplicate weight names.\n    if (this._addedWeightNames.indexOf(name) !== -1) {\n      throw new ValueError(\n          `Duplicate weight name ${name} for layer ${this.name}`);\n    }\n    this._addedWeightNames.push(name);\n\n    if (dtype == null) {\n      dtype = 'float32';\n    }\n\n    if (this.fastWeightInitDuringBuild) {\n      initializer = getInitializerFunc != null ? getInitializerFunc() :\n                                                 getInitializer('zeros');\n    }\n    const initValue = initializer.apply(shape, dtype);\n    const weight =\n        new LayerVariable(initValue, dtype, name, trainable, constraint);\n    initValue.dispose();\n    // Request backend not to dispose the weights of the model on scope() exit.\n    if (regularizer != null) {\n      this.addLoss(() => regularizer.apply(weight.read()));\n    }\n    if (trainable == null) {\n      trainable = true;\n    }\n    if (trainable) {\n      this._trainableWeights.push(weight);\n    } else {\n      this._nonTrainableWeights.push(weight);\n    }\n    return weight;\n  }\n\n  /**\n   * Set the fast-weight-initialization flag.\n   *\n   * In cases where the initialized weight values will be immediately\n   * overwritten by loaded weight values during model loading, setting\n   * the flag to `true` saves unnecessary calls to potentially expensive\n   * initializers and speeds up the loading process.\n   *\n   * @param value Target value of the flag.\n   */\n  setFastWeightInitDuringBuild(value: boolean) {\n    this.fastWeightInitDuringBuild = value;\n  }\n\n  /**\n   * Add losses to the layer.\n   *\n   * The loss may potentially be conditional on some inputs tensors,\n   * for instance activity losses are conditional on the layer's inputs.\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  addLoss(losses: RegularizerFn|RegularizerFn[]): void {\n    if (losses == null || Array.isArray(losses) && losses.length === 0) {\n      return;\n    }\n    // Update this.losses\n    losses = generic_utils.toList(losses);\n    if (this._losses !== undefined && this._losses !== null) {\n      this.losses.push(...losses);\n    }\n  }\n\n  /**\n   * Computes the output shape of the layer.\n   *\n   * Assumes that the layer will be built to match that input shape provided.\n   *\n   * @param inputShape A shape (tuple of integers) or a list of shape tuples\n   *   (one per output tensor of the layer). Shape tuples can include null for\n   *   free dimensions, instead of an integer.\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  /**\n   * Computes an output mask tensor.\n   *\n   * @param inputs Tensor or list of tensors.\n   * @param mask Tensor or list of tensors.\n   *\n   * @return null or a tensor (or list of tensors, one per output tensor of the\n   * layer).\n   */\n  computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor\n      |Tensor[] {\n    if (!this.supportsMasking) {\n      if (mask != null) {\n        if (Array.isArray(mask)) {\n          mask.forEach(maskElement => {\n            if (maskElement != null) {\n              throw new TypeError(\n                  `Layer ${this.name} does not support masking, ` +\n                  'but was passed an inputMask.');\n            }\n          });\n        } else {\n          throw new TypeError(\n              `Layer ${this.name} does not support masking, ` +\n              'but was passed an inputMask.');\n        }\n      }\n      // masking not explicitly supported: return null as mask\n      return null;\n    }\n    // if masking is explictly supported, by default\n    // carry over the input mask\n    return mask;\n  }\n\n  private setMaskMetadata(\n      inputs: Tensor|Tensor[], outputs: Tensor|Tensor[],\n      previousMask?: Tensor|Tensor[]): void {\n    if (!this.supportsMasking) {\n      return;\n    }\n\n    const outputMasks = this.computeMask(inputs, previousMask);\n    const outputsList = generic_utils.toList(outputs);\n    const outputMasksList = generic_utils.toList(outputMasks);\n\n    if (outputsList.length !== outputMasksList.length) {\n      throw new Error(\n          `${this.name} outputs ${outputsList.length} tensors ` +\n          `but ${outputsList.length} masks for those tensors`);\n    }\n    for (let i = 0; i < outputsList.length; i++) {\n      outputsList[i].kerasMask = outputMasksList[i];\n    }\n  }\n\n  /**\n   * Internal method to create an inbound node for the layer.\n   *\n   * @param inputTensors List of input tensors.\n   * @param outputTensors List of output tensors.\n   * @param inputMasks List of input masks (a mask can be a tensor, or null).\n   * @param outputMasks List of output masks (a mask can be a tensor, or null).\n   * @param inputShapes List of input shape tuples.\n   * @param outputShapes List of output shape tuples.\n   * @param kwargs Dictionary of keyword arguments that were passed to the\n   *   `call` method of the layer at the call that created the node.\n   */\n  private addInboundNode(\n      inputTensors: SymbolicTensor|SymbolicTensor[],\n      outputTensors: SymbolicTensor|SymbolicTensor[],\n      inputMasks: Tensor|Tensor[], outputMasks: Tensor|Tensor[],\n      inputShapes: Shape|Shape[], outputShapes: Shape|Shape[],\n      kwargs: {} = null): void {\n    const inputTensorList: SymbolicTensor[] =\n        generic_utils.toList(inputTensors);\n    outputTensors = generic_utils.toList(outputTensors);\n    inputMasks = generic_utils.toList(inputMasks);\n    outputMasks = generic_utils.toList(outputMasks);\n    inputShapes = types_utils.normalizeShapeList(inputShapes);\n    outputShapes = types_utils.normalizeShapeList(outputShapes);\n\n    // Collect input tensor(s) coordinates.\n    const inboundLayers: Layer[] = [];\n    const nodeIndices: number[] = [];\n    const tensorIndices: number[] = [];\n    for (const x of inputTensorList) {\n      /*\n       * TODO(michaelterry): Keras adds this value to tensors; it's not\n       * clear whether we'll use this or not.\n       */\n      inboundLayers.push(x.sourceLayer);\n      nodeIndices.push(x.nodeIndex);\n      tensorIndices.push(x.tensorIndex);\n    }\n\n    // Create node, add it to inbound nodes.\n    // (This call has side effects.)\n    // tslint:disable-next-line:no-unused-expression\n    new Node(\n        {\n          outboundLayer: this,\n          inboundLayers,\n          nodeIndices,\n          tensorIndices,\n          inputTensors: inputTensorList,\n          outputTensors,\n          inputMasks,\n          outputMasks,\n          inputShapes,\n          outputShapes\n        },\n        kwargs);\n\n    // Update tensor history\n    for (let i = 0; i < outputTensors.length; i++) {\n      // TODO(michaelterry: _uses_learning_phase not tracked.\n      outputTensors[i].sourceLayer = this;\n      outputTensors[i].nodeIndex = this.inboundNodes.length - 1;\n      outputTensors[i].tensorIndex = i;\n    }\n  }\n\n  /**\n   * Returns the config of the layer.\n   *\n   * A layer config is a TS dictionary (serializable)\n   * containing the configuration of a layer.\n   * The same layer can be reinstantiated later\n   * (without its trained weights) from this configuration.\n   *\n   * The config of a layer does not include connectivity\n   * information, nor the layer class name.  These are handled\n   * by 'Container' (one layer of abstraction above).\n   *\n   * Porting Note: The TS dictionary follows TS naming standards for\n   * keys, and uses tfjs-layers type-safe Enums.  Serialization methods\n   * should use a helper function to convert to the pythonic storage\n   * standard. (see serialization_utils.convertTsToPythonic)\n   *\n   * @returns TS dictionary of configuration.\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  getConfig(): serialization.ConfigDict {\n    const config:\n        serialization.ConfigDict = {name: this.name, trainable: this.trainable};\n    if (this.batchInputShape != null) {\n      config['batchInputShape'] = this.batchInputShape;\n    }\n    if (this.dtype != null) {\n      config['dtype'] = this.dtype;\n    }\n    return config;\n  }\n\n  /**\n   * Dispose the weight variables that this Layer instance holds.\n   *\n   * @returns {number} Number of disposed variables.\n   */\n  protected disposeWeights(): number {\n    this.weights.forEach(weight => weight.dispose());\n    return this.weights.length;\n  }\n\n  protected assertNotDisposed() {\n    if (this._refCount === 0) {\n      throw new Error(`Layer '${this.name}' is already disposed.`);\n    }\n  }\n\n  /**\n   * Attempt to dispose layer's weights.\n   *\n   * This method decreases the reference count of the Layer object by 1.\n   *\n   * A Layer is reference-counted. Its reference count is incremented by 1\n   * the first item its `apply()` method is called and when it becomes a part\n   * of a new `Node` (through calling the `apply()` method on a\n   * `tf.SymbolicTensor`).\n   *\n   * If the reference count of a Layer becomes 0, all the weights will be\n   * disposed and the underlying memory (e.g., the textures allocated in WebGL)\n   * will be freed.\n   *\n   * Note: If the reference count is greater than 0 after the decrement, the\n   * weights of the Layer will *not* be disposed.\n   *\n   * After a Layer is disposed, it cannot be used in calls such as `apply()`,\n   * `getWeights()` or `setWeights()` anymore.\n   *\n   * @returns A DisposeResult Object with the following fields:\n   *   - refCountAfterDispose: The reference count of the Container after this\n   *     `dispose()` call.\n   *   - numDisposedVariables: Number of `tf.Variable`s (i.e., weights) disposed\n   *     during this `dispose()` call.\n   * @throws {Error} If the layer is not built yet, or if the layer has already\n   *   been disposed.\n   *\n   * @doc {heading: 'Models', 'subheading': 'Classes'}\n   */\n  dispose(): DisposeResult {\n    if (!this.built) {\n      throw new Error(\n          `Cannot dispose Layer ${this.name} because it has not been ` +\n          `built yet.`);\n    }\n\n    if (this._refCount === null) {\n      throw new Error(\n          `Cannot dispose Layer ${this.name} because it has not been used ` +\n          `yet.`);\n    }\n\n    this.assertNotDisposed();\n\n    let numDisposedVariables = 0;\n    if (--this._refCount === 0) {\n      numDisposedVariables = this.disposeWeights();\n    }\n\n    return {refCountAfterDispose: this._refCount, numDisposedVariables};\n  }\n}\n\n/**\n * Collects the input shape(s) of a list of `tf.Tensor`s or\n * `tf.SymbolicTensor`s.\n *\n * TODO(michaelterry): Update PyKeras docs (backport).\n *\n * @param inputTensors List of input tensors (or single input tensor).\n *\n * @return List of shape tuples (or single tuple), one tuple per input.\n */\nfunction collectInputShape(inputTensors: SymbolicTensor|SymbolicTensor[]|Tensor|\n                           Tensor[]): Shape|Shape[] {\n  inputTensors =\n      generic_utils.toList(inputTensors) as SymbolicTensor[] | Tensor[];\n  const shapes: Shape[] = [];\n  for (const x of inputTensors) {\n    shapes.push(x.shape);\n  }\n  return generic_utils.singletonOrArray(shapes);\n}\n\n/**\n * Guesses output dtype based on inputs.\n *\n * At present, just returns 'float32' for any input.\n *\n * @param inputTensors List of input tensors (or single input tensor).\n *\n * @return The guessed DType. At present, always returns 'float32'.\n */\nfunction guessOutputDType(inputTensors: SymbolicTensor|SymbolicTensor[]|Tensor|\n                          Tensor[]): DataType {\n  return 'float32';\n}\n\n/**\n * Returns the list of input tensors necessary to compute `tensor`.\n *\n * Output will always be a list of tensors (potentially with 1 element).\n *\n * @param tensor The tensor to start from.\n * @param layer Origin layer of the tensor.\n * @param nodeIndex Origin node index of the tensor.\n *\n * @return Array of input tensors.\n */\nexport function getSourceInputs(\n    tensor: SymbolicTensor, layer?: Layer,\n    nodeIndex?: number): SymbolicTensor[] {\n  if (layer == null || (nodeIndex != null && nodeIndex > 0)) {\n    layer = tensor.sourceLayer;\n    nodeIndex = tensor.nodeIndex;\n  }\n  if (layer.inboundNodes.length === 0) {\n    return [tensor];\n  } else {\n    const node = layer.inboundNodes[nodeIndex];\n    if (node.inboundLayers.length === 0) {\n      return node.inputTensors;\n    } else {\n      const sourceTensors: SymbolicTensor[] = [];\n      for (let i = 0; i < node.inboundLayers.length; i++) {\n        const x = node.inputTensors[i];\n        const layer = node.inboundLayers[i];\n        const nodeIndex = node.nodeIndices[i];\n        const previousSources = getSourceInputs(x, layer, nodeIndex);\n        // Avoid input redundancy.\n        for (const x of previousSources) {\n          if (sourceTensors.indexOf(x) === -1) {\n            sourceTensors.push(x);\n          }\n        }\n      }\n      return sourceTensors;\n    }\n  }\n}\n\ntype MaybeSymbolic = SymbolicTensor|Tensor;\n\nfunction checkAllSymbolic(tensors: MaybeSymbolic|MaybeSymbolic[]):\n    tensors is SymbolicTensor|SymbolicTensor[] {\n  let allAreSymbolic = true;\n  for (const tensor of generic_utils.toList(tensors)) {\n    if (!(tensor instanceof SymbolicTensor)) {\n      allAreSymbolic = false;\n      break;\n    }\n  }\n  return allAreSymbolic;\n}\n\nfunction checkNoneSymbolic(tensors: MaybeSymbolic|\n                           MaybeSymbolic[]): tensors is Tensor|Tensor[] {\n  let noneAreSymbolic = true;\n  for (const tensor of generic_utils.toList(tensors)) {\n    if (tensor instanceof SymbolicTensor) {\n      noneAreSymbolic = false;\n      break;\n    }\n  }\n  return noneAreSymbolic;\n}\n"],"names":[],"mappings":";;;;;;;;;;;;AAAA;;;;;;;;GAQG,CAEH,6CAAA,EAA+C;;;AAE/C,OAAO,EAAmB,aAAa,EAAU,IAAI,EAAE,IAAI,EAAC,MAAM,uBAAuB,CAAC;AAE1F,OAAO,EAAC,qBAAqB,EAAE,MAAM,EAAC,MAAM,kBAAkB,CAAC;AAC/D,OAAO,EAAC,mBAAmB,EAAE,mBAAmB,EAAE,SAAS,EAAC,MAAM,WAAW,CAAC;AAE9E,OAAO,EAAC,cAAc,EAAE,mBAAmB,EAAE,YAAY,EAAE,UAAU,EAAC,MAAM,WAAW,CAAC;AACxF,OAAO,EAAC,cAAc,EAAc,MAAM,iBAAiB,CAAC;AAI5D,OAAO,KAAK,aAAa,MAAM,wBAAwB,CAAC;AACxD,OAAO,KAAK,WAAW,MAAM,sBAAsB,CAAC;AACpD,OAAO,KAAK,cAAc,MAAM,yBAAyB,CAAC;AAC1D,OAAO,EAAC,aAAa,EAAE,aAAa,EAAE,aAAa,EAAC,MAAM,cAAc,CAAC;;;;;;;;;;AAgCnE,MAAO,SAAS;IAcpB,YAAY,IAAmB,CAAA;QAC7B,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,CAAC;QACxB,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,CAAC;QACxB;;;UAGE,CACF,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;YACtB,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,KAAK,CAAC,MAAM,CAAC;SAC/B,MAAM;YACL,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;SACvB;QACD,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,CAAC;QAC5B,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,CAAC;QAC5B,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,CAAA,CAAE,CAAC;IAC9B,CAAC;CACF;AAUK,MAAO,cAAc;IAsBzB;;;;;;;;;;;;OAYG,CACH,YACa,KAAe,EAAW,KAAY,EACxC,WAAkB,EAAW,MAAwB,EACnD,QAAgB,EAAE,IAAa,EAC/B,iBAA0B,CAAA;QAH1B,IAAA,CAAA,KAAK,GAAL,KAAK,CAAU;QAAW,IAAA,CAAA,KAAK,GAAL,KAAK,CAAO;QACxC,IAAA,CAAA,WAAW,GAAX,WAAW,CAAO;QAAW,IAAA,CAAA,MAAM,GAAN,MAAM,CAAkB;QACnD,IAAA,CAAA,QAAQ,GAAR,QAAQ,CAAQ;QAChB,IAAA,CAAA,iBAAiB,GAAjB,iBAAiB,CAAS;QACrC,IAAI,CAAC,EAAE,OAAG,8TAAqB,EAAE,CAAC;QAClC,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,CAAC,YAAY,OAAG,kTAAmB,EAAC,IAAI,CAAC,CAAC;YAC9C,IAAI,CAAC,IAAI,OAAG,kTAAmB,EAAC,IAAI,CAAC,YAAY,CAAC,CAAC;SACpD;QACD,IAAI,CAAC,IAAI,GAAG,KAAK,CAAC,MAAM,CAAC;IAC3B,CAAC;CACF;AA2DD,IAAI,WAAW,GAAG,CAAC,CAAC;AAsBd,MAAO,IAAI;IAwCf,YACI,IAAc,EACd,mDAAmD;IAC5C,QAAiB,CAAA;QAAjB,IAAA,CAAA,QAAQ,GAAR,QAAQ,CAAS;QAC1B,IAAI,CAAC,EAAE,GAAG,WAAW,EAAE,CAAC;QACxB;;;;;;UAME,CACF,IAAI,CAAC,aAAa,GAAG,IAAI,CAAC,aAAa,CAAC;QAExC;;;;;UAKE,CAEF,2BAA2B;QAC3B,IAAI,CAAC,aAAa,GAAG,IAAI,CAAC,aAAa,CAAC;QACxC,oDAAoD;QACpD,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC,WAAW,CAAC;QACpC,oDAAoD;QACpD,IAAI,CAAC,aAAa,GAAG,IAAI,CAAC,aAAa,CAAC;QAExC;;;UAGE,CAEF,mDAAmD;QACnD,IAAI,CAAC,YAAY,GAAG,IAAI,CAAC,YAAY,CAAC;QACtC,oDAAoD;QACpD,IAAI,CAAC,aAAa,GAAG,IAAI,CAAC,aAAa,CAAC;QAExC;;;UAGE,CACF,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,UAAU,CAAC;QAClC,2DAA2D;QAC3D,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC,WAAW,CAAC;QAEpC,mDAAmD;QAEnD,gDAAgD;QAChD,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC,WAAW,CAAC;QACpC,iDAAiD;QACjD,IAAI,CAAC,YAAY,GAAG,IAAI,CAAC,YAAY,CAAC;QAEtC,oCAAoC;QACpC,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,aAAa,CAAE;YACtC,IAAI,KAAK,IAAI,IAAI,EAAE;gBACjB,KAAK,CAAC,aAAa,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;aAChC;SACF;QACD,IAAI,CAAC,aAAa,CAAC,YAAY,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;IAC7C,CAAC;IAED,SAAS,GAAA;QACP,MAAM,YAAY,GAAa,EAAE,CAAC;QAClC,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,aAAa,CAAE;YACtC,IAAI,KAAK,IAAI,IAAI,EAAE;gBACjB,YAAY,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;aAC/B,MAAM;gBACL,YAAY,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;aACzB;SACF;QACD,OAAO;YACL,aAAa,EAAE,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,IAAI,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI;YAClE,aAAa,EAAE,YAAY;YAC3B,WAAW,EAAE,IAAI,CAAC,WAAW;YAC7B,aAAa,EAAE,IAAI,CAAC,aAAa;SAClC,CAAC;IACJ,CAAC;CACF;AAkDD,IAAI,YAAY,GAAG,CAAC,CAAC;AAWf,MAAgB,KAAM,SAAQ,ySAAa,CAAC,YAAY;IAmD5D,YAAY,OAAkB,CAAA,CAAE,CAAA;QAC9B,KAAK,EAAE,CAAC;QAtBF,IAAA,CAAA,SAAS,GAAa,IAAI,CAAC;QAE3B,IAAA,CAAA,iBAAiB,GAAa,EAAE,CAAC;QAIzC,wEAAwE;QACxE,yEAAyE;QACzE,0EAA0E;QAC1E,gBAAgB;QACN,IAAA,CAAA,SAAS,GAAG,KAAK,CAAC;QAa1B,IAAI,CAAC,EAAE,GAAG,YAAY,EAAE,CAAC;QAEzB,IAAI,CAAC,mBAAmB,GAAG,IAAI,CAAC;QAEhC,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC;QACtB,IAAI,CAAC,eAAe,GAAG,KAAK,CAAC;QAE7B,yDAAyD;QACzD,IAAI,CAAC,iBAAiB,GAAG,EAAE,CAAC;QAC5B,IAAI,CAAC,oBAAoB,GAAG,EAAE,CAAC;QAC/B,IAAI,CAAC,OAAO,GAAG,EAAE,CAAC;QAClB,IAAI,CAAC,QAAQ,GAAG,EAAE,CAAC;QACnB,IAAI,CAAC,MAAM,GAAG,KAAK,CAAC;QAEpB;;;WAGG,CACH,IAAI,CAAC,YAAY,GAAG,EAAE,CAAC;QACvB,IAAI,CAAC,aAAa,GAAG,EAAE,CAAC;QAExB,IAAI,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;QACrB,IAAI,CAAC,IAAI,EAAE;YACT,MAAM,MAAM,GAAG,IAAI,CAAC,YAAY,EAAE,CAAC;YACnC,IAAI,GAAG,aAAa,CAAC,4SAAW,CAAC,MAAM,CAAC,GAAG,GAAG,OAAG,+SAAM,EAAC,MAAM,CAAC,CAAC;SACjE;QACD,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC;QAEjB,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,SAAS,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,SAAS,CAAC;QAEjE,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,IAAI,IAAI,CAAC,eAAe,IAAI,IAAI,EAAE;YAC3D;;;eAGG,CACH,IAAI,eAAsB,CAAC;YAC3B,IAAI,IAAI,CAAC,eAAe,IAAI,IAAI,EAAE;gBAChC,eAAe,GAAG,IAAI,CAAC,eAAe,CAAC;aACxC,MAAM,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;gBAClC,IAAI,SAAS,GAAW,IAAI,CAAC;gBAC7B,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;oBAC1B,SAAS,GAAG,IAAI,CAAC,SAAS,CAAC;iBAC5B;gBACD,eAAe,GAAG;oBAAC,SAAS;iBAAC,CAAC,MAAM,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;aACvD;YACD,IAAI,CAAC,eAAe,GAAG,eAAe,CAAC;YAEvC,aAAa;YACb,IAAI,KAAK,GAAG,IAAI,CAAC,KAAK,CAAC;YACvB,IAAI,KAAK,IAAI,IAAI,EAAE;gBACjB,KAAK,GAAG,IAAI,CAAC,UAAU,CAAC;aACzB;YACD,IAAI,KAAK,IAAI,IAAI,EAAE;gBACjB,KAAK,GAAG,SAAS,CAAC;aACnB;YACD,IAAI,CAAC,KAAK,GAAG,KAAK,CAAC;SACpB;QAED,IAAI,IAAI,CAAC,OAAO,IAAI,IAAI,EAAE;YACxB,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,OAAO,CAAC;SACpC,MAAM;YACL,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC;SAC5B;QAED,0EAA0E;QAC1E,6DAA6D;QAC7D,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC;QAEtB,IAAI,CAAC,yBAAyB,GAAG,KAAK,CAAC;IACzC,CAAC;IAED;;;;;;;;OAQG,CACO,MAAM,CAAC,OAAO,CAAC,KAAY,EAAE,SAAiB,EAAA;QACtD,OAAO,KAAK,CAAC,IAAI,GAAG,MAAM,GAAG,SAAS,CAAC,QAAQ,EAAE,CAAC;IACpD,CAAC;IAED;;;;;;OAMG,CACK,cAAc,CAAC,SAAiB,EAAE,QAAgB,EAAA;QACxD,IAAI,IAAI,CAAC,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;YAClC,MAAM,IAAI,2SAAY,CAClB,kCAAkC,GAClC,CAAA,wBAAA,EAA2B,QAAQ,CAAA,CAAA,CAAG,CAAC,CAAC;SAC7C;QACD,IAAI,IAAI,CAAC,YAAY,CAAC,MAAM,IAAI,SAAS,EAAE;YACzC,MAAM,IAAI,ySAAU,CAChB,CAAA,aAAA,EAAgB,QAAQ,CAAA,SAAA,EAAY,SAAS,CAAA,EAAA,CAAI,GACjD,CAAA,uBAAA,EAA0B,IAAI,CAAC,YAAY,CAAC,MAAM,CAAA,eAAA,CAAiB,CAAC,CAAC;SAC1E;QACD,OAAO,IAAI,CAAC,YAAY,CAAC,SAAS,CAAC,CAAC;IACtC,CAAC;IAED;;;;;;;;OAQG,CACH,UAAU,CAAC,SAAiB,EAAA;QAC1B,OAAO,aAAa,CAAC,iTAAgB,CACjC,IAAI,CAAC,cAAc,CAAC,SAAS,EAAE,OAAO,CAAC,CAAC,YAAY,CAAC,CAAC;IAC5D,CAAC;IAED;;;;;;;;OAQG,CACH,WAAW,CAAC,SAAiB,EAAA;QAC3B,OAAO,aAAa,CAAC,iTAAgB,CACjC,IAAI,CAAC,cAAc,CAAC,SAAS,EAAE,QAAQ,CAAC,CAAC,aAAa,CAAC,CAAC;IAC9D,CAAC;IAED,aAAa;IAEb;;;;;;;;;;OAUG,CACH,IAAI,KAAK,GAAA;QACP,IAAI,IAAI,CAAC,YAAY,CAAC,MAAM,GAAG,CAAC,EAAE;YAChC,MAAM,IAAI,6SAAc,CACpB,CAAA,MAAA,EAAS,IAAI,CAAC,IAAI,EAAE,GACpB,+BAA+B,GAC/B,oCAAoC,GACpC,kBAAkB,GAClB,sCAAsC,CAAC,CAAC;SAC7C,MAAM,IAAI,IAAI,CAAC,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;YACzC,MAAM,IAAI,6SAAc,CACpB,CAAA,MAAA,EAAS,IAAI,CAAC,IAAI,EAAE,GACpB,wCAAwC,CAAC,CAAC;SAC/C;QACD,OAAO,aAAa,CAAC,iTAAgB,CACjC,IAAI,CAAC,cAAc,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC,YAAY,CAAC,CAAC;IACpD,CAAC;IAED;;;;;;;;;;OAUG,CACH,IAAI,MAAM,GAAA;QACR,IAAI,IAAI,CAAC,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;YAClC,MAAM,IAAI,6SAAc,CACpB,CAAA,MAAA,EAAS,IAAI,CAAC,IAAI,EAAE,GACpB,wBAAwB,CAAC,CAAC;SAC/B;QACD,IAAI,IAAI,CAAC,YAAY,CAAC,MAAM,GAAG,CAAC,EAAE;YAChC,MAAM,IAAI,6SAAc,CACpB,CAAA,MAAA,EAAS,IAAI,CAAC,IAAI,EAAE,GACpB,+BAA+B,GAC/B,qCAAqC,GACrC,kBAAkB,GAClB,uCAAuC,CAAC,CAAC;SAC9C;QACD,OAAO,aAAa,CAAC,iTAAgB,CACjC,IAAI,CAAC,cAAc,CAAC,CAAC,EAAE,QAAQ,CAAC,CAAC,aAAa,CAAC,CAAC;IACtD,CAAC;IAED,IAAI,MAAM,GAAA;QACR,OAAO,IAAI,CAAC,OAAO,CAAC;IACtB,CAAC;IAED;;;;OAIG,CACH,eAAe,GAAA;QACb,kEAAkE;QAClE,qEAAqE;QACrE,yEAAyE;QACzE,wBAAwB;QACxB,OAAO,IAAI,CAAC,MAAM,CAAC,GAAG,EAAC,MAAM,CAAC,EAAE,AAAC,MAAM,EAAE,CAAC,CAAC;IAC7C,CAAC;IAED,IAAI,OAAO,GAAA;QACT,OAAO,IAAI,CAAC,QAAQ,CAAC;IACvB,CAAC;IAED,IAAI,KAAK,GAAA;QACP,OAAO,IAAI,CAAC,MAAM,CAAC;IACrB,CAAC;IAED,IAAI,KAAK,CAAC,KAAc,EAAA;QACtB,IAAI,CAAC,MAAM,GAAG,KAAK,CAAC;IACtB,CAAC;IAED,IAAI,SAAS,GAAA;QACX,OAAO,IAAI,CAAC,UAAU,CAAC;IACzB,CAAC;IAED,IAAI,SAAS,CAAC,SAAkB,EAAA;QAC9B,IAAI,CAAC,iBAAiB,CAAC,OAAO,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,SAAS,GAAG,SAAS,CAAC,CAAC;QAC7D,IAAI,CAAC,UAAU,GAAG,SAAS,CAAC;IAC9B,CAAC;IAED,IAAI,gBAAgB,GAAA;QAClB,IAAI,IAAI,CAAC,UAAU,EAAE;YACnB,OAAO,IAAI,CAAC,iBAAiB,CAAC,MAAM,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,SAAS,CAAC,CAAC;SACxD,MAAM;YACL,OAAO,EAAE,CAAC;SACX;IACH,CAAC;IAED,IAAI,gBAAgB,CAAC,OAAwB,EAAA;QAC3C,IAAI,CAAC,iBAAiB,GAAG,OAAO,CAAC;IACnC,CAAC;IAED,IAAI,mBAAmB,GAAA;QACrB,IAAI,IAAI,CAAC,SAAS,EAAE;YAClB,OAAO,IAAI,CAAC,iBAAiB,CAAC,MAAM,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,CAAC,SAAS,CAAC,CAClD,MAAM,CAAC,IAAI,CAAC,oBAAoB,CAAC,CAAC;SACxC,MAAM;YACL,OAAO,IAAI,CAAC,iBAAiB,CAAC,MAAM,CAAC,IAAI,CAAC,oBAAoB,CAAC,CAAC;SACjE;IACH,CAAC;IAED,IAAI,mBAAmB,CAAC,OAAwB,EAAA;QAC9C,IAAI,CAAC,oBAAoB,GAAG,OAAO,CAAC;IACtC,CAAC;IAED;;;OAGG,CACH,IAAI,OAAO,GAAA;QACT,OAAO,IAAI,CAAC,gBAAgB,CAAC,MAAM,CAAC,IAAI,CAAC,mBAAmB,CAAC,CAAC;IAChE,CAAC;IAED,IAAI,QAAQ,GAAA;QACV,OAAO,IAAI,CAAC,SAAS,CAAC;IACxB,CAAC;IAED;;;;;;OAMG,CACH,WAAW,GAAA;QACT,IAAI,CAAC,IAAI,CAAC,QAAQ,EAAE;YAClB,MAAM,IAAI,KAAK,CACX,+DAA+D,GAC/D,SAAS,CAAC,CAAC;SAChB;IACH,CAAC;IAED;;;;;;;;;;;OAWG,CACO,wBAAwB,CAAC,MACgB,EAAA;QACjD,MAAM,UAAU,GAAG,aAAa,CAAC,uSAAM,CAAC,MAAM,CAAC,CAAC;QAChD,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,IAAI,IAAI,CAAC,SAAS,CAAC,MAAM,KAAK,CAAC,EAAE;YACzD,OAAO;SACR;QACD,MAAM,SAAS,GAAG,aAAa,CAAC,uSAAM,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;QACvD,IAAI,UAAU,CAAC,MAAM,KAAK,SAAS,CAAC,MAAM,EAAE;YAC1C,MAAM,IAAI,ySAAU,CAChB,CAAA,MAAA,EAAS,IAAI,CAAC,IAAI,CAAA,SAAA,EAAY,SAAS,CAAC,MAAM,CAAA,SAAA,CAAW,GACzD,CAAA,gBAAA,EAAmB,UAAU,CAAC,MAAM,CAAA,gBAAA,CAAkB,GACtD,CAAA,gBAAA,EAAmB,MAAM,EAAE,CAAC,CAAC;SAClC;QACD,IAAK,IAAI,UAAU,GAAG,CAAC,EAAE,UAAU,GAAG,UAAU,CAAC,MAAM,EAAE,UAAU,EAAE,CAAE;YACrE,MAAM,CAAC,GAAG,UAAU,CAAC,UAAU,CAAC,CAAC;YACjC,MAAM,IAAI,GAAc,SAAS,CAAC,UAAU,CAAC,CAAC;YAC9C,IAAI,IAAI,IAAI,IAAI,EAAE;gBAChB,SAAS;aACV;YAED,cAAc;YACd,MAAM,IAAI,GAAG,CAAC,CAAC,IAAI,CAAC;YACpB,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,EAAE;gBACrB,IAAI,IAAI,KAAK,IAAI,CAAC,IAAI,EAAE;oBACtB,MAAM,IAAI,ySAAU,CAChB,CAAA,MAAA,EAAS,UAAU,CAAA,4BAAA,EAA+B,IAAI,CAAC,IAAI,CAAA,EAAA,CAAI,GAC/D,CAAA,cAAA,EAAiB,IAAI,CAAC,IAAI,CAAA,aAAA,EAAgB,IAAI,EAAE,CAAC,CAAC;iBACvD;aACF;YACD,IAAI,IAAI,CAAC,OAAO,IAAI,IAAI,EAAE;gBACxB,IAAI,IAAI,GAAG,IAAI,CAAC,OAAO,EAAE;oBACvB,MAAM,IAAI,ySAAU,CAChB,CAAA,MAAA,EAAS,UAAU,CAAA,4BAAA,EAA+B,IAAI,CAAC,IAAI,EAAE,GAC7D,CAAA,oBAAA,EAAuB,IAAI,CAAC,OAAO,CAAA,aAAA,EAAgB,IAAI,EAAE,CAAC,CAAC;iBAChE;aACF;YACD,IAAI,IAAI,CAAC,OAAO,IAAI,IAAI,EAAE;gBACxB,IAAI,IAAI,GAAG,IAAI,CAAC,OAAO,EAAE;oBACvB,MAAM,IAAI,ySAAU,CAChB,CAAA,MAAA,EAAS,UAAU,CAAA,4BAAA,EAA+B,IAAI,CAAC,IAAI,EAAE,GAC7D,CAAA,oBAAA,EAAuB,IAAI,CAAC,OAAO,CAAA,aAAA,EAAgB,IAAI,CAAA,CAAA,CAAG,CAAC,CAAC;iBACjE;aACF;YAED,eAAe;YACf,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;gBACtB,IAAI,CAAC,CAAC,KAAK,KAAK,IAAI,CAAC,KAAK,EAAE;oBAC1B,MAAM,IAAI,ySAAU,CAChB,CAAA,MAAA,EAAS,UAAU,CAAA,4BAAA,EAA+B,IAAI,CAAC,IAAI,CAAA,CAAA,CAAG,GAC9D,CAAA,iBAAA,EAAoB,IAAI,CAAC,KAAK,CAAA,cAAA,EAAiB,CAAC,CAAC,KAAK,CAAA,CAAA,CAAG,CAAC,CAAC;iBAChE;aACF;YAED,6BAA6B;YAC7B,IAAI,IAAI,CAAC,IAAI,EAAE;gBACb,MAAM,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC;gBACvB,IAAK,MAAM,GAAG,IAAI,IAAI,CAAC,IAAI,CAAE;oBAC3B,MAAM,IAAI,GAAG,MAAM,CAAC,GAAG,CAAC,CAAC;oBACzB,MAAM,KAAK,GAAG,IAAI,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;oBAC7B,iDAAiD;oBACjD,qEAAqE;oBACrE,+CAA+C;oBAC/C,MAAM,YAAY,GACd,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC,MAAM,GAAG,IAAI,CAAC,CAAC;oBAC5D,IAAI,KAAK,IAAI,IAAI,IAAI;wBAAC,KAAK;wBAAE,IAAI;qBAAC,CAAC,OAAO,CAAC,YAAY,CAAC,KAAK,CAAC,CAAC,EAAE;wBAC/D,MAAM,IAAI,ySAAU,CAChB,CAAA,MAAA,EAAS,UAAU,CAAA,4BAAA,CAA8B,GACjD,GAAG,IAAI,CAAC,IAAI,CAAA,gBAAA,EAAmB,IAAI,CAAA,mBAAA,CAAqB,GACxD,CAAA,WAAA,EAAc,KAAK,CAAA,eAAA,EAAkB,MAAM,CAAA,CAAA,CAAG,CAAC,CAAC;qBACrD;iBACF;aACF;YAED,eAAe;YACf,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;gBACtB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;oBAC1C,MAAM,OAAO,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;oBAC9B,MAAM,GAAG,GAAG,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;oBACvB,IAAI,OAAO,IAAI,IAAI,IAAI,GAAG,IAAI,IAAI,EAAE;wBAClC,IAAI,OAAO,KAAK,GAAG,EAAE;4BACnB,MAAM,IAAI,ySAAU,CAChB,CAAA,MAAA,EAAS,UAAU,CAAA,4BAAA,CAA8B,GACjD,GAAG,IAAI,CAAC,IAAI,CAAA,iBAAA,EAAoB,IAAI,CAAC,KAAK,CAAA,EAAA,CAAI,GAC9C,CAAA,YAAA,EAAe,CAAC,CAAC,KAAK,CAAA,CAAA,CAAG,CAAC,CAAC;yBAChC;qBACF;iBACF;aACF;SACF;IACH,CAAC;IAED;;;;;;;OAOG,CACH,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;QAC1C,OAAO,MAAM,CAAC;IAChB,CAAC;IAES,cAAc,CAAC,MAAuB,EAAE,MAAc,EAAA;QAC9D,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;YAC1B,IAAI,CAAC,SAAS,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;SAChC;IACH,CAAC;IAED;;;;OAIG,CACH,WAAW,CAAC,QAAkB,EAAA;QAC5B,IAAI,CAAC,SAAS,GAAG,QAAQ,CAAC;IAC5B,CAAC;IAED;;;OAGG,CACH,aAAa,GAAA;QACX,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC;IACxB,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OAmEG,CACH,gEAAgE;IAChE,KAAK,CACD,MAAuD,EACvD,MAAe,EAAA;QACjB,MAAM,GAAG,MAAM,IAAI,CAAA,CAAE,CAAC;QAEtB,IAAI,CAAC,iBAAiB,EAAE,CAAC;QAEzB,uCAAuC;QACvC,MAAM,UAAU,GAAG,aAAa,CAAC,uSAAM,CAAC,MAAM,CAAC,CAAC;QAEhD,MAAM,cAAc,GAAG,gBAAgB,CAAC,MAAM,CAAC,CAAC;QAChD,MAAM,eAAe,GAAG,iBAAiB,CAAC,MAAM,CAAC,CAAC;QAElD,IAAI,cAAc,KAAK,eAAe,EAAE;YACtC,MAAM,IAAI,ySAAU,CAChB,mCAAmC,GACnC,gCAAgC,CAAC,CAAC;SACvC;QAED,wDAAwD;QACxD,WAAO,wSAAS,EAAC,IAAI,CAAC,IAAI,EAAE,GAAG,EAAE;YAC/B,gEAAgE;YAChE,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE;gBACf;;;mBAGG,CACH,IAAI,CAAC,wBAAwB,CAAC,MAAM,CAAC,CAAC;gBAEtC,uCAAuC;gBACvC,MAAM,WAAW,GAAY,EAAE,CAAC;gBAChC,KAAK,MAAM,KAAK,IAAI,aAAa,CAAC,uSAAM,CAAC,MAAM,CAAC,CAAE;oBAChD,WAAW,CAAC,IAAI,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC;iBAC/B;gBACD,IAAI,CAAC,KAAK,CAAC,aAAa,CAAC,iTAAgB,CAAC,WAAW,CAAC,CAAC,CAAC;gBACxD,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;gBAElB,2DAA2D;gBAC3D,IAAI,IAAI,CAAC,cAAc,EAAE;oBACvB,IAAI,CAAC,UAAU,CAAC,IAAI,CAAC,cAAc,CAAC,CAAC;iBACtC;gBAED,IAAI,IAAI,CAAC,SAAS,KAAK,IAAI,IAAI,eAAe,EAAE;oBAC9C,oEAAoE;oBACpE,qEAAqE;oBACrE,aAAa;oBACb,IAAI,CAAC,SAAS,GAAG,CAAC,CAAC;iBACpB;aACF;YAED;;;cAGE,CACF,IAAI,CAAC,wBAAwB,CAAC,MAAM,CAAC,CAAC;YAEtC,2BAA2B;YAC3B,kEAAkE;YAElE,wEAAwE;YACxE,IAAI,eAAe,EAAE;gBACnB,IAAI,MAAM,GAAG,IAAI,CAAC,IAAI,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;gBAEvC,8DAA8D;gBAC9D,IAAI,IAAI,CAAC,eAAe,EAAE;oBACxB,qEAAqE;oBACrE,IAAI,CAAC,eAAe,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;iBACtC;gBAED,4DAA4D;gBAC5D,iDAAiD;gBACjD,MAAM,UAAU,GAAa,aAAa,CAAC,uSAAM,CAAC,MAAM,CAAC,CAAC;gBAC1D,MAAM,cAAc,GAAa,EAAE,CAAC;gBACpC,wEAAwE;gBACxE,WAAW;gBACX,KAAK,IAAI,CAAC,IAAI,UAAU,CAAE;oBACxB,IAAI,UAAU,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;wBAChC,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,CAAC;qBACf;oBACD,cAAc,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;iBACxB;gBACD,MAAM,GAAG,aAAa,CAAC,iTAAgB,CAAC,cAAc,CAAC,CAAC;gBAExD,IAAI,IAAI,CAAC,mBAAmB,IAAI,IAAI,EAAE;oBACpC,MAAM,IAAI,kTAAmB,CACzB,+CAA+C,GAC/C,sCAAsC,CAAC,CAAC;iBAC7C;gBAED,6CAA6C;gBAC7C,OAAO,MAAM,CAAC;aACf,MAAM;gBACL,MAAM,UAAU,GAAG,iBAAiB,CAAC,MAAM,CAAC,CAAC;gBAC7C,MAAM,WAAW,GAAG,IAAI,CAAC,kBAAkB,CAAC,UAAU,CAAC,CAAC;gBACxD,IAAI,MAAuC,CAAC;gBAC5C,MAAM,WAAW,GAAG,gBAAgB,CAAC,MAAM,CAAC,CAAC;gBAC7C,IAAI,CAAC,4BAA4B,CAC7B,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,UAAU,CAAC,CAAC,CAAU,CAAC,CAAC,CACxB,UAAmB,CAAC,CAAC;gBAEjD,IAAI,WAAW,IAAI,IAAI,IAAI,WAAW,CAAC,MAAM,GAAG,CAAC,IAC7C,KAAK,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,EAAE;oBACjC,kEAAkE;oBAClE,MAAM,GAAI,WAAuB,CACnB,GAAG,CACA,CAAC,KAAK,EAAE,KAAK,EAAE,CAAG,CAAD,GAAK,cAAc,CAChC,WAAW,EAAE,KAAK,EAAE,IAAI,EACxB,aAAa,CAAC,uSAAM,CAAC,MAAM,CAAC,EAAE,MAAM,EAAE,IAAI,CAAC,IAAI,EAC/C,KAAK,CAAC,CAAC,CAAC;iBAC9B,MAAM;oBACL,MAAM,GAAG,IAAI,cAAc,CACvB,WAAW,EAAE,WAAoB,EAAE,IAAI,EACvC,aAAa,CAAC,uSAAM,CAAC,MAAM,CAAC,EAAE,MAAM,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;iBACtD;gBAED;;;;;;kBAME,CACF,IAAI,CAAC,cAAc,CACf,MAAM,EAAE,MAAM,EAAE,IAAI,EAAE,IAAI,EAAE,UAAU,EAAE,WAAW,EAAE,MAAM,CAAC,CAAC;gBACjE,IAAI,CAAC,SAAS,EAAE,CAAC;gBAEjB,IAAI,IAAI,CAAC,mBAAmB,IAAI,IAAI,EAAE;oBACpC,MAAM,IAAI,kTAAmB,CACzB,+CAA+C,GAC/C,sCAAsC,CAAC,CAAC;iBAC7C;gBAED,OAAO,MAAM,CAAC;aACf;QACH,CAAC,CAAC,CAAC;IACL,CAAC;IAED;;;;;;OAMG,CACO,4BAA4B,CAAC,UAAiB,EAAA;QACtD,IAAI,IAAI,CAAC,eAAe,IAAI,IAAI,EAAE;YAChC,OAAO;SACR,MAAM,IAAI,UAAU,CAAC,MAAM,KAAK,IAAI,CAAC,eAAe,CAAC,MAAM,EAAE;YAC5D,OAAO,CAAC,IAAI,CACR,CAAA,8CAAA,CAAgD,GAChD,GAAG,IAAI,CAAC,SAAS,CAAC,UAAU,CAAC,CAAA,6BAAA,CAA+B,GAC5D,CAAA,iBAAA,EAAoB,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,eAAe,CAAC,CAAA,EAAA,CAAI,GAC5D,CAAA,aAAA,EAAgB,IAAI,CAAC,IAAI,EAAE,CAAC,CAAC;SAClC,MAAM;YACL,IAAI,WAAW,GAAG,KAAK,CAAC;YACxB,IAAI,CAAC,eAAe,CAAC,OAAO,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,EAAE;gBAC5C,IAAI,SAAS,IAAI,IAAI,IAAI,UAAU,CAAC,CAAC,CAAC,IAAI,IAAI,IAC1C,UAAU,CAAC,CAAC,CAAC,KAAK,SAAS,EAAE;oBAC/B,WAAW,GAAG,IAAI,CAAC;iBACpB;YACH,CAAC,CAAC,CAAC;YACH,IAAI,WAAW,EAAE;gBACf,OAAO,CAAC,IAAI,CACR,CAAA,8BAAA,CAAgC,GAChC,CAAA,CAAA,EAAI,IAAI,CAAC,SAAS,CAAC,UAAU,CAAC,CAAA,WAAA,CAAa,GAC3C,CAAA,+BAAA,EAAkC,IAAI,CAAC,IAAI,CAAA,EAAA,CAAI,GAC/C,GAAG,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,eAAe,CAAC,EAAE,CAAC,CAAC;aAChD;SACF;IACH,CAAC;IAED;;;;;;;;;;;OAWG,CACH,IAAI,WAAW,GAAA;QACb,IAAI,IAAI,CAAC,YAAY,IAAI,IAAI,IAAI,IAAI,CAAC,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;YAC/D,MAAM,IAAI,6SAAc,CACpB,CAAA,UAAA,EAAa,IAAI,CAAC,IAAI,CAAA,uCAAA,CAAyC,GAC/D,CAAA,qBAAA,CAAuB,CAAC,CAAC;SAC9B;QACD,MAAM,eAAe,GAAa,EAAE,CAAC;QACrC,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,YAAY,CAAE;YACpC,MAAM,WAAW,GAAG,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC;YACtD,IAAI,eAAe,CAAC,OAAO,CAAC,WAAW,CAAC,KAAK,CAAC,CAAC,EAAE;gBAC/C,eAAe,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;aACnC;SACF;QACD,IAAI,eAAe,CAAC,MAAM,KAAK,CAAC,EAAE;YAChC,MAAM,YAAY,GAAG,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,YAAY,CAAC;YACvD,IAAI,KAAK,CAAC,OAAO,CAAC,YAAY,CAAC,IAAI,KAAK,CAAC,OAAO,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,IAC7D,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;gBAC7B,OAAQ,YAAwB,CAAC,CAAC,CAAC,CAAC;aACrC,MAAM;gBACL,OAAO,YAAY,CAAC;aACrB;SAEF,MAAM;YACL,MAAM,IAAI,6SAAc,CACpB,CAAA,UAAA,EAAa,IAAI,CAAC,IAAI,CAAA,2CAAA,CAA6C,GACnE,CAAA,iEAAA,CAAmE,GACnE,CAAA,cAAA,CAAgB,CAAC,CAAC;QACtB,4CAA4C;SAC7C;IACH,CAAC;IAED;;;;;;;;;OASG,CACH,WAAW,GAAA;QACT,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE;YACf,MAAM,IAAI,2SAAY,CAClB,CAAA,mCAAA,EAAsC,IAAI,CAAC,IAAI,CAAA,EAAA,CAAI,GACnD,CAAA,0DAAA,CAA4D,GAC5D,CAAA,uBAAA,CAAyB,CAAC,CAAC;SAChC;QACD,OAAO,cAAc,CAAC,qTAAoB,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;IAC3D,CAAC;IAED;;;;;;;;;;OAUG,CACH,KAAK,CAAC,UAAyB,EAAA;QAC7B,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;IACpB,CAAC;IAED;;;;;;;OAOG,CACH,UAAU,CAAC,aAAa,GAAG,KAAK,EAAA;QAC9B,WAAO,+SAAa,EAAC,aAAa,CAAC,CAAC,CAAC,IAAI,CAAC,gBAAgB,CAAC,CAAC,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;IAC7E,CAAC;IAED;;;;;;;;;;;OAWG,CACH,UAAU,CAAC,OAAiB,EAAA;YAC1B,iPAAI,EAAC,GAAG,EAAE;YACR,MAAM,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC;YAC5B,IAAI,MAAM,CAAC,MAAM,KAAK,OAAO,CAAC,MAAM,EAAE;gBACpC,uEAAuE;gBACvE,kEAAkE;gBAClE,mEAAmE;gBACnE,0DAA0D;gBAC1D,MAAM,IAAI,ySAAU,CAChB,CAAA,yCAAA,EAA4C,IAAI,CAAC,IAAI,CAAA,EAAA,CAAI,GACzD,CAAA,6BAAA,EAAgC,OAAO,CAAC,MAAM,CAAA,EAAA,CAAI,GAClD,CAAA,4BAAA,EAA+B,MAAM,CAAC,MAAM,CAAA,UAAA,CAAY,GACxD,CAAA,kBAAA,EAAqB,OAAO,CAAA,GAAA,CAAK,CAAC,CAAC;aACxC;YACD,IAAI,MAAM,CAAC,MAAM,KAAK,CAAC,EAAE;gBACvB,OAAO;aACR;YACD,MAAM,iBAAiB,GAAmC,EAAE,CAAC;YAC7D,MAAM,WAAW,OAAG,+SAAa,EAAC,MAAM,CAAC,CAAC;YAC1C,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,WAAW,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBAC3C,MAAM,EAAE,GAAG,WAAW,CAAC,CAAC,CAAC,CAAC;gBAC1B,MAAM,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;gBACpB,MAAM,CAAC,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;gBACrB,IAAI,CAAC,8QAAI,CAAC,WAAW,CAAC,EAAE,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,EAAE;oBACxC,MAAM,IAAI,ySAAU,CAChB,CAAA,mBAAA,EAAsB,EAAE,CAAC,KAAK,CAAA,CAAA,CAAG,GACjC,CAAA,0CAAA,EAA6C,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC;iBAC7D;gBACD,iBAAiB,CAAC,IAAI,CAAC;oBAAC,CAAC;oBAAE,CAAC;iBAAC,CAAC,CAAC;aAChC;gBACD,+SAAa,EAAC,iBAAiB,CAAC,CAAC;QACnC,CAAC,CAAC,CAAC;IACL,CAAC;IAED;;;;;;;;;;;;;;OAcG,CACO,SAAS,CACf,IAAY,EAAE,KAAY,EAAE,KAAgB,EAAE,WAAyB,EACvE,WAAyB,EAAE,SAAmB,EAAE,UAAuB,EACvE,kBAA6B,EAAA;QAC/B,iCAAiC;QACjC,IAAI,IAAI,CAAC,iBAAiB,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;YAC/C,MAAM,IAAI,ySAAU,CAChB,CAAA,sBAAA,EAAyB,IAAI,CAAA,WAAA,EAAc,IAAI,CAAC,IAAI,EAAE,CAAC,CAAC;SAC7D;QACD,IAAI,CAAC,iBAAiB,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAElC,IAAI,KAAK,IAAI,IAAI,EAAE;YACjB,KAAK,GAAG,SAAS,CAAC;SACnB;QAED,IAAI,IAAI,CAAC,yBAAyB,EAAE;YAClC,WAAW,GAAG,kBAAkB,IAAI,IAAI,CAAC,CAAC,CAAC,kBAAkB,EAAE,CAAC,CAAC,KACtB,mTAAc,EAAC,OAAO,CAAC,CAAC;SACpE;QACD,MAAM,SAAS,GAAG,WAAW,CAAC,KAAK,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC;QAClD,MAAM,MAAM,GACR,IAAI,+SAAa,CAAC,SAAS,EAAE,KAAK,EAAE,IAAI,EAAE,SAAS,EAAE,UAAU,CAAC,CAAC;QACrE,SAAS,CAAC,OAAO,EAAE,CAAC;QACpB,2EAA2E;QAC3E,IAAI,WAAW,IAAI,IAAI,EAAE;YACvB,IAAI,CAAC,OAAO,CAAC,GAAG,CAAG,CAAD,UAAY,CAAC,KAAK,CAAC,MAAM,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC;SACtD;QACD,IAAI,SAAS,IAAI,IAAI,EAAE;YACrB,SAAS,GAAG,IAAI,CAAC;SAClB;QACD,IAAI,SAAS,EAAE;YACb,IAAI,CAAC,iBAAiB,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;SACrC,MAAM;YACL,IAAI,CAAC,oBAAoB,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;SACxC;QACD,OAAO,MAAM,CAAC;IAChB,CAAC;IAED;;;;;;;;;OASG,CACH,4BAA4B,CAAC,KAAc,EAAA;QACzC,IAAI,CAAC,yBAAyB,GAAG,KAAK,CAAC;IACzC,CAAC;IAED;;;;;;;OAOG,CACH,OAAO,CAAC,MAAqC,EAAA;QAC3C,IAAI,MAAM,IAAI,IAAI,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,IAAI,MAAM,CAAC,MAAM,KAAK,CAAC,EAAE;YAClE,OAAO;SACR;QACD,qBAAqB;QACrB,MAAM,GAAG,aAAa,CAAC,uSAAM,CAAC,MAAM,CAAC,CAAC;QACtC,IAAI,IAAI,CAAC,OAAO,KAAK,SAAS,IAAI,IAAI,CAAC,OAAO,KAAK,IAAI,EAAE;YACvD,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,GAAG,MAAM,CAAC,CAAC;SAC7B;IACH,CAAC;IAED;;;;;;;;;;OAUG,CACH,kBAAkB,CAAC,UAAyB,EAAA;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED;;;;;;;;OAQG,CACH,WAAW,CAAC,MAAuB,EAAE,IAAsB,EAAA;QAEzD,IAAI,CAAC,IAAI,CAAC,eAAe,EAAE;YACzB,IAAI,IAAI,IAAI,IAAI,EAAE;gBAChB,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE;oBACvB,IAAI,CAAC,OAAO,EAAC,WAAW,CAAC,EAAE;wBACzB,IAAI,WAAW,IAAI,IAAI,EAAE;4BACvB,MAAM,IAAI,SAAS,CACf,CAAA,MAAA,EAAS,IAAI,CAAC,IAAI,CAAA,2BAAA,CAA6B,GAC/C,8BAA8B,CAAC,CAAC;yBACrC;oBACH,CAAC,CAAC,CAAC;iBACJ,MAAM;oBACL,MAAM,IAAI,SAAS,CACf,CAAA,MAAA,EAAS,IAAI,CAAC,IAAI,CAAA,2BAAA,CAA6B,GAC/C,8BAA8B,CAAC,CAAC;iBACrC;aACF;YACD,wDAAwD;YACxD,OAAO,IAAI,CAAC;SACb;QACD,gDAAgD;QAChD,4BAA4B;QAC5B,OAAO,IAAI,CAAC;IACd,CAAC;IAEO,eAAe,CACnB,MAAuB,EAAE,OAAwB,EACjD,YAA8B,EAAA;QAChC,IAAI,CAAC,IAAI,CAAC,eAAe,EAAE;YACzB,OAAO;SACR;QAED,MAAM,WAAW,GAAG,IAAI,CAAC,WAAW,CAAC,MAAM,EAAE,YAAY,CAAC,CAAC;QAC3D,MAAM,WAAW,GAAG,aAAa,CAAC,uSAAM,CAAC,OAAO,CAAC,CAAC;QAClD,MAAM,eAAe,GAAG,aAAa,CAAC,uSAAM,CAAC,WAAW,CAAC,CAAC;QAE1D,IAAI,WAAW,CAAC,MAAM,KAAK,eAAe,CAAC,MAAM,EAAE;YACjD,MAAM,IAAI,KAAK,CACX,GAAG,IAAI,CAAC,IAAI,CAAA,SAAA,EAAY,WAAW,CAAC,MAAM,CAAA,SAAA,CAAW,GACrD,CAAA,IAAA,EAAO,WAAW,CAAC,MAAM,CAAA,wBAAA,CAA0B,CAAC,CAAC;SAC1D;QACD,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,WAAW,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;YAC3C,WAAW,CAAC,CAAC,CAAC,CAAC,SAAS,GAAG,eAAe,CAAC,CAAC,CAAC,CAAC;SAC/C;IACH,CAAC;IAED;;;;;;;;;;;OAWG,CACK,cAAc,CAClB,YAA6C,EAC7C,aAA8C,EAC9C,UAA2B,EAAE,WAA4B,EACzD,WAA0B,EAAE,YAA2B,EACvD,SAAa,IAAI,EAAA;QACnB,MAAM,eAAe,GACjB,aAAa,CAAC,uSAAM,CAAC,YAAY,CAAC,CAAC;QACvC,aAAa,GAAG,aAAa,CAAC,uSAAM,CAAC,aAAa,CAAC,CAAC;QACpD,UAAU,GAAG,aAAa,CAAC,uSAAM,CAAC,UAAU,CAAC,CAAC;QAC9C,WAAW,GAAG,aAAa,CAAC,uSAAM,CAAC,WAAW,CAAC,CAAC;QAChD,WAAW,GAAG,WAAW,CAAC,mTAAkB,CAAC,WAAW,CAAC,CAAC;QAC1D,YAAY,GAAG,WAAW,CAAC,mTAAkB,CAAC,YAAY,CAAC,CAAC;QAE5D,uCAAuC;QACvC,MAAM,aAAa,GAAY,EAAE,CAAC;QAClC,MAAM,WAAW,GAAa,EAAE,CAAC;QACjC,MAAM,aAAa,GAAa,EAAE,CAAC;QACnC,KAAK,MAAM,CAAC,IAAI,eAAe,CAAE;YAC/B;;;eAGG,CACH,aAAa,CAAC,IAAI,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC;YAClC,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC;YAC9B,aAAa,CAAC,IAAI,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC;SACnC;QAED,wCAAwC;QACxC,gCAAgC;QAChC,gDAAgD;QAChD,IAAI,IAAI,CACJ;YACE,aAAa,EAAE,IAAI;YACnB,aAAa;YACb,WAAW;YACX,aAAa;YACb,YAAY,EAAE,eAAe;YAC7B,aAAa;YACb,UAAU;YACV,WAAW;YACX,WAAW;YACX,YAAY;SACb,EACD,MAAM,CAAC,CAAC;QAEZ,wBAAwB;QACxB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,aAAa,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;YAC7C,uDAAuD;YACvD,aAAa,CAAC,CAAC,CAAC,CAAC,WAAW,GAAG,IAAI,CAAC;YACpC,aAAa,CAAC,CAAC,CAAC,CAAC,SAAS,GAAG,IAAI,CAAC,YAAY,CAAC,MAAM,GAAG,CAAC,CAAC;YAC1D,aAAa,CAAC,CAAC,CAAC,CAAC,WAAW,GAAG,CAAC,CAAC;SAClC;IACH,CAAC;IAED;;;;;;;;;;;;;;;;;;;;OAoBG,CACH,SAAS,GAAA;QACP,MAAM,MAAM,GACmB;YAAC,IAAI,EAAE,IAAI,CAAC,IAAI;YAAE,SAAS,EAAE,IAAI,CAAC,SAAS;QAAA,CAAC,CAAC;QAC5E,IAAI,IAAI,CAAC,eAAe,IAAI,IAAI,EAAE;YAChC,MAAM,CAAC,iBAAiB,CAAC,GAAG,IAAI,CAAC,eAAe,CAAC;SAClD;QACD,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;YACtB,MAAM,CAAC,OAAO,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC;SAC9B;QACD,OAAO,MAAM,CAAC;IAChB,CAAC;IAED;;;;OAIG,CACO,cAAc,GAAA;QACtB,IAAI,CAAC,OAAO,CAAC,OAAO,EAAC,MAAM,CAAC,EAAE,AAAC,MAAM,CAAC,OAAO,EAAE,CAAC,CAAC;QACjD,OAAO,IAAI,CAAC,OAAO,CAAC,MAAM,CAAC;IAC7B,CAAC;IAES,iBAAiB,GAAA;QACzB,IAAI,IAAI,CAAC,SAAS,KAAK,CAAC,EAAE;YACxB,MAAM,IAAI,KAAK,CAAC,CAAA,OAAA,EAAU,IAAI,CAAC,IAAI,CAAA,sBAAA,CAAwB,CAAC,CAAC;SAC9D;IACH,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OA6BG,CACH,OAAO,GAAA;QACL,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE;YACf,MAAM,IAAI,KAAK,CACX,CAAA,qBAAA,EAAwB,IAAI,CAAC,IAAI,CAAA,yBAAA,CAA2B,GAC5D,CAAA,UAAA,CAAY,CAAC,CAAC;SACnB;QAED,IAAI,IAAI,CAAC,SAAS,KAAK,IAAI,EAAE;YAC3B,MAAM,IAAI,KAAK,CACX,CAAA,qBAAA,EAAwB,IAAI,CAAC,IAAI,CAAA,8BAAA,CAAgC,GACjE,CAAA,IAAA,CAAM,CAAC,CAAC;SACb;QAED,IAAI,CAAC,iBAAiB,EAAE,CAAC;QAEzB,IAAI,oBAAoB,GAAG,CAAC,CAAC;QAC7B,IAAI,EAAE,IAAI,CAAC,SAAS,KAAK,CAAC,EAAE;YAC1B,oBAAoB,GAAG,IAAI,CAAC,cAAc,EAAE,CAAC;SAC9C;QAED,OAAO;YAAC,oBAAoB,EAAE,IAAI,CAAC,SAAS;YAAE,oBAAoB;QAAA,CAAC,CAAC;IACtE,CAAC;CACF;AAED;;;;;;;;;GASG,CACH,SAAS,iBAAiB,CAAC,YACQ;IACjC,YAAY,GACR,aAAa,CAAC,uSAAM,CAAC,YAAY,CAAgC,CAAC;IACtE,MAAM,MAAM,GAAY,EAAE,CAAC;IAC3B,KAAK,MAAM,CAAC,IAAI,YAAY,CAAE;QAC5B,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;KACtB;IACD,OAAO,aAAa,CAAC,iTAAgB,CAAC,MAAM,CAAC,CAAC;AAChD,CAAC;AAED;;;;;;;;GAQG,CACH,SAAS,gBAAgB,CAAC,YACQ;IAChC,OAAO,SAAS,CAAC;AACnB,CAAC;AAaK,SAAU,eAAe,CAC3B,MAAsB,EAAE,KAAa,EACrC,SAAkB;IACpB,IAAI,KAAK,IAAI,IAAI,IAAK,AAAD,SAAU,IAAI,IAAI,IAAI,SAAS,GAAG,CAAC,CAAC,CAAE;QACzD,KAAK,GAAG,MAAM,CAAC,WAAW,CAAC;QAC3B,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;KAC9B;IACD,IAAI,KAAK,CAAC,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;QACnC,OAAO;YAAC,MAAM;SAAC,CAAC;KACjB,MAAM;QACL,MAAM,IAAI,GAAG,KAAK,CAAC,YAAY,CAAC,SAAS,CAAC,CAAC;QAC3C,IAAI,IAAI,CAAC,aAAa,CAAC,MAAM,KAAK,CAAC,EAAE;YACnC,OAAO,IAAI,CAAC,YAAY,CAAC;SAC1B,MAAM;YACL,MAAM,aAAa,GAAqB,EAAE,CAAC;YAC3C,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;gBAClD,MAAM,CAAC,GAAG,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC;gBAC/B,MAAM,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;gBACpC,MAAM,SAAS,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;gBACtC,MAAM,eAAe,GAAG,eAAe,CAAC,CAAC,EAAE,KAAK,EAAE,SAAS,CAAC,CAAC;gBAC7D,0BAA0B;gBAC1B,KAAK,MAAM,CAAC,IAAI,eAAe,CAAE;oBAC/B,IAAI,aAAa,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;wBACnC,aAAa,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;qBACvB;iBACF;aACF;YACD,OAAO,aAAa,CAAC;SACtB;KACF;AACH,CAAC;AAID,SAAS,gBAAgB,CAAC,OAAsC;IAE9D,IAAI,cAAc,GAAG,IAAI,CAAC;IAC1B,KAAK,MAAM,MAAM,IAAI,aAAa,CAAC,uSAAM,CAAC,OAAO,CAAC,CAAE;QAClD,IAAI,CAAC,CAAC,MAAM,YAAY,cAAc,CAAC,EAAE;YACvC,cAAc,GAAG,KAAK,CAAC;YACvB,MAAM;SACP;KACF;IACD,OAAO,cAAc,CAAC;AACxB,CAAC;AAED,SAAS,iBAAiB,CAAC,OACe;IACxC,IAAI,eAAe,GAAG,IAAI,CAAC;IAC3B,KAAK,MAAM,MAAM,IAAI,aAAa,CAAC,uSAAM,CAAC,OAAO,CAAC,CAAE;QAClD,IAAI,MAAM,YAAY,cAAc,EAAE;YACpC,eAAe,GAAG,KAAK,CAAC;YACxB,MAAM;SACP;KACF;IACD,OAAO,eAAe,CAAC;AACzB,CAAC"}},
    {"offset": {"line": 1147, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-layers@4.22.0_@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-layers/dist/engine/input_layer.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-layers/src/engine/input_layer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport {DataType, serialization, Tensor} from '@tensorflow/tfjs-core';\n\nimport {getUid} from '../backend/state';\nimport {ValueError} from '../errors';\nimport {Shape} from '../keras_format/common';\nimport {Kwargs} from '../types';\n\nimport {DisposeResult, Layer, Node, SymbolicTensor} from './topology';\n\n/**\n * Constructor arguments for InputLayer.\n *\n * Note: You should provide only inputShape or batchInputShape (not both).\n * If only inputShape is provided, then the batchInputShape is determined by\n * the batchSize argument and the inputShape: [batchSize].concat(inputShape).\n */\nexport declare interface InputLayerArgs {\n  /** Input shape, not including the batch axis. */\n  inputShape?: Shape;\n  /** Optional input batch size (integer or null). */\n  batchSize?: number;\n  /** Batch input shape, including the batch axis. */\n  batchInputShape?: Shape;\n  /** Datatype of the input.  */\n  dtype?: DataType;\n  /**\n   * Whether the placeholder created is meant to be sparse.\n   */\n  sparse?: boolean;  // TODO(michaelterry): Not clear whether we'll need this.\n\n  /** Name of the layer. */\n  name?: string;\n}\n\nexport class InputLayer extends Layer {\n  /** @nocollapse */\n  static readonly className = 'InputLayer';\n  sparse: boolean;\n  constructor(args: InputLayerArgs) {\n    super({\n      dtype: args.dtype,\n      name: args.name != null ? args.name : getUid('input').toString()\n    });\n    // Normalize config.batchSize and config.sparse\n    if (args.batchSize == null) {\n      args.batchSize = null;\n    }\n    if (args.sparse == null) {\n      args.sparse = false;\n    }\n\n    this.trainable = false;\n    this.built = true;\n    this.sparse = args.sparse;\n\n    if (args.inputShape != null && args.batchInputShape != null) {\n      throw new ValueError(\n          'Only provide the inputShape OR ' +\n          'batchInputShape argument to inputLayer, not both at the same time.');\n    }\n    let batchInputShape = args.batchInputShape;\n    if (batchInputShape == null) {\n      if (args.inputShape == null) {\n        throw new ValueError(\n            'An InputLayer should be passed either a ' +\n            '`batchInputShape` or an `inputShape`.');\n      } else {\n        batchInputShape = [args.batchSize].concat(args.inputShape);\n      }\n    } else {\n      // TODO(michaelterry): Backport to PyKeras\n      if (args.batchSize != null) {\n        throw new ValueError(\n            'Cannot specify batchSize if batchInputShape is ' +\n            'specified when creating an InputLayer.');\n      }\n    }\n\n    const dtype = args.dtype || 'float32';\n\n    this.batchInputShape = batchInputShape;\n    this.dtype = dtype;\n    // TODO(michaelterry): Backport this to PyKeras?\n    this.inputSpec = [{shape: batchInputShape}];\n\n    const inputTensor = new SymbolicTensor(\n        this.dtype, this.batchInputShape, this, [], {}, this.name);\n    inputTensor.nodeIndex = 0;\n    inputTensor.tensorIndex = 0;\n\n    // Create an input node to add to this.outboundNode.\n    // (This call has side effects.)\n    // tslint:disable-next-line:no-unused-expression\n    new Node({\n      outboundLayer: this,\n      inboundLayers: [],\n      nodeIndices: [],\n      tensorIndices: [],\n      inputTensors: [inputTensor],\n      outputTensors: [inputTensor],\n      inputMasks: [null],\n      outputMasks: [null],\n      inputShapes: [batchInputShape],\n      outputShapes: [batchInputShape]\n    });\n  }\n\n  override apply(\n      inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n      kwargs?: Kwargs): Tensor|Tensor[]|SymbolicTensor {\n    throw new ValueError(\n        'Cannot pass any input to an ' +\n        `InputLayer's apply() method. InputLayer name: ${this.name}`);\n  }\n\n  override dispose(): DisposeResult {\n    // dispose() for InputLayer is overridden as no-op.\n    return {refCountAfterDispose: this._refCount, numDisposedVariables: 0};\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {\n      batchInputShape: this.batchInputShape,\n      dtype: this.dtype,\n      sparse: this.sparse,\n      name: this.name\n    };\n  }\n}\nserialization.registerClass(InputLayer);\n\n/**\n * Config for the Input function.\n *\n * Note: You should provide only shape or batchShape (not both).\n * If only shape is provided, then the batchShape becomes\n * [null].concat(inputShape).\n */\nexport interface InputConfig {\n  /**\n   * A shape, not including the batch size. For instance, `shape=[32]`\n   * indicates that the expected input will be batches of 32-dimensional\n   * vectors.\n   */\n  shape?: Shape;\n  /**\n   * A shape tuple (integer), including the batch size. For instance,\n   * `batchShape=[10, 32]` indicates that the expected input will be batches of\n   * 10 32-dimensional vectors. `batchShape=[null, 32]` indicates batches of an\n   * arbitrary number of 32-dimensional vectors.\n   */\n  batchShape?: Shape;\n  /**\n   * An optional name string for the layer. Should be unique in a model (do not\n   * reuse the same name twice). It will be autogenerated if it isn't provided.\n   */\n  name?: string;\n  dtype?: DataType;\n  /**\n   * A boolean specifying whether the placeholder to be created is sparse.\n   */\n  sparse?: boolean;\n}\n\nexport function Input(config: InputConfig): SymbolicTensor {\n  if (config.batchShape == null && config.shape == null) {\n    throw new Error(\n        'Please provide to Input either a `shape`' +\n        ' or a `batchShape` argument. Note that ' +\n        '`shape` does not include the batch ' +\n        'dimension.');\n  }\n  if (config.batchShape != null && config.shape != null) {\n    // TODO(michaelterry): Backport to PyKeras.\n    throw new ValueError(\n        'Please provide either a `shape` or `batchShape` ' +\n        'argument to Input, but not both.');\n  }\n  let batchShape = config.batchShape;\n  if (config.shape != null && batchShape == null) {\n    batchShape = [null].concat(config.shape);\n  }\n\n  let dtype = config.dtype;\n  if (dtype == null) {\n    dtype = 'float32';\n  }\n\n  const inputLayer = new InputLayer({\n    batchInputShape: batchShape,\n    name: config.name,\n    dtype,\n    sparse: config.sparse\n  });\n\n  const outputs = inputLayer.inboundNodes[0].outputTensors;\n  return outputs[0];\n}\n"],"names":[],"mappings":";;;;;;AAAA;;;;;;;;GAQG;AAEH,OAAO,EAAW,aAAa,EAAS,MAAM,uBAAuB,CAAC;AAEtE,OAAO,EAAC,MAAM,EAAC,MAAM,kBAAkB,CAAC;AACxC,OAAO,EAAC,UAAU,EAAC,MAAM,WAAW,CAAC;AAIrC,OAAO,EAAgB,KAAK,EAAE,IAAI,EAAE,cAAc,EAAC,MAAM,YAAY,CAAC;;;;;AA2BtE,MAAa,UAAW,SAAQ,gTAAK;IAInC,YAAY,IAAoB,CAAA;QAC9B,KAAK,CAAC;YACJ,KAAK,EAAE,IAAI,CAAC,KAAK;YACjB,IAAI,EAAE,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,KAAC,+SAAM,EAAC,OAAO,CAAC,CAAC,QAAQ,EAAE;SACjE,CAAC,CAAC;QACH,+CAA+C;QAC/C,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;YAC1B,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC;SACvB;QACD,IAAI,IAAI,CAAC,MAAM,IAAI,IAAI,EAAE;YACvB,IAAI,CAAC,MAAM,GAAG,KAAK,CAAC;SACrB;QAED,IAAI,CAAC,SAAS,GAAG,KAAK,CAAC;QACvB,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;QAClB,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC;QAE1B,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,IAAI,IAAI,CAAC,eAAe,IAAI,IAAI,EAAE;YAC3D,MAAM,IAAI,ySAAU,CAChB,iCAAiC,GACjC,oEAAoE,CAAC,CAAC;SAC3E;QACD,IAAI,eAAe,GAAG,IAAI,CAAC,eAAe,CAAC;QAC3C,IAAI,eAAe,IAAI,IAAI,EAAE;YAC3B,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;gBAC3B,MAAM,IAAI,ySAAU,CAChB,0CAA0C,GAC1C,uCAAuC,CAAC,CAAC;aAC9C,MAAM;gBACL,eAAe,GAAG;oBAAC,IAAI,CAAC,SAAS;iBAAC,CAAC,MAAM,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;aAC5D;SACF,MAAM;YACL,0CAA0C;YAC1C,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;gBAC1B,MAAM,IAAI,ySAAU,CAChB,iDAAiD,GACjD,wCAAwC,CAAC,CAAC;aAC/C;SACF;QAED,MAAM,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,SAAS,CAAC;QAEtC,IAAI,CAAC,eAAe,GAAG,eAAe,CAAC;QACvC,IAAI,CAAC,KAAK,GAAG,KAAK,CAAC;QACnB,gDAAgD;QAChD,IAAI,CAAC,SAAS,GAAG;YAAC;gBAAC,KAAK,EAAE,eAAe;YAAA,CAAC;SAAC,CAAC;QAE5C,MAAM,WAAW,GAAG,IAAI,yTAAc,CAClC,IAAI,CAAC,KAAK,EAAE,IAAI,CAAC,eAAe,EAAE,IAAI,EAAE,EAAE,EAAE,CAAA,CAAE,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;QAC/D,WAAW,CAAC,SAAS,GAAG,CAAC,CAAC;QAC1B,WAAW,CAAC,WAAW,GAAG,CAAC,CAAC;QAE5B,oDAAoD;QACpD,gCAAgC;QAChC,gDAAgD;QAChD,IAAI,+SAAI,CAAC;YACP,aAAa,EAAE,IAAI;YACnB,aAAa,EAAE,EAAE;YACjB,WAAW,EAAE,EAAE;YACf,aAAa,EAAE,EAAE;YACjB,YAAY,EAAE;gBAAC,WAAW;aAAC;YAC3B,aAAa,EAAE;gBAAC,WAAW;aAAC;YAC5B,UAAU,EAAE;gBAAC,IAAI;aAAC;YAClB,WAAW,EAAE;gBAAC,IAAI;aAAC;YACnB,WAAW,EAAE;gBAAC,eAAe;aAAC;YAC9B,YAAY,EAAE;gBAAC,eAAe;aAAC;SAChC,CAAC,CAAC;IACL,CAAC;IAEQ,KAAK,CACV,MAAuD,EACvD,MAAe,EAAA;QACjB,MAAM,IAAI,ySAAU,CAChB,8BAA8B,GAC9B,CAAA,8CAAA,EAAiD,IAAI,CAAC,IAAI,EAAE,CAAC,CAAC;IACpE,CAAC;IAEQ,OAAO,GAAA;QACd,mDAAmD;QACnD,OAAO;YAAC,oBAAoB,EAAE,IAAI,CAAC,SAAS;YAAE,oBAAoB,EAAE,CAAC;QAAA,CAAC,CAAC;IACzE,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YACL,eAAe,EAAE,IAAI,CAAC,eAAe;YACrC,KAAK,EAAE,IAAI,CAAC,KAAK;YACjB,MAAM,EAAE,IAAI,CAAC,MAAM;YACnB,IAAI,EAAE,IAAI,CAAC,IAAI;SAChB,CAAC;IACJ,CAAC;;AA5FD,gBAAA,EAAkB,CACF,WAAA,SAAS,GAAG,YAAY,CAAC;;AA6F3C,ySAAa,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC;AAmClC,SAAU,KAAK,CAAC,MAAmB;IACvC,IAAI,MAAM,CAAC,UAAU,IAAI,IAAI,IAAI,MAAM,CAAC,KAAK,IAAI,IAAI,EAAE;QACrD,MAAM,IAAI,KAAK,CACX,0CAA0C,GAC1C,yCAAyC,GACzC,qCAAqC,GACrC,YAAY,CAAC,CAAC;KACnB;IACD,IAAI,MAAM,CAAC,UAAU,IAAI,IAAI,IAAI,MAAM,CAAC,KAAK,IAAI,IAAI,EAAE;QACrD,2CAA2C;QAC3C,MAAM,IAAI,ySAAU,CAChB,kDAAkD,GAClD,kCAAkC,CAAC,CAAC;KACzC;IACD,IAAI,UAAU,GAAG,MAAM,CAAC,UAAU,CAAC;IACnC,IAAI,MAAM,CAAC,KAAK,IAAI,IAAI,IAAI,UAAU,IAAI,IAAI,EAAE;QAC9C,UAAU,GAAG;YAAC,IAAI;SAAC,CAAC,MAAM,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC;KAC1C;IAED,IAAI,KAAK,GAAG,MAAM,CAAC,KAAK,CAAC;IACzB,IAAI,KAAK,IAAI,IAAI,EAAE;QACjB,KAAK,GAAG,SAAS,CAAC;KACnB;IAED,MAAM,UAAU,GAAG,IAAI,UAAU,CAAC;QAChC,eAAe,EAAE,UAAU;QAC3B,IAAI,EAAE,MAAM,CAAC,IAAI;QACjB,KAAK;QACL,MAAM,EAAE,MAAM,CAAC,MAAM;KACtB,CAAC,CAAC;IAEH,MAAM,OAAO,GAAG,UAAU,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,aAAa,CAAC;IACzD,OAAO,OAAO,CAAC,CAAC,CAAC,CAAC;AACpB,CAAC"}},
    {"offset": {"line": 1297, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-layers@4.22.0_@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-layers/dist/engine/executor.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-layers/src/engine/executor.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\n\nimport {cast, dispose, memory, Tensor, util} from '@tensorflow/tfjs-core';\n\nimport {ValueError} from '../errors';\nimport {Kwargs} from '../types';\nimport {LruCache} from '../utils/executor_utils';\nimport {toList} from '../utils/generic_utils';\n\nimport {InputLayer} from './input_layer';\nimport {SymbolicTensor} from './topology';\n\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\nfunction assertFeedCompatibility(key: SymbolicTensor, val: Tensor): Tensor {\n  // Check dtype compatibility.\n  if (key.dtype == null || key.dtype === val.dtype) {\n    //  a.  If types match, return val tensor as is.\n    return val;\n  }\n  try {\n    //  b. Attempt to convert to expected type.\n    return cast(val, key.dtype);\n  } catch (err) {\n    //  c. If conversion fails, return helpful error.\n    throw new ValueError(\n        `The dtype of the feed (${val.dtype}) can not be cast to the dtype ` +\n        `of the key '${key.name}' (${key.dtype}).`);\n  }\n}\n\n/**\n * A concrete Tensor value for a symbolic tensor as the key.\n */\nexport interface Feed {\n  key: SymbolicTensor;\n  value: Tensor;\n}\n\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\nexport class FeedDict {\n  private id2Value: {[id: number]: Tensor} = {};\n  private id2Mask: {[id: number]: Tensor} = {};\n  private name2Id: {[name: string]: number} = {};\n\n  /**\n   * Constructor, optionally does copy-construction.\n   * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n   *   copy-construction will be performed.\n   */\n  constructor(feeds?: Feed[]|FeedDict) {\n    if (feeds instanceof FeedDict) {\n      for (const id in feeds.id2Value) {\n        this.id2Value[id] = feeds.id2Value[id];\n        if (id in feeds.id2Mask) {\n          this.id2Mask[id] = feeds.id2Mask[id];\n        }\n      }\n    } else {\n      if (feeds == null) {\n        return;\n      }\n      for (const feed of feeds) {\n        this.add(feed.key, feed.value);\n      }\n    }\n  }\n\n  /**\n   * Add a key-value pair to the FeedDict.\n   *\n   * @param key The key of the feed.\n   * @param value The value of the tensor feed.\n   * @param mask The value of the mask feed (optional).\n   * @returns This `FeedDict`.\n   * @throws ValueError: If the key `SymbolicTensor` already exists in the\n   *   `FeedDict`.\n   */\n  add(key: SymbolicTensor, value: Tensor, mask?: Tensor): FeedDict {\n    if (this.id2Value[key.id] == null) {\n      this.id2Value[key.id] = assertFeedCompatibility(key, value);\n      this.name2Id[key.name] = key.id;\n      if (mask != null) {\n        this.id2Mask[key.id] = mask;\n      }\n    } else {\n      throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);\n    }\n    return this;\n  }\n\n  /**\n   * Add a Feed to the FeedDict.\n   * @param feed The new `Feed` to add.\n   * @returns This `FeedDict`.\n   */\n  addFeed(feed: Feed) {\n    this.add(feed.key, feed.value);\n  }\n\n  /**\n   * Probe whether a key already exists in the FeedDict.\n   * @param key\n   */\n  hasKey(key: SymbolicTensor): boolean {\n    return this.id2Value[key.id] != null;\n  }\n\n  /**\n   * Get all the SymbolicTensor available in this FeedDict.\n   */\n  names(): string[] {\n    return Object.keys(this.name2Id);\n  }\n\n  /**\n   * Get the feed value for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed value.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n  getValue(key: SymbolicTensor|string): Tensor {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Value[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n      return this.id2Value[id];\n    }\n  }\n\n  /**\n   * Get the feed mask for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed mask.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n  getMask(key: SymbolicTensor|string): Tensor {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Mask[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n      return this.id2Mask[id];\n    }\n  }\n\n  /** Dispose all mask Tensors held by this object. */\n  disposeMasks() {\n    if (this.id2Mask != null) {\n      dispose(this.id2Mask);\n    }\n  }\n}\n\n// Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\nexport const cachedSorted: LruCache<SymbolicTensor[]> =\n    new LruCache<SymbolicTensor[]>();\n\n// Cache for recipient count maps for given execution targets (i.e., fetches).\nexport const cachedRecipientCounts: LruCache<RecipientCounts> =\n    new LruCache<RecipientCounts>();\n\nexport function updateCacheMaxEntries(maxEntries: number) {\n  if (cachedSorted != null) {\n    cachedSorted.setMaxEntries(maxEntries);\n  }\n  if (cachedRecipientCounts != null) {\n    cachedRecipientCounts.setMaxEntries(maxEntries);\n  }\n}\n\n/**\n * Interface for the optional object used for probing the memory\n * usage and other statistics during execution.\n */\nexport interface ExecutionProbe {\n  /**\n   * Maximum number of tensors that exist during all steps of the\n   * execution. Tensor counts are measured at the beginning of every\n   * step.\n   */\n  maxNumTensors?: number;\n\n  /**\n   * Minimum number of tensors that exist during all steps of the\n   * execution. Tensor counts are measured at the beginning of every\n   * step.\n   */\n  minNumTensors?: number;\n}\n\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\nexport function execute(\n    fetches: SymbolicTensor|SymbolicTensor[], feedDict: FeedDict,\n    kwargs?: Kwargs, probe?: ExecutionProbe): Tensor|\n    Tensor[]|[Tensor | Tensor[]] {\n  const training: boolean = kwargs == null ? false : kwargs['training'];\n\n  const arrayFetches = Array.isArray(fetches);\n  const fetchArray: SymbolicTensor[] =\n      arrayFetches ? fetches : [fetches];\n\n  const outputNames = fetchArray.map(t => t.name);\n  const finalOutputs: Tensor[] = [];\n  const feedNames = feedDict.names();\n  for (const outputName of outputNames) {\n    if (feedNames.indexOf(outputName) !== -1) {\n      finalOutputs.push(feedDict.getValue(outputName));\n    } else {\n      finalOutputs.push(null);\n    }\n  }\n\n  if (probe != null) {\n    // For optional probing of memory footprint during execution.\n    probe.maxNumTensors = -Infinity;\n    probe.minNumTensors = Infinity;\n  }\n\n  // Check cache.\n  const fetchAndFeedKey =\n      outputNames.join(',') + '|' + feedDict.names().sort().join(',');\n  let sorted: SymbolicTensor[] = cachedSorted.get(fetchAndFeedKey);\n  let recipientCounts: {[fetchName: string]: number};\n  if (sorted == null) {\n    // Cache doesn't contain the desired combination of fetches. Compute\n    // topological sort for the combination for the first time.\n    const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n    sorted = out.sorted;\n    recipientCounts = out.recipientCounts;\n\n    // Store results in cache for future use.\n    cachedSorted.put(fetchAndFeedKey, sorted);\n    cachedRecipientCounts.put(fetchAndFeedKey, recipientCounts);\n  }\n  recipientCounts = {};\n  if (!training) {\n    Object.assign(recipientCounts, cachedRecipientCounts.get(fetchAndFeedKey));\n  }\n\n  const internalFeedDict = new FeedDict(feedDict);\n\n  // Start iterative execution on the topologically-sorted SymbolicTensors.\n  for (let i = 0; i < sorted.length; ++i) {\n    if (probe != null) {\n      // For optional probing of memory usage during execution.\n      const numTensors = memory().numTensors;\n      if (numTensors > probe.maxNumTensors) {\n        probe.maxNumTensors = numTensors;\n      }\n      if (numTensors < probe.minNumTensors) {\n        probe.minNumTensors = numTensors;\n      }\n    }\n\n    const symbolic = sorted[i];\n    const srcLayer = symbolic.sourceLayer;\n    if (srcLayer instanceof InputLayer) {\n      continue;\n    }\n    const inputValues: Tensor[] = [];\n    const inputMasks: Tensor[] = [];\n    const tensorsToDispose: Tensor[] = [];\n\n    let maskExists = false;\n    for (const input of symbolic.inputs) {\n      const value = internalFeedDict.getValue(input);\n      const mask = internalFeedDict.getMask(input);\n      inputValues.push(value);\n      inputMasks.push(mask);\n      if (mask != null) {\n        maskExists = true;\n      }\n      if (!training) {\n        recipientCounts[input.name]--;\n        if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) &&\n            outputNames.indexOf(input.name) === -1 && !value.isDisposed &&\n            input.sourceLayer.stateful !== true) {\n          tensorsToDispose.push(value);\n        }\n      }\n    }\n\n    if (maskExists) {\n      kwargs = kwargs || {};\n      kwargs['mask'] = inputMasks[0];\n    }\n    const outputTensors =\n        toList(srcLayer.apply(inputValues, kwargs)) as Tensor[];\n    let outputMask: Tensor|Tensor[] = null;\n    if (srcLayer.supportsMasking) {\n      outputMask = srcLayer.computeMask(inputValues, inputMasks);\n    }\n    const layerOutputs = getNodeOutputs(symbolic);\n    const outputSymbolicTensors =\n        Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n    for (let i = 0; i < outputSymbolicTensors.length; ++i) {\n      if (!internalFeedDict.hasKey(outputSymbolicTensors[i])) {\n        internalFeedDict.add(\n            outputSymbolicTensors[i], outputTensors[i],\n            Array.isArray(outputMask) ? outputMask[0] : outputMask);\n      }\n      const index = outputNames.indexOf(outputSymbolicTensors[i].name);\n      if (index !== -1) {\n        finalOutputs[index] = outputTensors[i];\n      }\n    }\n\n    if (!training) {\n      // Clean up Tensors that are no longer needed.\n      dispose(tensorsToDispose);\n    }\n  }\n  // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n  // tensors as we go, because these tensors are sometimes passed over a\n  // series of mutliple layers, i.e., not obeying the immediate input\n  // relations in the graph. If this becomes a memory-usage concern,\n  // we can improve this in the future.\n  internalFeedDict.disposeMasks();\n\n  return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n\ntype RecipientCounts = {\n  [fetchName: string]: number\n};\n\nexport type RecipientMap = {\n  [fetchName: string]: Set<string>;\n};\n\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\nfunction getTopologicalSortAndRecipientCounts(\n    fetches: SymbolicTensor[], feedDict: FeedDict):\n    {sorted: SymbolicTensor[], recipientCounts: RecipientCounts} {\n  util.assert(\n      fetches != null && fetches.length > 0,\n      () => `Expected at least one fetch, got none`);\n\n  let finalSorted: SymbolicTensor[] = [];\n  let finalRecipientMap: RecipientMap = {};\n  if (fetches.length === 1) {\n    // Special-casing 1 fetch for efficiency.\n    const out =\n        getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n    finalSorted = out.sorted;\n    finalRecipientMap = out.recipientMap;\n  } else {\n    const visited = new Set<string>();\n    for (const fetch of fetches) {\n      const {sorted, recipientMap} =\n          getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict);\n\n      // Merge sorted SymbolicTensor Arrays.\n      for (const symbolicTensor of sorted) {\n        if (!visited.has(symbolicTensor.name)) {\n          finalSorted.push(symbolicTensor);\n          visited.add(symbolicTensor.name);\n        }\n      }\n\n      // Merge recipient maps.\n      for (const name in recipientMap) {\n        if (finalRecipientMap[name] == null) {\n          finalRecipientMap[name] = new Set<string>();\n        }\n        recipientMap[name].forEach(\n            recipient => finalRecipientMap[name].add(recipient));\n      }\n    }\n  }\n  return {\n    sorted: finalSorted,\n    recipientCounts: recipientMap2Counts(finalRecipientMap)\n  };\n}\n\nfunction recipientMap2Counts(recipientMap: RecipientMap): RecipientCounts {\n  const recipientCounts: RecipientCounts = {};\n  for (const name in recipientMap) {\n    recipientCounts[name] = recipientMap[name].size;\n  }\n  return recipientCounts;\n}\n\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\nexport function getTopologicalSortAndRecipientCountsForOneFetch(\n    fetch: SymbolicTensor, feedDict: FeedDict):\n    {sorted: SymbolicTensor[], recipientMap: RecipientMap} {\n  const visited = new Set<string>();\n  const sorted: SymbolicTensor[] = [];\n  const recipientMap: RecipientMap = {};\n\n  // Put keys of the feedDict into visited first, so they don't have to be\n  // walked. This is needed in case where there are feeds for intermediate\n  // SymbolicTensors of the graph.\n  for (const key of feedDict.names()) {\n    visited.add(key);\n  }\n\n  const stack: SymbolicTensor[] = [];\n  const marks: number[] = [];\n\n  // Initial population of stack and marks.\n  stack.push(fetch);\n\n  while (stack.length > 0) {\n    const top = stack[stack.length - 1];\n    if (visited.has(top.name)) {\n      stack.pop();\n      continue;\n    }\n    const topIsMarked = marks[marks.length - 1] === stack.length - 1;\n    if (top.inputs.length === 0 || topIsMarked) {\n      // Input SymbolicTensor or all children have been visited.\n      stack.pop();\n      sorted.push(top);\n      visited.add(top.name);\n      if (topIsMarked) {\n        marks.pop();\n      }\n    } else {\n      // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n      // been visited yet. Push them onto the stack.\n      marks.push(stack.length - 1);\n      for (const input of top.inputs) {\n        // Increment the recipient count. Note that this needs to happen\n        // regardless of whether the SymbolicTensor has been visited before.\n        if (recipientMap[input.name] == null) {\n          recipientMap[input.name] = new Set<string>();\n        }\n        recipientMap[input.name].add(top.name);\n\n        if (visited.has(input.name)) {\n          continue;  // Avoid repeated visits to the same SymbolicTensor.\n        }\n        stack.push(input);\n      }\n    }\n  }\n  return {sorted, recipientMap};\n}\n\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\nfunction getNodeOutputs(fetch: SymbolicTensor): SymbolicTensor|\n    SymbolicTensor[] {\n  let layerOutputs: SymbolicTensor|SymbolicTensor[];\n  if (fetch.sourceLayer.inboundNodes.length === 1) {\n    layerOutputs = fetch.sourceLayer.output;\n  } else {\n    let nodeIndex: number = null;\n    for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n      for (const outputTensor of fetch.sourceLayer.inboundNodes[i]\n               .outputTensors) {\n        if (outputTensor.id === fetch.id) {\n          nodeIndex = i;\n          break;\n        }\n      }\n    }\n    layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n  }\n  return layerOutputs;\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;AAAA;;;;;;;;GAQG,CAEH;;GAEG;;;AAEH,OAAO,EAAC,IAAI,EAAE,OAAO,EAAE,MAAM,EAAU,IAAI,EAAC,MAAM,uBAAuB,CAAC;AAE1E,OAAO,EAAC,UAAU,EAAC,MAAM,WAAW,CAAC;AAErC,OAAO,EAAC,QAAQ,EAAC,MAAM,yBAAyB,CAAC;AACjD,OAAO,EAAC,MAAM,EAAC,MAAM,wBAAwB,CAAC;AAE9C,OAAO,EAAC,UAAU,EAAC,MAAM,eAAe,CAAC;AACzC,OAAO,EAAC,cAAc,EAAC,MAAM,YAAY,CAAC;;;;;;;AAE1C;;GAEG,CACH,SAAS,uBAAuB,CAAC,GAAmB,EAAE,GAAW;IAC/D,6BAA6B;IAC7B,IAAI,GAAG,CAAC,KAAK,IAAI,IAAI,IAAI,GAAG,CAAC,KAAK,KAAK,GAAG,CAAC,KAAK,EAAE;QAChD,gDAAgD;QAChD,OAAO,GAAG,CAAC;KACZ;IACD,IAAI;QACF,2CAA2C;QAC3C,WAAO,qPAAI,EAAC,GAAG,EAAE,GAAG,CAAC,KAAK,CAAC,CAAC;KAC7B,CAAC,OAAO,GAAG,EAAE;QACZ,iDAAiD;QACjD,MAAM,IAAI,ySAAU,CAChB,CAAA,uBAAA,EAA0B,GAAG,CAAC,KAAK,CAAA,+BAAA,CAAiC,GACpE,CAAA,YAAA,EAAe,GAAG,CAAC,IAAI,CAAA,GAAA,EAAM,GAAG,CAAC,KAAK,CAAA,EAAA,CAAI,CAAC,CAAC;KACjD;AACH,CAAC;AAcK,MAAO,QAAQ;IAKnB;;;;OAIG,CACH,YAAY,KAAuB,CAAA;QAT3B,IAAA,CAAA,QAAQ,GAA2B,CAAA,CAAE,CAAC;QACtC,IAAA,CAAA,OAAO,GAA2B,CAAA,CAAE,CAAC;QACrC,IAAA,CAAA,OAAO,GAA6B,CAAA,CAAE,CAAC;QAQ7C,IAAI,KAAK,YAAY,QAAQ,EAAE;YAC7B,IAAK,MAAM,EAAE,IAAI,KAAK,CAAC,QAAQ,CAAE;gBAC/B,IAAI,CAAC,QAAQ,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,QAAQ,CAAC,EAAE,CAAC,CAAC;gBACvC,IAAI,EAAE,IAAI,KAAK,CAAC,OAAO,EAAE;oBACvB,IAAI,CAAC,OAAO,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,OAAO,CAAC,EAAE,CAAC,CAAC;iBACtC;aACF;SACF,MAAM;YACL,IAAI,KAAK,IAAI,IAAI,EAAE;gBACjB,OAAO;aACR;YACD,KAAK,MAAM,IAAI,IAAI,KAAK,CAAE;gBACxB,IAAI,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;aAChC;SACF;IACH,CAAC;IAED;;;;;;;;;OASG,CACH,GAAG,CAAC,GAAmB,EAAE,KAAa,EAAE,IAAa,EAAA;QACnD,IAAI,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,IAAI,IAAI,EAAE;YACjC,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,uBAAuB,CAAC,GAAG,EAAE,KAAK,CAAC,CAAC;YAC5D,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,GAAG,CAAC,EAAE,CAAC;YAChC,IAAI,IAAI,IAAI,IAAI,EAAE;gBAChB,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC;aAC7B;SACF,MAAM;YACL,MAAM,IAAI,ySAAU,CAAC,CAAA,oBAAA,EAAuB,GAAG,CAAC,IAAI,CAAA,KAAA,EAAQ,GAAG,CAAC,EAAE,EAAE,CAAC,CAAC;SACvE;QACD,OAAO,IAAI,CAAC;IACd,CAAC;IAED;;;;OAIG,CACH,OAAO,CAAC,IAAU,EAAA;QAChB,IAAI,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;IACjC,CAAC;IAED;;;OAGG,CACH,MAAM,CAAC,GAAmB,EAAA;QACxB,OAAO,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,IAAI,IAAI,CAAC;IACvC,CAAC;IAED;;OAEG,CACH,KAAK,GAAA;QACH,OAAO,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;IACnC,CAAC;IAED;;;;;;OAMG,CACH,QAAQ,CAAC,GAA0B,EAAA;QACjC,IAAI,GAAG,YAAY,yTAAc,EAAE;YACjC,IAAI,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,IAAI,IAAI,EAAE;gBACjC,MAAM,IAAI,ySAAU,CAAC,CAAA,iBAAA,EAAoB,GAAG,CAAC,IAAI,EAAE,CAAC,CAAC;aACtD,MAAM;gBACL,OAAO,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC;aAC9B;SACF,MAAM;YACL,MAAM,EAAE,GAAG,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC;YAC7B,IAAI,EAAE,IAAI,IAAI,EAAE;gBACd,MAAM,IAAI,ySAAU,CAAC,CAAA,sCAAA,EAAyC,GAAG,EAAE,CAAC,CAAC;aACtE;YACD,OAAO,IAAI,CAAC,QAAQ,CAAC,EAAE,CAAC,CAAC;SAC1B;IACH,CAAC;IAED;;;;;;OAMG,CACH,OAAO,CAAC,GAA0B,EAAA;QAChC,IAAI,GAAG,YAAY,yTAAc,EAAE;YACjC,IAAI,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,EAAE,CAAC,IAAI,IAAI,EAAE;gBACjC,MAAM,IAAI,ySAAU,CAAC,CAAA,iBAAA,EAAoB,GAAG,CAAC,IAAI,EAAE,CAAC,CAAC;aACtD,MAAM;gBACL,OAAO,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC;aAC7B;SACF,MAAM;YACL,MAAM,EAAE,GAAG,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC;YAC7B,IAAI,EAAE,IAAI,IAAI,EAAE;gBACd,MAAM,IAAI,ySAAU,CAAC,CAAA,sCAAA,EAAyC,GAAG,EAAE,CAAC,CAAC;aACtE;YACD,OAAO,IAAI,CAAC,OAAO,CAAC,EAAE,CAAC,CAAC;SACzB;IACH,CAAC;IAED,kDAAA,EAAoD,CACpD,YAAY,GAAA;QACV,IAAI,IAAI,CAAC,OAAO,IAAI,IAAI,EAAE;gBACxB,oPAAO,EAAC,IAAI,CAAC,OAAO,CAAC,CAAC;SACvB;IACH,CAAC;CACF;AAIM,MAAM,YAAY,GACrB,IAAI,wTAAQ,EAAoB,CAAC;AAG9B,MAAM,qBAAqB,GAC9B,IAAI,wTAAQ,EAAmB,CAAC;AAE9B,SAAU,qBAAqB,CAAC,UAAkB;IACtD,IAAI,YAAY,IAAI,IAAI,EAAE;QACxB,YAAY,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC;KACxC;IACD,IAAI,qBAAqB,IAAI,IAAI,EAAE;QACjC,qBAAqB,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC;KACjD;AACH,CAAC;AA2CK,SAAU,OAAO,CACnB,OAAwC,EAAE,QAAkB,EAC5D,MAAe,EAAE,KAAsB;IAEzC,MAAM,QAAQ,GAAY,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC;IAEtE,MAAM,YAAY,GAAG,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC;IAC5C,MAAM,UAAU,GACZ,YAAY,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC;QAAC,OAAO;KAAC,CAAC;IAEvC,MAAM,WAAW,GAAG,UAAU,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,IAAI,CAAC,CAAC;IAChD,MAAM,YAAY,GAAa,EAAE,CAAC;IAClC,MAAM,SAAS,GAAG,QAAQ,CAAC,KAAK,EAAE,CAAC;IACnC,KAAK,MAAM,UAAU,IAAI,WAAW,CAAE;QACpC,IAAI,SAAS,CAAC,OAAO,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE;YACxC,YAAY,CAAC,IAAI,CAAC,QAAQ,CAAC,QAAQ,CAAC,UAAU,CAAC,CAAC,CAAC;SAClD,MAAM;YACL,YAAY,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SACzB;KACF;IAED,IAAI,KAAK,IAAI,IAAI,EAAE;QACjB,6DAA6D;QAC7D,KAAK,CAAC,aAAa,GAAG,CAAC,QAAQ,CAAC;QAChC,KAAK,CAAC,aAAa,GAAG,QAAQ,CAAC;KAChC;IAED,eAAe;IACf,MAAM,eAAe,GACjB,WAAW,CAAC,IAAI,CAAC,GAAG,CAAC,GAAG,GAAG,GAAG,QAAQ,CAAC,KAAK,EAAE,CAAC,IAAI,EAAE,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;IACpE,IAAI,MAAM,GAAqB,YAAY,CAAC,GAAG,CAAC,eAAe,CAAC,CAAC;IACjE,IAAI,eAA8C,CAAC;IACnD,IAAI,MAAM,IAAI,IAAI,EAAE;QAClB,oEAAoE;QACpE,2DAA2D;QAC3D,MAAM,GAAG,GAAG,oCAAoC,CAAC,UAAU,EAAE,QAAQ,CAAC,CAAC;QACvE,MAAM,GAAG,GAAG,CAAC,MAAM,CAAC;QACpB,eAAe,GAAG,GAAG,CAAC,eAAe,CAAC;QAEtC,yCAAyC;QACzC,YAAY,CAAC,GAAG,CAAC,eAAe,EAAE,MAAM,CAAC,CAAC;QAC1C,qBAAqB,CAAC,GAAG,CAAC,eAAe,EAAE,eAAe,CAAC,CAAC;KAC7D;IACD,eAAe,GAAG,CAAA,CAAE,CAAC;IACrB,IAAI,CAAC,QAAQ,EAAE;QACb,MAAM,CAAC,MAAM,CAAC,eAAe,EAAE,qBAAqB,CAAC,GAAG,CAAC,eAAe,CAAC,CAAC,CAAC;KAC5E;IAED,MAAM,gBAAgB,GAAG,IAAI,QAAQ,CAAC,QAAQ,CAAC,CAAC;IAEhD,yEAAyE;IACzE,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;QACtC,IAAI,KAAK,IAAI,IAAI,EAAE;YACjB,yDAAyD;YACzD,MAAM,UAAU,OAAG,mPAAM,EAAE,EAAC,UAAU,CAAC;YACvC,IAAI,UAAU,GAAG,KAAK,CAAC,aAAa,EAAE;gBACpC,KAAK,CAAC,aAAa,GAAG,UAAU,CAAC;aAClC;YACD,IAAI,UAAU,GAAG,KAAK,CAAC,aAAa,EAAE;gBACpC,KAAK,CAAC,aAAa,GAAG,UAAU,CAAC;aAClC;SACF;QAED,MAAM,QAAQ,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;QAC3B,MAAM,QAAQ,GAAG,QAAQ,CAAC,WAAW,CAAC;QACtC,IAAI,QAAQ,YAAY,wTAAU,EAAE;YAClC,SAAS;SACV;QACD,MAAM,WAAW,GAAa,EAAE,CAAC;QACjC,MAAM,UAAU,GAAa,EAAE,CAAC;QAChC,MAAM,gBAAgB,GAAa,EAAE,CAAC;QAEtC,IAAI,UAAU,GAAG,KAAK,CAAC;QACvB,KAAK,MAAM,KAAK,IAAI,QAAQ,CAAC,MAAM,CAAE;YACnC,MAAM,KAAK,GAAG,gBAAgB,CAAC,QAAQ,CAAC,KAAK,CAAC,CAAC;YAC/C,MAAM,IAAI,GAAG,gBAAgB,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC;YAC7C,WAAW,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YACxB,UAAU,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YACtB,IAAI,IAAI,IAAI,IAAI,EAAE;gBAChB,UAAU,GAAG,IAAI,CAAC;aACnB;YACD,IAAI,CAAC,QAAQ,EAAE;gBACb,eAAe,CAAC,KAAK,CAAC,IAAI,CAAC,EAAE,CAAC;gBAC9B,IAAI,eAAe,CAAC,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,KAAK,CAAC,IAC5D,WAAW,CAAC,OAAO,CAAC,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,UAAU,IAC3D,KAAK,CAAC,WAAW,CAAC,QAAQ,KAAK,IAAI,EAAE;oBACvC,gBAAgB,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;iBAC9B;aACF;SACF;QAED,IAAI,UAAU,EAAE;YACd,MAAM,GAAG,MAAM,IAAI,CAAA,CAAE,CAAC;YACtB,MAAM,CAAC,MAAM,CAAC,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC;SAChC;QACD,MAAM,aAAa,OACf,qTAAM,EAAC,QAAQ,CAAC,KAAK,CAAC,WAAW,EAAE,MAAM,CAAC,CAAa,CAAC;QAC5D,IAAI,UAAU,GAAoB,IAAI,CAAC;QACvC,IAAI,QAAQ,CAAC,eAAe,EAAE;YAC5B,UAAU,GAAG,QAAQ,CAAC,WAAW,CAAC,WAAW,EAAE,UAAU,CAAC,CAAC;SAC5D;QACD,MAAM,YAAY,GAAG,cAAc,CAAC,QAAQ,CAAC,CAAC;QAC9C,MAAM,qBAAqB,GACvB,KAAK,CAAC,OAAO,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC;YAAC,YAAY;SAAC,CAAC;QAChE,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,qBAAqB,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YACrD,IAAI,CAAC,gBAAgB,CAAC,MAAM,CAAC,qBAAqB,CAAC,CAAC,CAAC,CAAC,EAAE;gBACtD,gBAAgB,CAAC,GAAG,CAChB,qBAAqB,CAAC,CAAC,CAAC,EAAE,aAAa,CAAC,CAAC,CAAC,EAC1C,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,UAAU,CAAC,CAAC;aAC7D;YACD,MAAM,KAAK,GAAG,WAAW,CAAC,OAAO,CAAC,qBAAqB,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC;YACjE,IAAI,KAAK,KAAK,CAAC,CAAC,EAAE;gBAChB,YAAY,CAAC,KAAK,CAAC,GAAG,aAAa,CAAC,CAAC,CAAC,CAAC;aACxC;SACF;QAED,IAAI,CAAC,QAAQ,EAAE;YACb,8CAA8C;gBAC9C,oPAAO,EAAC,gBAAgB,CAAC,CAAC;SAC3B;KACF;IACD,iEAAiE;IACjE,sEAAsE;IACtE,mEAAmE;IACnE,kEAAkE;IAClE,qCAAqC;IACrC,gBAAgB,CAAC,YAAY,EAAE,CAAC;IAEhC,OAAO,YAAY,CAAC,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC;AACvD,CAAC;AAUD;;;;;;;;;;GAUG,CACH,SAAS,oCAAoC,CACzC,OAAyB,EAAE,QAAkB;IAE/C,8QAAI,CAAC,MAAM,CACP,OAAO,IAAI,IAAI,IAAI,OAAO,CAAC,MAAM,GAAG,CAAC,EACrC,GAAG,CAAG,CAAD,AAAC,qCAAA,CAAuC,CAAC,CAAC;IAEnD,IAAI,WAAW,GAAqB,EAAE,CAAC;IACvC,IAAI,iBAAiB,GAAiB,CAAA,CAAE,CAAC;IACzC,IAAI,OAAO,CAAC,MAAM,KAAK,CAAC,EAAE;QACxB,yCAAyC;QACzC,MAAM,GAAG,GACL,+CAA+C,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC,CAAC;QAC1E,WAAW,GAAG,GAAG,CAAC,MAAM,CAAC;QACzB,iBAAiB,GAAG,GAAG,CAAC,YAAY,CAAC;KACtC,MAAM;QACL,MAAM,OAAO,GAAG,IAAI,GAAG,EAAU,CAAC;QAClC,KAAK,MAAM,KAAK,IAAI,OAAO,CAAE;YAC3B,MAAM,EAAC,MAAM,EAAE,YAAY,EAAC,GACxB,+CAA+C,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;YAErE,sCAAsC;YACtC,KAAK,MAAM,cAAc,IAAI,MAAM,CAAE;gBACnC,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,cAAc,CAAC,IAAI,CAAC,EAAE;oBACrC,WAAW,CAAC,IAAI,CAAC,cAAc,CAAC,CAAC;oBACjC,OAAO,CAAC,GAAG,CAAC,cAAc,CAAC,IAAI,CAAC,CAAC;iBAClC;aACF;YAED,wBAAwB;YACxB,IAAK,MAAM,IAAI,IAAI,YAAY,CAAE;gBAC/B,IAAI,iBAAiB,CAAC,IAAI,CAAC,IAAI,IAAI,EAAE;oBACnC,iBAAiB,CAAC,IAAI,CAAC,GAAG,IAAI,GAAG,EAAU,CAAC;iBAC7C;gBACD,YAAY,CAAC,IAAI,CAAC,CAAC,OAAO,EACtB,SAAS,CAAC,EAAE,AAAC,iBAAiB,CAAC,IAAI,CAAC,CAAC,GAAG,CAAC,SAAS,CAAC,CAAC,CAAC;aAC1D;SACF;KACF;IACD,OAAO;QACL,MAAM,EAAE,WAAW;QACnB,eAAe,EAAE,mBAAmB,CAAC,iBAAiB,CAAC;KACxD,CAAC;AACJ,CAAC;AAED,SAAS,mBAAmB,CAAC,YAA0B;IACrD,MAAM,eAAe,GAAoB,CAAA,CAAE,CAAC;IAC5C,IAAK,MAAM,IAAI,IAAI,YAAY,CAAE;QAC/B,eAAe,CAAC,IAAI,CAAC,GAAG,YAAY,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC;KACjD;IACD,OAAO,eAAe,CAAC;AACzB,CAAC;AAaK,SAAU,+CAA+C,CAC3D,KAAqB,EAAE,QAAkB;IAE3C,MAAM,OAAO,GAAG,IAAI,GAAG,EAAU,CAAC;IAClC,MAAM,MAAM,GAAqB,EAAE,CAAC;IACpC,MAAM,YAAY,GAAiB,CAAA,CAAE,CAAC;IAEtC,wEAAwE;IACxE,wEAAwE;IACxE,gCAAgC;IAChC,KAAK,MAAM,GAAG,IAAI,QAAQ,CAAC,KAAK,EAAE,CAAE;QAClC,OAAO,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC;KAClB;IAED,MAAM,KAAK,GAAqB,EAAE,CAAC;IACnC,MAAM,KAAK,GAAa,EAAE,CAAC;IAE3B,yCAAyC;IACzC,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;IAElB,MAAO,KAAK,CAAC,MAAM,GAAG,CAAC,CAAE;QACvB,MAAM,GAAG,GAAG,KAAK,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;QACpC,IAAI,OAAO,CAAC,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC,EAAE;YACzB,KAAK,CAAC,GAAG,EAAE,CAAC;YACZ,SAAS;SACV;QACD,MAAM,WAAW,GAAG,KAAK,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,KAAK,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC;QACjE,IAAI,GAAG,CAAC,MAAM,CAAC,MAAM,KAAK,CAAC,IAAI,WAAW,EAAE;YAC1C,0DAA0D;YAC1D,KAAK,CAAC,GAAG,EAAE,CAAC;YACZ,MAAM,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;YACjB,OAAO,CAAC,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC;YACtB,IAAI,WAAW,EAAE;gBACf,KAAK,CAAC,GAAG,EAAE,CAAC;aACb;SACF,MAAM;YACL,oEAAoE;YACpE,8CAA8C;YAC9C,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YAC7B,KAAK,MAAM,KAAK,IAAI,GAAG,CAAC,MAAM,CAAE;gBAC9B,gEAAgE;gBAChE,oEAAoE;gBACpE,IAAI,YAAY,CAAC,KAAK,CAAC,IAAI,CAAC,IAAI,IAAI,EAAE;oBACpC,YAAY,CAAC,KAAK,CAAC,IAAI,CAAC,GAAG,IAAI,GAAG,EAAU,CAAC;iBAC9C;gBACD,YAAY,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,GAAG,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC;gBAEvC,IAAI,OAAO,CAAC,GAAG,CAAC,KAAK,CAAC,IAAI,CAAC,EAAE;oBAC3B,SAAS,CAAE,oDAAoD;iBAChE;gBACD,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;aACnB;SACF;KACF;IACD,OAAO;QAAC,MAAM;QAAE,YAAY;IAAA,CAAC,CAAC;AAChC,CAAC;AAED;;;;;GAKG,CACH,SAAS,cAAc,CAAC,KAAqB;IAE3C,IAAI,YAA6C,CAAC;IAClD,IAAI,KAAK,CAAC,WAAW,CAAC,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;QAC/C,YAAY,GAAG,KAAK,CAAC,WAAW,CAAC,MAAM,CAAC;KACzC,MAAM;QACL,IAAI,SAAS,GAAW,IAAI,CAAC;QAC7B,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,WAAW,CAAC,YAAY,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YAC9D,KAAK,MAAM,YAAY,IAAI,KAAK,CAAC,WAAW,CAAC,YAAY,CAAC,CAAC,CAAC,CAClD,aAAa,CAAE;gBACvB,IAAI,YAAY,CAAC,EAAE,KAAK,KAAK,CAAC,EAAE,EAAE;oBAChC,SAAS,GAAG,CAAC,CAAC;oBACd,MAAM;iBACP;aACF;SACF;QACD,YAAY,GAAG,KAAK,CAAC,WAAW,CAAC,WAAW,CAAC,SAAS,CAAC,CAAC;KACzD;IACD,OAAO,YAAY,CAAC;AACtB,CAAC"}},
    {"offset": {"line": 1715, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-layers@4.22.0_@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-layers/dist/engine/container.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-layers/src/engine/container.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original source: keras/engine/topology.py */\n\nimport {NamedTensorMap, Scalar, serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\n\nimport {getUid} from '../backend/state';\nimport {NotImplementedError, RuntimeError, ValueError} from '../errors';\nimport {Shape} from '../keras_format/common';\nimport {TensorKeyWithArgsArray} from '../keras_format/node_config';\nimport {PyJsonDict} from '../keras_format/types';\nimport {deserialize as deserializeLayer} from '../layers/serialization';\nimport {Kwargs} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport {convertTsToPythonic} from '../utils/serialization_utils';\nimport * as types_utils from '../utils/types_utils';\nimport {batchSetValue, LayerVariable} from '../variables';\nimport {version as layersVersion} from '../version';\n\nimport {execute, FeedDict} from './executor';\nimport {InputLayer} from './input_layer';\nimport {DisposeResult, Layer, Node, SymbolicTensor} from './topology';\n\n/** Constructor config for Container. */\nexport interface ContainerArgs {\n  inputs: SymbolicTensor|SymbolicTensor[];\n  outputs: SymbolicTensor|SymbolicTensor[];\n  name?: string;\n}\n\n// get weights key from tensor map in order to check if it is from keras v3.\n// e.g. dense/0\nconst isKerasSavedModelFormat = (weights: NamedTensorMap): boolean => {\n  const keys = Object.keys(weights);\n  if (keys.length === 0) {\n    return false;\n  }\n  const key = keys[0].split('/');\n  return !isNaN(parseInt(key[key.length - 1], 10));\n};\n\n/**\n * A Container is a directed acyclic graph of layers.\n *\n * It is the topological form of a \"model\". A LayersModel\n * is simply a Container with added training routines.\n *\n */\nexport abstract class Container extends Layer {\n  inputs: SymbolicTensor[];\n  outputs: SymbolicTensor[];\n\n  inputLayers: Layer[];\n  inputLayersNodeIndices: number[];\n  inputLayersTensorIndices: number[];\n\n  outputLayers: Layer[];\n  outputLayersNodeIndices: number[];\n  outputLayersTensorIndices: number[];\n\n  layers: Layer[];\n  layersByDepth: {[depth: string]: Layer[]};\n  nodesByDepth: {[depth: string]: Node[]};\n\n  internalContainerRefs: Container[];\n\n  containerNodes = new Set<string>();\n\n  // TODO(michaelterry): Add cache support\n  // private outputMaskCache: any;\n  // private outputTensorCache: any;\n  // private outputShapeCache: any;\n\n  inputNames: string[];\n  outputNames: string[];\n  feedInputShapes: Shape[];\n\n  protected internalInputShapes: Shape[];\n  protected internalOutputShapes: Shape[];\n  // TODO(cais): Maybe 'feed' should not in the names of these variables,\n  //   due to the fact that our backend is not symbolic.\n  protected feedInputNames: string[];\n  protected feedOutputNames: string[];\n\n  constructor(args: ContainerArgs) {\n    // No args passed to super's constructor.\n    super({});\n    this.name = args.name;\n    if (this.name == null) {\n      const prefix = this.getClassName().toLowerCase();\n      this.name = getUid(prefix);\n    }\n\n    this.supportsMasking = false;\n    this.trainable_ = true;\n\n    // TODO(michaelterry): Initialize perInputLosses/Updates here.\n\n    // Container-specific properties.\n    if (Array.isArray(args.inputs)) {\n      this.inputs = args.inputs.slice();\n    } else {\n      this.inputs = [args.inputs];\n    }\n    if (Array.isArray(args.outputs)) {\n      this.outputs = args.outputs.slice();\n    } else {\n      this.outputs = [args.outputs];\n    }\n\n    // Check for redundancy in inputs.\n    if (generic_utils.unique(this.inputs).length !== this.inputs.length) {\n      throw new ValueError(\n          'The list of inputs passed to the model is ' +\n          'redundant. All inputs should only appear once. Found: ' +\n          `${this.inputs.map(x => x.name)}`);\n    }\n\n    // Check for redundancy in outputs.\n    if (generic_utils.unique(this.outputs).length !== this.outputs.length) {\n      console.warn(\n          'The list of outputs passed to the model is redundant. ' +\n          'All outputs should only appear once. Found: ' +\n          `${this.outputs.map(x => x.name)}`);\n    }\n\n    /*\n      List of initial layers (1 to 1 mapping with this.inputs, hence the same\n      layer might appear twice)\n    */\n    this.inputLayers = [];\n    this.inputLayersNodeIndices = [];\n    this.inputLayersTensorIndices = [];\n    /*\n      List of layers (1 to 1 mapping with this.outputs, hence the same layer\n      might appear twice)\n    */\n    this.outputLayers = [];\n    this.outputLayersNodeIndices = [];\n    this.outputLayersTensorIndices = [];\n    /*\n      All layers in order of horizontal graph traversal. Entries are unique.\n      Includes input and output layers.\n    */\n    this.layers = [];\n\n    /*\n      References to container layers that were constructed internally. We need\n      these to properly dispose of tensors from nested containers.\n    */\n    this.internalContainerRefs = [];\n\n    // TODO(michaelterry): Determine if caching still needed with eager\n    // backend.\n    /*\n      This is for performance optimization when calling the Container on new\n      inputs. Every time the Container is called on a set on input tensors,\n      we compute the output tensors, output masks and output shapes in one pass,\n      then cache them here. When one of these outputs is queried later,\n      we retrieve it from there instead of recomputing it.\n    */\n    // this.outputTensorCache = {};\n    // this.outputShapeCache = {};\n\n    // Build this.outputLayers:\n    for (const x of this.outputs) {\n      const layer = x.sourceLayer;\n      const nodeIndex = x.nodeIndex;\n      const tensorIndex = x.tensorIndex;\n      this.outputLayers.push(layer);\n      this.outputLayersNodeIndices.push(nodeIndex);\n      this.outputLayersTensorIndices.push(tensorIndex);\n    }\n\n    // TODO(michaelterry): Add output mask cache code.\n\n    // Build this.inputLayers:\n    for (const x of this.inputs) {\n      const layer = x.sourceLayer;\n      const nodeIndex = x.nodeIndex;\n      const tensorIndex = x.tensorIndex;\n      /*\n        It's supposed to be an input layer, so only one node\n        and one tensor output.\n      */\n      generic_utils.assert(nodeIndex === 0, 'input layer has >1 nodes');\n      generic_utils.assert(tensorIndex === 0, 'input layer has >1 tensors');\n      this.inputLayers.push(layer);\n      this.inputLayersNodeIndices.push(nodeIndex);\n      this.inputLayersTensorIndices.push(tensorIndex);\n    }\n\n    // Build this.inputNames and this.outputNames.\n    this.inputNames = [];\n    this.outputNames = [];\n    this.feedInputShapes = [];\n    this.feedInputNames = [];\n    this.feedOutputNames = [];\n    for (let i = 0; i < this.inputLayers.length; i++) {\n      const layer = this.inputLayers[i];\n      // Check that layer is an InputLayer.\n      if (!(layer instanceof InputLayer)) {\n        throw new TypeError(\n            'Input layers to a LayersModel must be InputLayer objects. ' +\n            `Received inputs: ${args.inputs}. ` +\n            `Input ${i} (0-based) originates ` +\n            `from layer type ${layer.getClassName()}.`);\n      }\n      this.inputNames.push(layer.name);\n      this.feedInputShapes.push(layer.batchInputShape);\n\n      this.feedInputNames.push(layer.name);\n    }\n    for (const layer of this.outputLayers) {\n      this.outputNames.push(layer.name);\n    }\n\n    this.internalInputShapes = this.inputs.map(x => x.shape);\n    this.internalOutputShapes = this.outputs.map(x => x.shape);\n\n    /*\n      Container_nodes: set of nodes included in the graph (not all nodes\n      included in the layers are relevant to the current graph).\n    */\n    // ids of all nodes relevant to the Container:\n    const nodesDepths: {[nodeID: string]: number} = {};\n    // To recover nodes from their ID.\n    const nodeIDToNode: {[nodeID: string]: Node} = {};\n    const layersDepths: {[layerID: string]: number} = {};\n    // To layers from their ID.\n    const layerIDToLayer: {[layerID: string]: Layer} = {};\n    const layerIndices: {[layerID: string]: number} = {};\n    const nodesInDecreasingDepth: Node[] = [];\n\n    /**\n     * Builds a map of the graph of layers.\n     *\n     * This recursively updates the map `layerIndices`,\n     * the list `nodesInDecreasingDepth` and the set `containerNodes`.\n     *\n     * @param tensor Some tensor in a graph.\n     * @param finishedNodes Set of nodes whose subgraphs have been traversed\n     *         completely. Useful to prevent duplicated work.\n     * @param nodesInProgress Set of nodes that are currently active on the\n     *         recursion stack. Useful to detect cycles.\n     * @param layer Layer from which `tensor` comes from. If not provided,\n     *   will be obtained from tensor.sourceLayer.\n     * @param nodeIndex Node index from which `tensor` comes from.\n     * @param tensorIndex TensorIndex from which `tensor` comes from.\n     *\n     * @exception RuntimeError if a cycle is detected.\n     */\n    const buildMapOfGraph =\n        (tensor: SymbolicTensor, finishedNodes: Node[], nodesInProgress: Node[],\n         layer?: Layer, nodeIndex?: number, tensorIndex?: number) => {\n          if (layer == null || nodeIndex == null || tensorIndex == null) {\n            layer = tensor.sourceLayer;\n            nodeIndex = tensor.nodeIndex;\n            tensorIndex = tensor.tensorIndex;\n          }\n          const node = layer.inboundNodes[nodeIndex];\n\n          // Prevent cycles.\n          if (nodesInProgress.indexOf(node) !== -1) {\n            throw new RuntimeError(\n                `The tensor ${tensor.name} at layer \"${layer.name}\" ` +\n                'is part of a cycle.');\n          }\n\n          // Don't repeat work for shared subgraphs\n          if (finishedNodes.indexOf(node) !== -1) {\n            return;\n          }\n\n          // Update containerNodes.\n          this.containerNodes.add(Container.nodeKey(layer, nodeIndex));\n\n          // Store the traversal order for layer sorting.\n          if (!(layer.id in layerIndices)) {\n            layerIndices[layer.id] = Object.keys(layerIndices).length;\n          }\n\n          if (nodesInProgress.indexOf(node) === -1) {\n            nodesInProgress.push(node);\n          }\n\n          // Propagate to all previous tensors connected to this node.\n          const numInboundLayers = node.inboundLayers.length;\n          for (let i = 0; i < numInboundLayers; i++) {\n            const x = node.inputTensors[i];\n            const layer = node.inboundLayers[i];\n            const nodeIndex = node.nodeIndices[i];\n            const tensorIndex = node.tensorIndices[i];\n            buildMapOfGraph(\n                x, finishedNodes, nodesInProgress, layer, nodeIndex,\n                tensorIndex);\n          }\n          finishedNodes.push(node);\n          while (nodesInProgress.indexOf(node) >= 0) {\n            nodesInProgress.splice(nodesInProgress.indexOf(node), 1);\n          }\n          nodesInDecreasingDepth.push(node);\n        };\n\n    const finishedNodes: Node[] = [];\n    const nodesInProgress: Node[] = [];\n    for (const x of this.outputs) {\n      buildMapOfGraph(x, finishedNodes, nodesInProgress);\n    }\n\n    const reversedNodesInDecreasingDepth =\n        nodesInDecreasingDepth.slice().reverse();\n    for (const node of reversedNodesInDecreasingDepth) {\n      nodeIDToNode[node.id] = node;\n      // If the depth is not set, the node has no outbound nodes (depth 0).\n      if (!(node.id in nodesDepths)) {\n        nodesDepths[node.id] = 0;\n      }\n      let depth = nodesDepths[node.id];\n\n      // Update the depth of the corresponding layer\n      const previousDepth =\n          (layersDepths[node.outboundLayer.id] == null ?\n               0 :\n               layersDepths[node.outboundLayer.id]);\n\n      /*\n        If we've seen this layer before at a higher depth, we should use that\n        depth instead of the node depth.  This is necessary for shared layers\n        that have inputs at different depth levels in the graph.\n      */\n      depth = Math.max(depth, previousDepth);\n      layersDepths[node.outboundLayer.id] = depth;\n      layerIDToLayer[node.outboundLayer.id] = node.outboundLayer;\n      nodesDepths[node.id] = depth;\n\n      // Update the depth of inbound nodes.\n      for (let i = 0; i < node.inboundLayers.length; i++) {\n        const inboundLayer = node.inboundLayers[i];\n        const nodeIndex = node.nodeIndices[i];\n        const inboundNode = inboundLayer.inboundNodes[nodeIndex];\n        const previousDepth =\n            (nodesDepths[inboundNode.id] == null ? 0 :\n                                                   nodesDepths[inboundNode.id]);\n        nodesDepths[inboundNode.id] = Math.max(depth + 1, previousDepth);\n        nodeIDToNode[inboundNode.id] = inboundNode;\n      }\n    }\n\n    // Build a dict {depth: list of nodes with this depth}\n    const nodesByDepth: {[depth: string]: Node[]} = {};\n    for (const nodeID in nodesDepths) {\n      const depth = nodesDepths[nodeID];\n      if (!(depth in nodesByDepth)) {\n        nodesByDepth[depth] = [];\n      }\n      nodesByDepth[depth].push(nodeIDToNode[nodeID]);\n    }\n\n    // Build a dict {depth: list of layers with this depth}\n    const layersByDepth: {[depth: string]: Layer[]} = {};\n    for (const layerID in layersDepths) {\n      const depth = layersDepths[layerID];\n      if (!(depth in layersByDepth)) {\n        layersByDepth[depth] = [];\n      }\n      layersByDepth[depth].push(layerIDToLayer[layerID]);\n    }\n\n    // Get sorted list of layer depths.\n    let depthKeys = Object.keys(layersByDepth)\n                        .map(x => parseInt(x, 10))\n                        .sort(generic_utils.reverseNumberCompare);\n\n    // Set this.layers and this.layersByDepth.\n    this.layers = [];\n    for (const depth of depthKeys) {\n      const layersForDepth = layersByDepth[depth];\n      // Container.layers needs to have a deterministic order:\n      // here we order them by traversal order.\n      layersForDepth.sort((a, b) => {\n        const aIndex = layerIndices[a.id];\n        const bIndex = layerIndices[b.id];\n        if (aIndex < bIndex) {\n          return -1;\n        }\n        if (aIndex > bIndex) {\n          return 1;\n        }\n        return 0;\n      });\n      for (const layer of layersForDepth) {\n        if (layer instanceof Container) {\n          this.internalContainerRefs.push(layer);\n        }\n        this.layers.push(layer);\n      }\n    }\n    this.layersByDepth = layersByDepth;\n\n    // Get sorted list of node depths;\n    depthKeys = Object.keys(nodesByDepth)\n                    .map(x => parseInt(x, 10))\n                    .sort(generic_utils.reverseNumberCompare);\n\n    // Check that all tensors required are computable.\n    // computable_tensors: all tensors in the graph\n    // that can be computed from the inputs provided.\n    const computableTensors = this.inputs.slice();\n\n    // To provide a better error msg.\n    const layersWithCompleteInput: string[] = [];\n    for (const depth of depthKeys) {\n      for (const node of nodesByDepth[depth]) {\n        const layer = node.outboundLayer;\n        if (layer != null) {\n          for (const x of node.inputTensors) {\n            if (computableTensors.indexOf(x) === -1) {\n              throw new RuntimeError(\n                  `Graph disconnected: cannot obtain value for tensor ${x}` +\n                  ` at layer \"${layer.name}\". ` +\n                  'The following previous layers were accessed without ' +\n                  `issue: ${layersWithCompleteInput}`);\n            }\n          }\n          for (const x of node.outputTensors) {\n            computableTensors.push(x);\n          }\n          layersWithCompleteInput.push(layer.name);\n        }\n      }\n    }\n\n    // Set this.containerNodes and this.nodesByDepth.\n    this.nodesByDepth = nodesByDepth;\n\n    // Ensure name unicity, which will be crucial for serialization\n    // (since serialized nodes refer to layers by their name).\n    const allNames = this.layers.map(x => x.name);\n    for (const name of allNames) {\n      const numOccurrences = allNames.filter(x => x === name).length;\n      if (numOccurrences !== 1) {\n        throw new RuntimeError(\n            `The name \"${name}\" is used ${numOccurrences} times ` +\n            'in the model. All layer names should be unique. Layer names: ' +\n            JSON.stringify(allNames));\n      }\n    }\n\n    // Layer parameters.\n    // The new container starts with a single inbound node\n    // for its inputs, and no outbound nodes.\n    // Will be appended to by future calls to apply().\n    this.outboundNodes = [];\n    // Will be appended to below, and by future calls to apply().\n    this.inboundNodes = [];\n\n    // Create the node linking internal inputs to internal outputs.\n    // (This call has side effects.)\n    // tslint:disable-next-line:no-unused-expression\n    new Node({\n      outboundLayer: this,\n      inboundLayers: [],\n      nodeIndices: [],\n      tensorIndices: [],\n      inputTensors: this.inputs,\n      outputTensors: this.outputs,\n      inputMasks: this.inputs.map(x => null),\n      outputMasks: this.outputs.map(x => null),\n      inputShapes: this.inputs.map(x => x.shape),\n      outputShapes: this.outputs.map(x => x.shape)\n    });\n    this.built = true;\n    this._refCount = 1;  // The ref count of a container always start at 1.\n  }\n\n  protected override assertNotDisposed() {\n    if (this._refCount === 0) {\n      throw new Error(`Container '${this.name}' is already disposed.`);\n    }\n  }\n\n  /**\n   * Attempt to dispose a LayersModel's weights.\n   *\n   * This method decrease the reference count of the LayersModel object by 1.\n   *\n   * A LayersModel is reference-counted. Its reference count is incremented by 1\n   * when it is first constructed and when it is used as a Layer of another\n   * LayersModel.\n   *\n   * If the reference count of a LayersModel becomes 0, the `dispose` method of\n   * all its constituent `Layer`s will be called.\n   *\n   * Note: If the reference count is greater than 0 after the decrement, the\n   * `dispose` method of its constituent `Layer`s will *not* be called.\n   *\n   * After a LayersModel is disposed, it cannot be used in calls such as\n   * 'predict`, `evaluate` or `fit` anymore.\n   *\n   * @returns A DisposeResult Object with the following fields:\n   *   - refCountAfterDispose: The reference count of the LayersModel after this\n   *     `dispose()` call.\n   *   - numDisposedVariables: Number of `tf.Variable`s (i.e., weights) disposed\n   *     during this `dispose()` call.\n   * @throws {Error} If the layer is not built yet, or if the LayersModel has\n   *   already been disposed.\n   */\n  override dispose(): DisposeResult {\n    this.assertNotDisposed();\n    const result:\n        DisposeResult = {refCountAfterDispose: null, numDisposedVariables: 0};\n    if (--this._refCount === 0) {\n      for (const layer of this.layers) {\n        result.numDisposedVariables += layer.dispose().numDisposedVariables;\n      }\n\n      // Call dispose on each internally created container layer again to ensure\n      // their refCounts hit zero and their tensors are subsequently deleted.\n      for (const container of this.internalContainerRefs) {\n        result.numDisposedVariables += container.dispose().numDisposedVariables;\n      }\n    }\n    result.refCountAfterDispose = this._refCount;\n    return result;\n  }\n\n  override get trainable() {\n    return this.trainable_;\n  }\n\n  override set trainable(trainable: boolean) {\n    this.layers.forEach(layer => {\n      // tslint:disable-next-line:no-any\n      ((layer as any)._trainableWeights as LayerVariable[])\n          .forEach(w => w.trainable = trainable);\n    });\n    this.trainable_ = trainable;\n  }\n\n  override get trainableWeights(): LayerVariable[] {\n    // Porting Note: This check below is to prevent errors where the\n    //   _trainableWeights inherited from the parent class (Layer) gets\n    //   inadvertently used.\n    if (this._trainableWeights.length > 0) {\n      throw new ValueError(\n          'Container instance unexpectedly contains _trainableWeights.' +\n          'The trainable weights of a Container are a union of the ' +\n          'trainable weights of its consituent Layers. Its own ' +\n          '_trainableWeights must remain an empty Array.');\n    }\n\n    if (!this.trainable) {\n      return [];\n    }\n    let weights: LayerVariable[] = [];\n    for (const layer of this.layers) {\n      weights = weights.concat(layer.trainableWeights);\n    }\n    return weights;\n  }\n\n  override get nonTrainableWeights(): LayerVariable[] {\n    const weights: LayerVariable[] = [];\n    for (const layer of this.layers) {\n      weights.push(...layer.nonTrainableWeights);\n    }\n    if (!this.trainable) {\n      const trainableWeights: LayerVariable[] = [];\n      for (const layer of this.layers) {\n        trainableWeights.push(...layer.trainableWeights);\n      }\n      return trainableWeights.concat(weights);\n    }\n    return weights;\n  }\n\n  override get weights(): LayerVariable[] {\n    return this.trainableWeights.concat(this.nonTrainableWeights);\n  }\n\n  /**\n   * Loads all layer weights from a JSON object.\n   *\n   * Porting Note: HDF5 weight files cannot be directly loaded in JavaScript /\n   *   TypeScript. The utility script at `scripts/pykeras.py` offers means\n   *   to convert them into JSON strings compatible with this method.\n   * Porting Note: TensorFlow.js Layers supports only loading by name currently.\n   *\n   * @param weights A JSON mapping weight names to weight values as nested\n   *   arrays of numbers, or a `NamedTensorMap`, i.e., a JSON mapping weight\n   *   names to `tf.Tensor` objects.\n   * @param strict Require that the provided weights exactly match those\n   *   required by the container.  Default: `true`.  Passing `false` means that\n   *   extra weights and missing weights will be silently ignored.\n   */\n  loadWeights(weights: NamedTensorMap, strict = true) {\n    const nameToWeight: {[name: string]: LayerVariable} = {};\n    let totalWeightsCount = 0;\n    const modelIsKerasSavedModelFormat = isKerasSavedModelFormat(weights);\n    if (modelIsKerasSavedModelFormat) {\n      this.parseWeights(weights);\n    }\n    // Check if weights from keras v3.\n    for (const layer of this.layers) {\n      for (const [index, weight] of layer.weights.entries()) {\n        // Parse the name to layerName/index.\n        // e.g. dense/0, dense/1, dense_1/0, dense_1/1\n        const parsedName = modelIsKerasSavedModelFormat ?\n            `${weight.name.split('/').slice(0, -1).join('/') + '/'}${index}` :\n            weight.originalName;\n        if (nameToWeight[parsedName] != null) {\n          throw new ValueError(`Duplicate weight name: ${parsedName}`);\n        }\n        nameToWeight[parsedName] = weight;\n        totalWeightsCount++;\n      }\n    }\n\n    const weightValueTuples: Array<[LayerVariable, Tensor]> = [];\n    for (const name in weights) {\n      // TF 2.2.0 added cell name to the weight name in the format of\n      // layer_name/cell_name/weight_name, we need to remove\n      // the inner cell name.\n      let validatedName = name;\n      if (nameToWeight[name] == null) {\n        const tokens = name.split('/');\n        const shortenNameArray =\n            tokens.slice(0, -2).concat([tokens[tokens.length - 1]]);\n        validatedName = shortenNameArray.join('/');\n      }\n      if (nameToWeight[validatedName] != null) {\n        weightValueTuples.push([nameToWeight[validatedName], weights[name]]);\n      } else if (strict) {\n        throw new ValueError(\n            `Provided weight data has no target variable: ${name}`);\n      }\n      delete nameToWeight[validatedName];\n    }\n\n    if (strict) {\n      // Check that all weights are set.\n      const unsetNames: string[] = [];\n      for (const name in nameToWeight) {\n        unsetNames.push(name);\n      }\n      if (unsetNames.length > 0) {\n        throw new ValueError(\n            `${unsetNames.length} of ${\n                totalWeightsCount} weights are not set: ` +\n            `${unsetNames}`);\n      }\n    }\n\n    batchSetValue(weightValueTuples);\n  }\n\n  protected parseWeights(weights: NamedTensorMap) {\n    for (const key in Object.keys(weights)) {\n      const listParts = key.split('/');\n      const list = ['vars', 'layer_checkpoint_dependencies'];\n      // For keras v3, the weights name are saved based on the folder structure.\n      // e.g. _backbone/_layer_checkpoint_dependencies/transformer/_self../\n      // _output_dense/vars/0\n      // Therefore we discard the `vars` and `layer_checkpoint_depencies` within\n      // the saved name and only keeps the layer name and weights.\n      // This can help to mapping the actual name of the layers and load each\n      // weight accordingly.\n      const newKey = listParts\n                         .map(str => {\n                           if (str.startsWith('_')) {\n                             return str.slice(1);\n                           }\n                           return str;\n                         })\n                         .filter(str => !list.includes(str))\n                         .join('/');\n      if (newKey !== key) {\n        weights[newKey] = weights[key];\n        delete weights[key];\n      }\n    }\n  }\n\n  /**\n   * Util shared between different serialization methods.\n   * @returns LayersModel config with Keras version information added.\n   */\n  protected updatedConfig(): serialization.ConfigDict {\n    const theConfig = this.getConfig();\n    const modelConfig: serialization.ConfigDict = {};\n    modelConfig['className'] = this.getClassName();\n    modelConfig['config'] = theConfig;\n    modelConfig['kerasVersion'] = `tfjs-layers ${layersVersion}`;\n    // TODO(nielsene): Replace something like K.backend() once\n    // possible.\n    modelConfig['backend'] = 'TensorFlow.js';\n    return modelConfig;\n  }\n\n  /**\n   * Returns a JSON string containing the network configuration.\n   *\n   * To load a network from a JSON save file, use\n   * models.modelFromJSON(jsonString);\n   * @param extraJsonArgs Unused in tfjs-layers, maintained for PyKeras\n   * @param returnString Whether the return value should be stringified\n   *    (default: `true`).\n   * @returns a JSON string if `returnString` (default), or a JSON object if\n   *   `!returnString`.\n   */\n  // tslint:disable-next-line:no-any\n  toJSON(unused?: any, returnString = true): string|PyJsonDict {\n    const modelConfig = convertTsToPythonic(this.updatedConfig()) as PyJsonDict;\n    return returnString ? JSON.stringify(modelConfig) : modelConfig;\n  }\n\n  /**\n   * Call the model on new inputs.\n   *\n   * In this case `call` just reapplies all ops in the graph to the new inputs\n   * (e.g. build a new computational graph from the provided inputs).\n   *\n   * @param inputs A tensor or list of tensors.\n   * @param mask A mask or list of masks. A mask can be either a tensor or null\n   *   (no mask).\n   *\n   * @return A tensor if there is a single output, or a list of tensors if there\n   *   are more than one outputs.\n   */\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = generic_utils.toList(inputs);\n      const feedDict = new FeedDict();\n      for (let i = 0; i < this.inputs.length; ++i) {\n        feedDict.add(this.inputs[i], inputs[i]);\n      }\n      return execute(this.outputs, feedDict, kwargs) as Tensor | Tensor[];\n    });\n  }\n\n  /**\n   * Computes an output mask tensor.\n   *\n   * @param inputs Tensor or list of tensors.\n   * @param mask Tensor or list of tensors.\n   *\n   * @return null or a tensor (or list of tensors, one per output tensor of the\n   * layer).\n   */\n  override computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor\n      |Tensor[] {\n    return tidy(() => {\n      inputs = generic_utils.toList(inputs);\n      let masks: Tensor[];\n      if (mask == null) {\n        masks = generic_utils.pyListRepeat(null, inputs.length);\n      } else {\n        masks = generic_utils.toList(mask);\n      }\n      // TODO(michaelterry): Add support for mask caching.\n      return this.runInternalGraph(inputs, masks)[1];\n    });\n  }\n\n  /**\n   * Computes the output shape of the layer.\n   *\n   * Assumes that the layer will be built to match that input shape provided.\n   *\n   * @param inputShape A shape (tuple of integers) or a list of shape tuples\n   *   (one per output tensor of the layer). Shape tuples can include null for\n   *   free dimensions, instead of an integer.\n   */\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    const inputShapes = types_utils.normalizeShapeList(inputShape);\n    if (inputShapes.length !== this.inputLayers.length) {\n      throw new ValueError(\n          `Invalid inputShape argument ${inputShape}: ` +\n          `model has ${this.inputLayers.length} tensor inputs.`);\n    }\n\n    // TODO(michaelterry): Add caching\n    const layersToOutputShapes: {[shapeKey: string]: Shape} = {};\n    for (let i = 0; i < inputShapes.length; i++) {\n      const layer = this.inputLayers[i];\n      const inputShape = inputShapes[i];\n      // It's an input layer: computeOutputShape is identity,\n      // and there is only one node and one tensor output.\n      const shapeKey = layer.name + '_0_0';\n      layersToOutputShapes[shapeKey] = inputShape;\n    }\n\n    const depthKeys = Object.keys(this.nodesByDepth)\n                          .map(x => parseInt(x, 10))\n                          .sort(generic_utils.reverseNumberCompare);\n    // Iterate over nodes, by depth level.\n    if (depthKeys.length > 1) {\n      for (const depth of depthKeys) {\n        const nodes = this.nodesByDepth[depth];\n        for (const node of nodes) {\n          // This is always a single layer, never a list.\n          const layer = node.outboundLayer;\n          if (this.inputLayers.map(x => x.id).indexOf(layer.id) !== -1) {\n            // We've already covered the input layers a few lines above.\n            continue;\n          }\n          // Potentially redundant list, same size of node.inputTensors.\n          const inputShapes: Shape[] = [];\n          for (let j = 0; j < node.inboundLayers.length; j++) {\n            const inboundLayer = node.inboundLayers[j];\n            const nodeIndex = node.nodeIndices[j];\n            const tensorIndex = node.tensorIndices[j];\n            const shapeKey = `${inboundLayer.name}_${nodeIndex}_${tensorIndex}`;\n            const inputShape = layersToOutputShapes[shapeKey];\n            inputShapes.push(inputShape);\n          }\n\n          const outputShape = layer.computeOutputShape(\n              generic_utils.singletonOrArray(inputShapes));\n\n          const outputShapes = types_utils.normalizeShapeList(outputShape);\n          const nodeIndex = layer.inboundNodes.indexOf(node);\n          for (let j = 0; j < outputShapes.length; j++) {\n            const shapeKey = `${layer.name}_${nodeIndex}_${j}`;\n            layersToOutputShapes[shapeKey] = outputShapes[j];\n          }\n        }\n      }\n    }\n\n    // Read final output shapes from layersToOutputShapes.\n    const outputShapes: Shape[] = [];\n    const outputShapeKeys: string[] = [];\n    for (let i = 0; i < this.outputLayers.length; i++) {\n      const layer = this.outputLayers[i];\n      const nodeIndex = this.outputLayersNodeIndices[i];\n      const tensorIndex = this.outputLayersTensorIndices[i];\n      const shapeKey = `${layer.name}_${nodeIndex}_${tensorIndex}`;\n      outputShapeKeys.push(shapeKey);\n    }\n\n    for (let i = 0; i < outputShapeKeys.length; i++) {\n      const key = outputShapeKeys[i];\n      generic_utils.assert(key in layersToOutputShapes);\n      outputShapes.push(layersToOutputShapes[key]);\n    }\n\n    // TODO(michaelterry): Update cache\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  /**\n   * Computes output tensors for new inputs.\n   *\n   * Note:\n   *   - Expects `inputs` to be a list (potentially with 1 element).\n   *\n   * @param inputs List of tensors\n   * @param masks List of masks (tensors or null).\n   * @return Three lists: outputTensors, outputMasks, outputShapes\n   */\n  protected runInternalGraph(inputs: Tensor[], masks?: Tensor[]):\n      [Tensor[], Tensor[], Shape[]] {\n    if (masks == null) {\n      masks = generic_utils.pyListRepeat(null, inputs.length);\n    }\n\n    // Dictionary mapping reference tensors to tuples\n    // (computed tensor, compute mask)\n    // we assume a 1:1 mapping from tensor to mask\n    // TODO: raise exception when a `.computeMask()` call\n    // does not return a list the same size as `call`\n    const tensorMap: {[tensorID: string]: [Tensor, Tensor]} = {};\n    for (let i = 0; i < this.inputs.length; ++i) {\n      const x = this.inputs[i];\n      const y = inputs[i];\n      const mask = masks[i];\n      tensorMap[x.id] = [y, mask];\n    }\n\n    const depthKeys = Object.keys(this.nodesByDepth)\n                          .map(x => parseInt(x, 10))\n                          .sort(generic_utils.reverseNumberCompare);\n    for (const depth of depthKeys) {\n      const nodes = this.nodesByDepth[depth];\n      for (const node of nodes) {\n        // This is always a single layer, never a list.\n        const layer = node.outboundLayer;\n        const referenceInputTensors = node.inputTensors;\n        const referenceOutputTensors = node.outputTensors;\n\n        // If all previous input tensors are available in tensorMap,\n        // then call node.inboundLayer on them.\n        // List of tuples [input, mask]:\n        const computedData = new Array<[Tensor, Tensor]>();\n        for (const x of referenceInputTensors) {\n          if (x.id in tensorMap) {\n            computedData.push(tensorMap[x.id]);\n          }\n        }\n        if (computedData.length === referenceInputTensors.length) {\n          // TODO(michaelterry): Add K.name_scope here, if we need it.\n          let kwargs: Kwargs = {};\n          let computedTensors: Tensor[];\n          let computedMasks: Tensor[];\n          let outputTensors: Tensor[];\n          let outputMasks: Tensor[];\n          // call layer\n          if (node.callArgs != null) {\n            kwargs = node.callArgs;\n          }\n          if (computedData.length === 1) {\n            const [computedTensor, computedMask] = computedData[0];\n            if (kwargs['mask'] == null) {\n              kwargs['mask'] = computedMask;\n            }\n            outputTensors =\n                generic_utils.toList(layer.call(computedTensor, kwargs));\n            outputMasks = generic_utils.toList(\n                layer.computeMask(computedTensor, computedMask));\n            computedTensors = [computedTensor];\n            computedMasks = [computedMask];\n          } else {\n            computedTensors = computedData.map(x => x[0]);\n            computedMasks = computedData.map(x => x[1]);\n            if (kwargs['mask'] == null) {\n              kwargs['mask'] = computedMasks;\n            }\n            outputTensors =\n                generic_utils.toList(layer.call(computedTensors, kwargs));\n            outputMasks = generic_utils.toList(\n                layer.computeMask(computedTensors, computedMasks));\n          }\n\n          if (layer.activityRegularizer) {\n            throw new NotImplementedError(\n                'LayersModel invocation with concrete Tensor value(s) in the ' +\n                'presence of activity regularizer(s) is not supported yet.');\n          }\n          // TODO(michaelterry): Add model updates and losses\n\n          // Update tensor map.\n          for (let i = 0; i < referenceOutputTensors.length; ++i) {\n            const x = referenceOutputTensors[i];\n            const y = outputTensors[i];\n            const mask = outputMasks[i];\n            tensorMap[x.id] = [y, mask];\n          }\n        }\n      }\n    }\n\n    const outputTensors: Tensor[] = [];\n    const outputMasks: Tensor[] = [];\n    const outputShapes: Shape[] = [];\n    for (const x of this.outputs) {\n      generic_utils.assert(\n          x.id in tensorMap, `Could not compute output ${x.name} : ${x.id}`);\n      const [tensor, mask] = tensorMap[x.id];\n      outputShapes.push(tensor.shape);\n      outputTensors.push(tensor);\n      outputMasks.push(mask);\n    }\n\n    // TODO(michaelterry): Add support for caches.\n    return [outputTensors, outputMasks, outputShapes];\n  }\n\n  /**\n   * Builds a map of internal node keys to node ordering.\n   * Used in serializaion a node orderings may change as unused nodes are\n   * dropped. Porting Note:  This helper method was pulled out of getConfig to\n   * improve readability.\n   * @param layers An array of Layers in the model.\n   * @returns Map of Node Keys to index order within the layer.\n   */\n  private buildNodeConversionMap(layers: Layer[]): {[nodeKey: string]: number} {\n    const nodeConversionMap: {[nodeKey: string]: number} = {};\n    let keptNodes: number;\n    for (const layer of this.layers) {\n      keptNodes = layer instanceof Container ? 1 : 0;\n      for (let originalNodeIndex = 0;\n           originalNodeIndex < layer.inboundNodes.length; originalNodeIndex++) {\n        const nodeKey = Container.nodeKey(layer, originalNodeIndex);\n        if (this.containerNodes.has(nodeKey)) {\n          // i.e. we mark it to be saved\n          nodeConversionMap[nodeKey] = keptNodes;\n          keptNodes += 1;\n        }\n      }\n    }\n    return nodeConversionMap;\n  }\n\n  /**\n   * Retrieves a layer based on either its name (unique) or index.\n   *\n   * Indices are based on order of horizontal graph traversal (bottom-up).\n   *\n   * If both `name` and `index` are specified, `index` takes precedence.\n   *\n   * @param name Name of layer.\n   * @param index Index of layer.\n   * @returns A Layer instance.\n   * @throws ValueError: In case of invalid layer name or index.\n   *\n   * @doc {\n   *    heading: 'Layers',\n   *    subheading: 'Classes',\n   *    namespace: 'layers',\n   *    subclasses: ['LayersModel']\n   * }\n   */\n  getLayer(name: string): Layer;\n  getLayer(index: number): Layer;\n  getLayer(name: string, index: number): Layer;\n  getLayer(nameOrIndex?: string|number, index?: number): Layer {\n    if (index != null) {\n      return this.findLayer(index);\n    } else {\n      if (nameOrIndex == null) {\n        throw new ValueError('Provide either a layer name or layer index');\n      }\n      if (typeof nameOrIndex === 'number') {\n        return this.findLayer(nameOrIndex);\n      }\n    }\n\n    for (const layer of this.layers) {\n      if (layer.name === nameOrIndex) {\n        return layer;\n      }\n    }\n    throw new ValueError(`No such layer: ${nameOrIndex}`);\n  }\n\n  findLayer(index: number): Layer {\n    if (this.layers.length <= index) {\n      throw new ValueError(\n          `Was asked to retrieve layer at index ${index}, but model only ` +\n          `has ${this.layers.length} layer(s).`);\n    } else {\n      return this.layers[index];\n    }\n  }\n\n  /**\n   * Retrieves the Container's current loss values.\n   *\n   * Used for regularizers during training.\n   */\n  override calculateLosses(): Scalar[] {\n    // Porting Node: This is an augmentation to Container.loss in PyKeras.\n    //   In PyKeras, Container.loss returns symbolic tensors. Here a concrete\n    //   Tensor (specifically Scalar) values are returned. This is due to the\n    //   imperative backend.\n    return tidy(() => {\n      const losses: Scalar[] = [];\n      for (const layer of this.layers) {\n        for (let nodeIndex = 0; nodeIndex < layer.inboundNodes.length;\n             ++nodeIndex) {\n          const nodeKey = Container.nodeKey(layer, nodeIndex);\n          if (this.containerNodes.has(nodeKey)) {\n            losses.push(...layer.calculateLosses());\n          }\n        }\n      }\n      // TODO(cais): Add any unconditional model-level losses?\n      return losses;\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {name: this.name};\n\n    // Build a map from layer unique name (self._node_key)\n    // to the index of the nodes that are saved in the config.\n    // Only nodes in container_nodes are saved.\n    const nodeConversionMap: {[nodeKey: string]: number} =\n        this.buildNodeConversionMap(this.layers);\n\n    // Serialize and save the layers in layerConfigs\n    const layerConfigs = [];\n    for (const layer of this.layers) {\n      const layerClassName = layer.getClassName();\n      const layerConfig = layer.getConfig();\n      const filteredInboundNodes = [];\n      for (let originalNodeIndex = 0;\n           originalNodeIndex < layer.inboundNodes.length; originalNodeIndex++) {\n        const node = layer.inboundNodes[originalNodeIndex];\n        const nodeKey = Container.nodeKey(layer, originalNodeIndex);\n        let kwargs = {};\n        if (this.containerNodes.has(nodeKey)) {\n          // The node is relevant to the model:\n          // add to filteredInboundNodes.\n          if (node.callArgs) {\n            try {\n              JSON.stringify(node.callArgs);\n              kwargs = node.callArgs;\n            } catch (err) {\n              console.warn(\n                  `Layer ${layer.name} was passed ` +\n                  `non-serializable keyword arguments: ` +\n                  `${node.callArgs}. They will not be included ` +\n                  `in the serialized model (and thus will be ` +\n                  `missing at deserialization time).`);\n              kwargs = {};\n            }\n          }\n          if (node.inboundLayers.length > 0) {\n            const nodeData = [];\n            for (let i = 0; i < node.inboundLayers.length; i++) {\n              const inboundLayer = node.inboundLayers[i];\n              const nodeIndex = node.nodeIndices[i];\n              const tensorIndex = node.tensorIndices[i];\n              const nodeKey = Container.nodeKey(inboundLayer, nodeIndex);\n              let newNodeIndex = nodeConversionMap[nodeKey];\n              if (newNodeIndex == null) {\n                newNodeIndex = 0;\n              }\n              nodeData.push(\n                  [inboundLayer.name, newNodeIndex, tensorIndex, kwargs]);\n            }\n            filteredInboundNodes.push(nodeData);\n          }\n        }\n      }\n      const dict: serialization.ConfigDict = {};\n      dict['name'] = layer.name;\n      dict['className'] = layerClassName;\n      dict['config'] = layerConfig;\n      dict['inboundNodes'] = filteredInboundNodes;\n      layerConfigs.push(dict);\n    }\n    config['layers'] = layerConfigs;\n    // Gather info about inputs and outputs\n    const modelInputs = [];\n    for (let i = 0; i < this.inputLayers.length; i++) {\n      const layer = this.inputLayers[i];\n      const nodeIndex = this.inputLayersNodeIndices[i];\n\n      const nodeKey = Container.nodeKey(layer, nodeIndex);\n      if (!this.containerNodes.has(nodeKey)) {\n        continue;\n      }\n      let newNodeIndex = nodeConversionMap[nodeKey];\n      if (newNodeIndex === null || newNodeIndex === undefined) {\n        newNodeIndex = 0;\n      }\n      const tensorIndex = this.inputLayersTensorIndices[i];\n      modelInputs.push([layer.name, newNodeIndex, tensorIndex]);\n    }\n    config['inputLayers'] = modelInputs;\n\n    const modelOutputs = [];\n    for (let i = 0; i < this.outputLayers.length; i++) {\n      const layer = this.outputLayers[i];\n      const nodeIndex = this.outputLayersNodeIndices[i];\n\n      const nodeKey = Container.nodeKey(layer, nodeIndex);\n      if (!this.containerNodes.has(nodeKey)) {\n        continue;\n      }\n      let newNodeIndex = nodeConversionMap[nodeKey];\n      if (newNodeIndex === null || newNodeIndex === undefined) {\n        newNodeIndex = 0;\n      }\n      const tensorIndex = this.outputLayersTensorIndices[i];\n      modelOutputs.push([layer.name, newNodeIndex, tensorIndex]);\n    }\n    config['outputLayers'] = modelOutputs;\n    return config;\n  }\n\n  /**\n   * Instantiates a LayersModel from its config (output of `get_config()`).\n   * @param cls the class to create\n   * @param config LayersModel config dictionary.\n   * @param customObjects An optional dictionary of custom objects.\n   * @param fastWeightInit Optional flag to use fast weight initialization\n   *   during deserialization. This is applicable to cases in which\n   *   the initialization will be immediately overwritten by loaded weight\n   *   values. Default: `false`.\n   * @returns A LayersModel instance.\n   * @throws ValueError: In case of improperly formatted config dict.\n   */\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict,\n      fastWeightInit = false): T {\n    // Layer instances created during\n    // the graph reconstruction process\n    const createdLayers: {[layerName: string]: Layer} = {};\n\n    // Dictionary mapping layer instances to\n    // node data that specifies a layer call.\n    // It acts as a queue that maintains any unprocessed\n    // layer call until it becomes possible to process it\n    // (i.e. until the input tensors to the call all exist).\n    const unprocessedNodes: {[layer: string]: TensorKeyWithArgsArray[][]} = {};\n    function addUnprocessedNode(\n        layer: Layer, nodeData: TensorKeyWithArgsArray[]) {\n      if (!(layer.name in unprocessedNodes)) {\n        unprocessedNodes[layer.name] = [nodeData];\n      } else {\n        unprocessedNodes[layer.name].push(nodeData);\n      }\n    }\n\n    function processNode(layer: Layer, nodeData: TensorKeyWithArgsArray[]) {\n      const inputTensors: SymbolicTensor[] = [];\n      let kwargs;\n      for (const inputData of nodeData) {\n        const inboundLayerName = inputData[0];\n        const inboundNodeIndex = inputData[1];\n        const inboundTensorIndex = inputData[2];\n\n        kwargs = inputData[3] == null ?\n            {} :\n            inputData[3] as serialization.ConfigDict;\n        if (!(inboundLayerName in createdLayers)) {\n          addUnprocessedNode(layer, nodeData);\n          return;\n        }\n        const inboundLayer = createdLayers[inboundLayerName];\n        if (inboundLayer.inboundNodes.length <= inboundNodeIndex) {\n          addUnprocessedNode(layer, nodeData);\n          return;\n        }\n        const inboundNode = inboundLayer.inboundNodes[inboundNodeIndex];\n        inputTensors.push(inboundNode.outputTensors[inboundTensorIndex]);\n      }\n      // Call layer on its inputs, thus creating the node\n      // and building the layer if needed.\n      // Note: This has Eager vs Graph Implications.\n      if (inputTensors.length > 0) {\n        layer.apply(\n            generic_utils.singletonOrArray(inputTensors),\n            kwargs);  // was ** kwargs\n      }\n    }\n\n    /**\n     * Deserialize a layer, then call it on appropriate inputs.\n     * @param layerData: layer config dict.\n     * @throws ValueError: In case of improperly formatted `layer_data`\n     * dict.\n     */\n    function processLayer(layerData: serialization.ConfigDict|null) {\n      const layerName = layerData['name'] as string;\n      // Instantiate layer.\n      const layer =\n          deserializeLayer(\n              layerData,\n              config['customObjects'] != null ?\n                  config['customObjects'] as serialization.ConfigDict :\n                  {}) as Layer;\n      layer.setFastWeightInitDuringBuild(fastWeightInit);\n      createdLayers[layerName] = layer;\n      // Gather layer inputs.\n      const inboundNodesData =\n          layerData['inboundNodes'] as TensorKeyWithArgsArray[][];\n      inboundNodesData.forEach(nodeData => {\n        if (!(nodeData instanceof Array)) {\n          throw new ValueError(\n              `Corrupted configuration, expected array for nodeData: ${\n                  nodeData}`);\n        }\n        // We don't process nodes (i.e. make layer calls)\n        // on the fly because the inbound node may not yet exist,\n        // in case of layer shared at different topological depths\n        // (e.g.a model such as A(B(A(B(x)))))\n        addUnprocessedNode(layer, nodeData);\n      });\n    }\n\n    // First, we create all layers and enqueue nodes to be processed.\n    const name = config['name'];\n    const layersFromConfig = config['layers'] as serialization.ConfigDict[];\n    for (const layerData of layersFromConfig) {\n      processLayer(layerData);\n    }\n\n    // Then we process nodes in order of layer depth.\n    // Nodes that cannot yet be processed(if the inbound node\n    // does not yet exist) are re - enqueued, and the process\n    // is repeated until all nodes are processed.\n    while (!generic_utils.isObjectEmpty(unprocessedNodes)) {\n      for (const layerData of layersFromConfig) {\n        const layer = createdLayers[layerData['name'] as string];\n        if (layer.name in unprocessedNodes) {\n          const currentUnprocessedNodesForLayer = unprocessedNodes[layer.name];\n          delete unprocessedNodes[layer.name];\n          for (const nodeData of currentUnprocessedNodesForLayer) {\n            processNode(layer, nodeData);\n          }\n        }\n      }\n    }\n\n    const inputTensors: SymbolicTensor[] = [];\n    const outputTensors: SymbolicTensor[] = [];\n    const inputLayersFromConfig =\n        config['inputLayers'] as serialization.ConfigDict[];\n    for (const layerData of inputLayersFromConfig) {\n      const layerName = layerData[0] as string;\n      const nodeIndex = layerData[1] as number;\n      const tensorIndex = layerData[2] as number;\n      generic_utils.assert(layerName in createdLayers);\n      const layer = createdLayers[layerName];\n      const layerOutputTensors = layer.inboundNodes[nodeIndex].outputTensors;\n      inputTensors.push(layerOutputTensors[tensorIndex]);\n    }\n    const outputLayersFromConfig =\n        config['outputLayers'] as serialization.ConfigDict[];\n    for (const layerData of outputLayersFromConfig) {\n      const layerName = layerData[0] as string;\n      const nodeIndex = layerData[1] as number;\n      const tensorIndex = layerData[2] as number;\n      generic_utils.assert(layerName in createdLayers);\n      const layer = createdLayers[layerName];\n      const layerOutputTensors = layer.inboundNodes[nodeIndex].outputTensors;\n      outputTensors.push(layerOutputTensors[tensorIndex]);\n    }\n    return new cls({inputs: inputTensors, outputs: outputTensors, name});\n  }\n\n  /**\n   * Determine whether the container is stateful.\n   *\n   * Porting Note: this is the equivalent of the stateful @property of\n   *   the Container class in PyKeras.\n   */\n  override get stateful(): boolean {\n    // Porting Note: This check is to prevent inadvertent setting of the\n    //   _stateful property of the Container instance.\n    if (this._stateful) {\n      throw new ValueError(\n          'Container instance unexpectedly has _stateful = true. The ' +\n          'statefulness of a Container is determined by the Layers it ' +\n          'contains. Its _stateful property must remain the default false.');\n    }\n    for (const layer of this.layers) {\n      if (layer.stateful) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Reset the state of all stateful constituent layers (if any).\n   *\n   * Examples of stateful layers include RNN layers whose `stateful` property\n   * is set as `true`.\n   */\n  override resetStates() {\n    tidy(() => {\n      this.layers.forEach(layer => {\n        // tslint:disable:no-any\n        if (layer.stateful) {\n          layer.resetStates();\n        }\n        // tslint:enable:no-any\n      });\n    });\n  }\n}\n"],"names":[],"mappings":";;;;AAAA;;;;;;;;GAQG,CAEH,6CAAA,EAA+C;AAE/C,OAAO,EAAgD,IAAI,EAAC,MAAM,uBAAuB,CAAC;AAE1F,OAAO,EAAC,MAAM,EAAC,MAAM,kBAAkB,CAAC;AACxC,OAAO,EAAC,mBAAmB,EAAE,YAAY,EAAE,UAAU,EAAC,MAAM,WAAW,CAAC;AAIxE,OAAO,EAAC,WAAW,IAAI,gBAAgB,EAAC,MAAM,yBAAyB,CAAC;AAExE,OAAO,KAAK,aAAa,MAAM,wBAAwB,CAAC;AACxD,OAAO,EAAC,mBAAmB,EAAC,MAAM,8BAA8B,CAAC;AACjE,OAAO,KAAK,WAAW,MAAM,sBAAsB,CAAC;AACpD,OAAO,EAAC,aAAa,EAAgB,MAAM,cAAc,CAAC;AAC1D,OAAO,EAAC,OAAO,IAAI,aAAa,EAAC,MAAM,YAAY,CAAC;AAEpD,OAAO,EAAC,OAAO,EAAE,QAAQ,EAAC,MAAM,YAAY,CAAC;AAC7C,OAAO,EAAC,UAAU,EAAC,MAAM,eAAe,CAAC;AACzC,OAAO,EAAgB,KAAK,EAAE,IAAI,EAAiB,MAAM,YAAY,CAAC;;;;;;;;;;;;;AAStE,4EAA4E;AAC5E,eAAe;AACf,MAAM,uBAAuB,GAAG,CAAC,OAAuB,EAAW,EAAE;IACnE,MAAM,IAAI,GAAG,MAAM,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;IAClC,IAAI,IAAI,CAAC,MAAM,KAAK,CAAC,EAAE;QACrB,OAAO,KAAK,CAAC;KACd;IACD,MAAM,GAAG,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC;IAC/B,OAAO,CAAC,KAAK,CAAC,QAAQ,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,GAAG,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC;AACnD,CAAC,CAAC;AASI,MAAgB,SAAU,SAAQ,gTAAK;IAoC3C,YAAY,IAAmB,CAAA;QAC7B,yCAAyC;QACzC,KAAK,CAAC,CAAA,CAAE,CAAC,CAAC;QApBZ,IAAA,CAAA,cAAc,GAAG,IAAI,GAAG,EAAU,CAAC;QAqBjC,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;QACtB,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,EAAE;YACrB,MAAM,MAAM,GAAG,IAAI,CAAC,YAAY,EAAE,CAAC,WAAW,EAAE,CAAC;YACjD,IAAI,CAAC,IAAI,OAAG,+SAAM,EAAC,MAAM,CAAC,CAAC;SAC5B;QAED,IAAI,CAAC,eAAe,GAAG,KAAK,CAAC;QAC7B,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC;QAEvB,8DAA8D;QAE9D,iCAAiC;QACjC,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,EAAE;YAC9B,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC,KAAK,EAAE,CAAC;SACnC,MAAM;YACL,IAAI,CAAC,MAAM,GAAG;gBAAC,IAAI,CAAC,MAAM;aAAC,CAAC;SAC7B;QACD,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,OAAO,CAAC,EAAE;YAC/B,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,CAAC,KAAK,EAAE,CAAC;SACrC,MAAM;YACL,IAAI,CAAC,OAAO,GAAG;gBAAC,IAAI,CAAC,OAAO;aAAC,CAAC;SAC/B;QAED,kCAAkC;QAClC,IAAI,aAAa,CAAC,uSAAM,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,MAAM,KAAK,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE;YACnE,MAAM,IAAI,ySAAU,CAChB,4CAA4C,GAC5C,wDAAwD,GACxD,GAAG,IAAI,CAAC,MAAM,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;SACxC;QAED,mCAAmC;QACnC,IAAI,aAAa,CAAC,uSAAM,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,MAAM,KAAK,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE;YACrE,OAAO,CAAC,IAAI,CACR,wDAAwD,GACxD,8CAA8C,GAC9C,GAAG,IAAI,CAAC,OAAO,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;SACzC;QAED;;;UAGE,CACF,IAAI,CAAC,WAAW,GAAG,EAAE,CAAC;QACtB,IAAI,CAAC,sBAAsB,GAAG,EAAE,CAAC;QACjC,IAAI,CAAC,wBAAwB,GAAG,EAAE,CAAC;QACnC;;;UAGE,CACF,IAAI,CAAC,YAAY,GAAG,EAAE,CAAC;QACvB,IAAI,CAAC,uBAAuB,GAAG,EAAE,CAAC;QAClC,IAAI,CAAC,yBAAyB,GAAG,EAAE,CAAC;QACpC;;;UAGE,CACF,IAAI,CAAC,MAAM,GAAG,EAAE,CAAC;QAEjB;;;UAGE,CACF,IAAI,CAAC,qBAAqB,GAAG,EAAE,CAAC;QAEhC,mEAAmE;QACnE,WAAW;QACX;;;;;;UAME,CACF,+BAA+B;QAC/B,8BAA8B;QAE9B,2BAA2B;QAC3B,KAAK,MAAM,CAAC,IAAI,IAAI,CAAC,OAAO,CAAE;YAC5B,MAAM,KAAK,GAAG,CAAC,CAAC,WAAW,CAAC;YAC5B,MAAM,SAAS,GAAG,CAAC,CAAC,SAAS,CAAC;YAC9B,MAAM,WAAW,GAAG,CAAC,CAAC,WAAW,CAAC;YAClC,IAAI,CAAC,YAAY,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YAC9B,IAAI,CAAC,uBAAuB,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;YAC7C,IAAI,CAAC,yBAAyB,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;SAClD;QAED,kDAAkD;QAElD,0BAA0B;QAC1B,KAAK,MAAM,CAAC,IAAI,IAAI,CAAC,MAAM,CAAE;YAC3B,MAAM,KAAK,GAAG,CAAC,CAAC,WAAW,CAAC;YAC5B,MAAM,SAAS,GAAG,CAAC,CAAC,SAAS,CAAC;YAC9B,MAAM,WAAW,GAAG,CAAC,CAAC,WAAW,CAAC;YAClC;;;cAGE,CACF,aAAa,CAAC,uSAAM,CAAC,SAAS,KAAK,CAAC,EAAE,0BAA0B,CAAC,CAAC;YAClE,aAAa,CAAC,uSAAM,CAAC,WAAW,KAAK,CAAC,EAAE,4BAA4B,CAAC,CAAC;YACtE,IAAI,CAAC,WAAW,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YAC7B,IAAI,CAAC,sBAAsB,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;YAC5C,IAAI,CAAC,wBAAwB,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;SACjD;QAED,8CAA8C;QAC9C,IAAI,CAAC,UAAU,GAAG,EAAE,CAAC;QACrB,IAAI,CAAC,WAAW,GAAG,EAAE,CAAC;QACtB,IAAI,CAAC,eAAe,GAAG,EAAE,CAAC;QAC1B,IAAI,CAAC,cAAc,GAAG,EAAE,CAAC;QACzB,IAAI,CAAC,eAAe,GAAG,EAAE,CAAC;QAC1B,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,WAAW,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;YAChD,MAAM,KAAK,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;YAClC,qCAAqC;YACrC,IAAI,CAAC,CAAC,KAAK,YAAY,wTAAU,CAAC,EAAE;gBAClC,MAAM,IAAI,SAAS,CACf,4DAA4D,GAC5D,CAAA,iBAAA,EAAoB,IAAI,CAAC,MAAM,CAAA,EAAA,CAAI,GACnC,CAAA,MAAA,EAAS,CAAC,CAAA,sBAAA,CAAwB,GAClC,CAAA,gBAAA,EAAmB,KAAK,CAAC,YAAY,EAAE,CAAA,CAAA,CAAG,CAAC,CAAC;aACjD;YACD,IAAI,CAAC,UAAU,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;YACjC,IAAI,CAAC,eAAe,CAAC,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC,CAAC;YAEjD,IAAI,CAAC,cAAc,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;SACtC;QACD,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,YAAY,CAAE;YACrC,IAAI,CAAC,WAAW,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;SACnC;QAED,IAAI,CAAC,mBAAmB,GAAG,IAAI,CAAC,MAAM,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,KAAK,CAAC,CAAC;QACzD,IAAI,CAAC,oBAAoB,GAAG,IAAI,CAAC,OAAO,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,KAAK,CAAC,CAAC;QAE3D;;;UAGE,CACF,8CAA8C;QAC9C,MAAM,WAAW,GAA+B,CAAA,CAAE,CAAC;QACnD,kCAAkC;QAClC,MAAM,YAAY,GAA6B,CAAA,CAAE,CAAC;QAClD,MAAM,YAAY,GAAgC,CAAA,CAAE,CAAC;QACrD,2BAA2B;QAC3B,MAAM,cAAc,GAA+B,CAAA,CAAE,CAAC;QACtD,MAAM,YAAY,GAAgC,CAAA,CAAE,CAAC;QACrD,MAAM,sBAAsB,GAAW,EAAE,CAAC;QAE1C;;;;;;;;;;;;;;;;;WAiBG,CACH,MAAM,eAAe,GACjB,CAAC,MAAsB,EAAE,aAAqB,EAAE,eAAuB,EACtE,KAAa,EAAE,SAAkB,EAAE,WAAoB,EAAE,EAAE;YAC1D,IAAI,KAAK,IAAI,IAAI,IAAI,SAAS,IAAI,IAAI,IAAI,WAAW,IAAI,IAAI,EAAE;gBAC7D,KAAK,GAAG,MAAM,CAAC,WAAW,CAAC;gBAC3B,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;gBAC7B,WAAW,GAAG,MAAM,CAAC,WAAW,CAAC;aAClC;YACD,MAAM,IAAI,GAAG,KAAK,CAAC,YAAY,CAAC,SAAS,CAAC,CAAC;YAE3C,kBAAkB;YAClB,IAAI,eAAe,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;gBACxC,MAAM,IAAI,2SAAY,CAClB,CAAA,WAAA,EAAc,MAAM,CAAC,IAAI,CAAA,WAAA,EAAc,KAAK,CAAC,IAAI,CAAA,EAAA,CAAI,GACrD,qBAAqB,CAAC,CAAC;aAC5B;YAED,yCAAyC;YACzC,IAAI,aAAa,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;gBACtC,OAAO;aACR;YAED,yBAAyB;YACzB,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC,CAAC;YAE7D,+CAA+C;YAC/C,IAAI,CAAC,CAAC,KAAK,CAAC,EAAE,IAAI,YAAY,CAAC,EAAE;gBAC/B,YAAY,CAAC,KAAK,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC,MAAM,CAAC;aAC3D;YAED,IAAI,eAAe,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;gBACxC,eAAe,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;aAC5B;YAED,4DAA4D;YAC5D,MAAM,gBAAgB,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,CAAC;YACnD,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,gBAAgB,EAAE,CAAC,EAAE,CAAE;gBACzC,MAAM,CAAC,GAAG,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC;gBAC/B,MAAM,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;gBACpC,MAAM,SAAS,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;gBACtC,MAAM,WAAW,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;gBAC1C,eAAe,CACX,CAAC,EAAE,aAAa,EAAE,eAAe,EAAE,KAAK,EAAE,SAAS,EACnD,WAAW,CAAC,CAAC;aAClB;YACD,aAAa,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YACzB,MAAO,eAAe,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,CAAE;gBACzC,eAAe,CAAC,MAAM,CAAC,eAAe,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC;aAC1D;YACD,sBAAsB,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QACpC,CAAC,CAAC;QAEN,MAAM,aAAa,GAAW,EAAE,CAAC;QACjC,MAAM,eAAe,GAAW,EAAE,CAAC;QACnC,KAAK,MAAM,CAAC,IAAI,IAAI,CAAC,OAAO,CAAE;YAC5B,eAAe,CAAC,CAAC,EAAE,aAAa,EAAE,eAAe,CAAC,CAAC;SACpD;QAED,MAAM,8BAA8B,GAChC,sBAAsB,CAAC,KAAK,EAAE,CAAC,OAAO,EAAE,CAAC;QAC7C,KAAK,MAAM,IAAI,IAAI,8BAA8B,CAAE;YACjD,YAAY,CAAC,IAAI,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC;YAC7B,qEAAqE;YACrE,IAAI,CAAC,CAAC,IAAI,CAAC,EAAE,IAAI,WAAW,CAAC,EAAE;gBAC7B,WAAW,CAAC,IAAI,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC;aAC1B;YACD,IAAI,KAAK,GAAG,WAAW,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;YAEjC,8CAA8C;YAC9C,MAAM,aAAa,GACf,AAAC,YAAY,CAAC,IAAI,CAAC,aAAa,CAAC,EAAE,CAAC,IAAI,IAAI,CAAC,CAAC,CACzC,CAAC,CAAC,CAAC,CACH,YAAY,CAAC,IAAI,CAAC,aAAa,CAAC,EAAE,CAAC,CAAC,CAAC;YAE9C;;;;cAIE,CACF,KAAK,GAAG,IAAI,CAAC,GAAG,CAAC,KAAK,EAAE,aAAa,CAAC,CAAC;YACvC,YAAY,CAAC,IAAI,CAAC,aAAa,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC;YAC5C,cAAc,CAAC,IAAI,CAAC,aAAa,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC;YAC3D,WAAW,CAAC,IAAI,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC;YAE7B,qCAAqC;YACrC,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;gBAClD,MAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;gBAC3C,MAAM,SAAS,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;gBACtC,MAAM,WAAW,GAAG,YAAY,CAAC,YAAY,CAAC,SAAS,CAAC,CAAC;gBACzD,MAAM,aAAa,GACf,AAAC,WAAW,CAAC,WAAW,CAAC,EAAE,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CACH,WAAW,CAAC,WAAW,CAAC,EAAE,CAAC,CAAC,CAAC;gBACxE,WAAW,CAAC,WAAW,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,GAAG,CAAC,KAAK,GAAG,CAAC,EAAE,aAAa,CAAC,CAAC;gBACjE,YAAY,CAAC,WAAW,CAAC,EAAE,CAAC,GAAG,WAAW,CAAC;aAC5C;SACF;QAED,sDAAsD;QACtD,MAAM,YAAY,GAA8B,CAAA,CAAE,CAAC;QACnD,IAAK,MAAM,MAAM,IAAI,WAAW,CAAE;YAChC,MAAM,KAAK,GAAG,WAAW,CAAC,MAAM,CAAC,CAAC;YAClC,IAAI,CAAC,CAAC,KAAK,IAAI,YAAY,CAAC,EAAE;gBAC5B,YAAY,CAAC,KAAK,CAAC,GAAG,EAAE,CAAC;aAC1B;YACD,YAAY,CAAC,KAAK,CAAC,CAAC,IAAI,CAAC,YAAY,CAAC,MAAM,CAAC,CAAC,CAAC;SAChD;QAED,uDAAuD;QACvD,MAAM,aAAa,GAA+B,CAAA,CAAE,CAAC;QACrD,IAAK,MAAM,OAAO,IAAI,YAAY,CAAE;YAClC,MAAM,KAAK,GAAG,YAAY,CAAC,OAAO,CAAC,CAAC;YACpC,IAAI,CAAC,CAAC,KAAK,IAAI,aAAa,CAAC,EAAE;gBAC7B,aAAa,CAAC,KAAK,CAAC,GAAG,EAAE,CAAC;aAC3B;YACD,aAAa,CAAC,KAAK,CAAC,CAAC,IAAI,CAAC,cAAc,CAAC,OAAO,CAAC,CAAC,CAAC;SACpD;QAED,mCAAmC;QACnC,IAAI,SAAS,GAAG,MAAM,CAAC,IAAI,CAAC,aAAa,CAAC,CACrB,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,QAAQ,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CACzB,IAAI,CAAC,aAAa,CAAC,qTAAoB,CAAC,CAAC;QAE9D,0CAA0C;QAC1C,IAAI,CAAC,MAAM,GAAG,EAAE,CAAC;QACjB,KAAK,MAAM,KAAK,IAAI,SAAS,CAAE;YAC7B,MAAM,cAAc,GAAG,aAAa,CAAC,KAAK,CAAC,CAAC;YAC5C,wDAAwD;YACxD,yCAAyC;YACzC,cAAc,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE;gBAC3B,MAAM,MAAM,GAAG,YAAY,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;gBAClC,MAAM,MAAM,GAAG,YAAY,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;gBAClC,IAAI,MAAM,GAAG,MAAM,EAAE;oBACnB,OAAO,CAAC,CAAC,CAAC;iBACX;gBACD,IAAI,MAAM,GAAG,MAAM,EAAE;oBACnB,OAAO,CAAC,CAAC;iBACV;gBACD,OAAO,CAAC,CAAC;YACX,CAAC,CAAC,CAAC;YACH,KAAK,MAAM,KAAK,IAAI,cAAc,CAAE;gBAClC,IAAI,KAAK,YAAY,SAAS,EAAE;oBAC9B,IAAI,CAAC,qBAAqB,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;iBACxC;gBACD,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;aACzB;SACF;QACD,IAAI,CAAC,aAAa,GAAG,aAAa,CAAC;QAEnC,kCAAkC;QAClC,SAAS,GAAG,MAAM,CAAC,IAAI,CAAC,YAAY,CAAC,CACpB,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,QAAQ,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CACzB,IAAI,CAAC,aAAa,CAAC,qTAAoB,CAAC,CAAC;QAE1D,kDAAkD;QAClD,+CAA+C;QAC/C,iDAAiD;QACjD,MAAM,iBAAiB,GAAG,IAAI,CAAC,MAAM,CAAC,KAAK,EAAE,CAAC;QAE9C,iCAAiC;QACjC,MAAM,uBAAuB,GAAa,EAAE,CAAC;QAC7C,KAAK,MAAM,KAAK,IAAI,SAAS,CAAE;YAC7B,KAAK,MAAM,IAAI,IAAI,YAAY,CAAC,KAAK,CAAC,CAAE;gBACtC,MAAM,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC;gBACjC,IAAI,KAAK,IAAI,IAAI,EAAE;oBACjB,KAAK,MAAM,CAAC,IAAI,IAAI,CAAC,YAAY,CAAE;wBACjC,IAAI,iBAAiB,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;4BACvC,MAAM,IAAI,2SAAY,CAClB,CAAA,mDAAA,EAAsD,CAAC,EAAE,GACzD,CAAA,WAAA,EAAc,KAAK,CAAC,IAAI,CAAA,GAAA,CAAK,GAC7B,sDAAsD,GACtD,CAAA,OAAA,EAAU,uBAAuB,EAAE,CAAC,CAAC;yBAC1C;qBACF;oBACD,KAAK,MAAM,CAAC,IAAI,IAAI,CAAC,aAAa,CAAE;wBAClC,iBAAiB,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;qBAC3B;oBACD,uBAAuB,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;iBAC1C;aACF;SACF;QAED,iDAAiD;QACjD,IAAI,CAAC,YAAY,GAAG,YAAY,CAAC;QAEjC,+DAA+D;QAC/D,0DAA0D;QAC1D,MAAM,QAAQ,GAAG,IAAI,CAAC,MAAM,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,IAAI,CAAC,CAAC;QAC9C,KAAK,MAAM,IAAI,IAAI,QAAQ,CAAE;YAC3B,MAAM,cAAc,GAAG,QAAQ,CAAC,MAAM,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,KAAK,IAAI,CAAC,CAAC,MAAM,CAAC;YAC/D,IAAI,cAAc,KAAK,CAAC,EAAE;gBACxB,MAAM,IAAI,2SAAY,CAClB,CAAA,UAAA,EAAa,IAAI,CAAA,UAAA,EAAa,cAAc,CAAA,OAAA,CAAS,GACrD,+DAA+D,GAC/D,IAAI,CAAC,SAAS,CAAC,QAAQ,CAAC,CAAC,CAAC;aAC/B;SACF;QAED,oBAAoB;QACpB,sDAAsD;QACtD,yCAAyC;QACzC,kDAAkD;QAClD,IAAI,CAAC,aAAa,GAAG,EAAE,CAAC;QACxB,6DAA6D;QAC7D,IAAI,CAAC,YAAY,GAAG,EAAE,CAAC;QAEvB,+DAA+D;QAC/D,gCAAgC;QAChC,gDAAgD;QAChD,IAAI,+SAAI,CAAC;YACP,aAAa,EAAE,IAAI;YACnB,aAAa,EAAE,EAAE;YACjB,WAAW,EAAE,EAAE;YACf,aAAa,EAAE,EAAE;YACjB,YAAY,EAAE,IAAI,CAAC,MAAM;YACzB,aAAa,EAAE,IAAI,CAAC,OAAO;YAC3B,UAAU,EAAE,IAAI,CAAC,MAAM,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,IAAI,CAAC;YACtC,WAAW,EAAE,IAAI,CAAC,OAAO,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,IAAI,CAAC;YACxC,WAAW,EAAE,IAAI,CAAC,MAAM,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,KAAK,CAAC;YAC1C,YAAY,EAAE,IAAI,CAAC,OAAO,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,KAAK,CAAC;SAC7C,CAAC,CAAC;QACH,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;QAClB,IAAI,CAAC,SAAS,GAAG,CAAC,CAAC,CAAE,kDAAkD;IACzE,CAAC;IAEkB,iBAAiB,GAAA;QAClC,IAAI,IAAI,CAAC,SAAS,KAAK,CAAC,EAAE;YACxB,MAAM,IAAI,KAAK,CAAC,CAAA,WAAA,EAAc,IAAI,CAAC,IAAI,CAAA,sBAAA,CAAwB,CAAC,CAAC;SAClE;IACH,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;OAyBG,CACM,OAAO,GAAA;QACd,IAAI,CAAC,iBAAiB,EAAE,CAAC;QACzB,MAAM,MAAM,GACQ;YAAC,oBAAoB,EAAE,IAAI;YAAE,oBAAoB,EAAE,CAAC;QAAA,CAAC,CAAC;QAC1E,IAAI,EAAE,IAAI,CAAC,SAAS,KAAK,CAAC,EAAE;YAC1B,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;gBAC/B,MAAM,CAAC,oBAAoB,IAAI,KAAK,CAAC,OAAO,EAAE,CAAC,oBAAoB,CAAC;aACrE;YAED,0EAA0E;YAC1E,uEAAuE;YACvE,KAAK,MAAM,SAAS,IAAI,IAAI,CAAC,qBAAqB,CAAE;gBAClD,MAAM,CAAC,oBAAoB,IAAI,SAAS,CAAC,OAAO,EAAE,CAAC,oBAAoB,CAAC;aACzE;SACF;QACD,MAAM,CAAC,oBAAoB,GAAG,IAAI,CAAC,SAAS,CAAC;QAC7C,OAAO,MAAM,CAAC;IAChB,CAAC;IAED,IAAa,SAAS,GAAA;QACpB,OAAO,IAAI,CAAC,UAAU,CAAC;IACzB,CAAC;IAED,IAAa,SAAS,CAAC,SAAkB,EAAA;QACvC,IAAI,CAAC,MAAM,CAAC,OAAO,EAAC,KAAK,CAAC,EAAE;YAC1B,kCAAkC;YAChC,KAAa,CAAC,iBAAqC,CAChD,OAAO,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,SAAS,GAAG,SAAS,CAAC,CAAC;QAC7C,CAAC,CAAC,CAAC;QACH,IAAI,CAAC,UAAU,GAAG,SAAS,CAAC;IAC9B,CAAC;IAED,IAAa,gBAAgB,GAAA;QAC3B,gEAAgE;QAChE,mEAAmE;QACnE,wBAAwB;QACxB,IAAI,IAAI,CAAC,iBAAiB,CAAC,MAAM,GAAG,CAAC,EAAE;YACrC,MAAM,IAAI,ySAAU,CAChB,6DAA6D,GAC7D,0DAA0D,GAC1D,sDAAsD,GACtD,+CAA+C,CAAC,CAAC;SACtD;QAED,IAAI,CAAC,IAAI,CAAC,SAAS,EAAE;YACnB,OAAO,EAAE,CAAC;SACX;QACD,IAAI,OAAO,GAAoB,EAAE,CAAC;QAClC,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;YAC/B,OAAO,GAAG,OAAO,CAAC,MAAM,CAAC,KAAK,CAAC,gBAAgB,CAAC,CAAC;SAClD;QACD,OAAO,OAAO,CAAC;IACjB,CAAC;IAED,IAAa,mBAAmB,GAAA;QAC9B,MAAM,OAAO,GAAoB,EAAE,CAAC;QACpC,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;YAC/B,OAAO,CAAC,IAAI,CAAC,GAAG,KAAK,CAAC,mBAAmB,CAAC,CAAC;SAC5C;QACD,IAAI,CAAC,IAAI,CAAC,SAAS,EAAE;YACnB,MAAM,gBAAgB,GAAoB,EAAE,CAAC;YAC7C,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;gBAC/B,gBAAgB,CAAC,IAAI,CAAC,GAAG,KAAK,CAAC,gBAAgB,CAAC,CAAC;aAClD;YACD,OAAO,gBAAgB,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC;SACzC;QACD,OAAO,OAAO,CAAC;IACjB,CAAC;IAED,IAAa,OAAO,GAAA;QAClB,OAAO,IAAI,CAAC,gBAAgB,CAAC,MAAM,CAAC,IAAI,CAAC,mBAAmB,CAAC,CAAC;IAChE,CAAC;IAED;;;;;;;;;;;;;;OAcG,CACH,WAAW,CAAC,OAAuB,EAAE,MAAM,GAAG,IAAI,EAAA;QAChD,MAAM,YAAY,GAAoC,CAAA,CAAE,CAAC;QACzD,IAAI,iBAAiB,GAAG,CAAC,CAAC;QAC1B,MAAM,4BAA4B,GAAG,uBAAuB,CAAC,OAAO,CAAC,CAAC;QACtE,IAAI,4BAA4B,EAAE;YAChC,IAAI,CAAC,YAAY,CAAC,OAAO,CAAC,CAAC;SAC5B;QACD,kCAAkC;QAClC,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;YAC/B,KAAK,MAAM,CAAC,KAAK,EAAE,MAAM,CAAC,IAAI,KAAK,CAAC,OAAO,CAAC,OAAO,EAAE,CAAE;gBACrD,qCAAqC;gBACrC,8CAA8C;gBAC9C,MAAM,UAAU,GAAG,4BAA4B,CAAC,CAAC,CAC7C,GAAG,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,GAAG,GAAG,GAAG,KAAK,EAAE,CAAC,CAAC,CAClE,MAAM,CAAC,YAAY,CAAC;gBACxB,IAAI,YAAY,CAAC,UAAU,CAAC,IAAI,IAAI,EAAE;oBACpC,MAAM,IAAI,ySAAU,CAAC,CAAA,uBAAA,EAA0B,UAAU,EAAE,CAAC,CAAC;iBAC9D;gBACD,YAAY,CAAC,UAAU,CAAC,GAAG,MAAM,CAAC;gBAClC,iBAAiB,EAAE,CAAC;aACrB;SACF;QAED,MAAM,iBAAiB,GAAmC,EAAE,CAAC;QAC7D,IAAK,MAAM,IAAI,IAAI,OAAO,CAAE;YAC1B,+DAA+D;YAC/D,sDAAsD;YACtD,uBAAuB;YACvB,IAAI,aAAa,GAAG,IAAI,CAAC;YACzB,IAAI,YAAY,CAAC,IAAI,CAAC,IAAI,IAAI,EAAE;gBAC9B,MAAM,MAAM,GAAG,IAAI,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC;gBAC/B,MAAM,gBAAgB,GAClB,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC;oBAAC,MAAM,CAAC,MAAM,CAAC,MAAM,GAAG,CAAC,CAAC;iBAAC,CAAC,CAAC;gBAC5D,aAAa,GAAG,gBAAgB,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;aAC5C;YACD,IAAI,YAAY,CAAC,aAAa,CAAC,IAAI,IAAI,EAAE;gBACvC,iBAAiB,CAAC,IAAI,CAAC;oBAAC,YAAY,CAAC,aAAa,CAAC;oBAAE,OAAO,CAAC,IAAI,CAAC;iBAAC,CAAC,CAAC;aACtE,MAAM,IAAI,MAAM,EAAE;gBACjB,MAAM,IAAI,ySAAU,CAChB,CAAA,6CAAA,EAAgD,IAAI,EAAE,CAAC,CAAC;aAC7D;YACD,OAAO,YAAY,CAAC,aAAa,CAAC,CAAC;SACpC;QAED,IAAI,MAAM,EAAE;YACV,kCAAkC;YAClC,MAAM,UAAU,GAAa,EAAE,CAAC;YAChC,IAAK,MAAM,IAAI,IAAI,YAAY,CAAE;gBAC/B,UAAU,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;aACvB;YACD,IAAI,UAAU,CAAC,MAAM,GAAG,CAAC,EAAE;gBACzB,MAAM,IAAI,ySAAU,CAChB,GAAG,UAAU,CAAC,MAAM,CAAA,IAAA,EAChB,iBAAiB,CAAA,sBAAA,CAAwB,GAC7C,GAAG,UAAU,EAAE,CAAC,CAAC;aACtB;SACF;YAED,+SAAa,EAAC,iBAAiB,CAAC,CAAC;IACnC,CAAC;IAES,YAAY,CAAC,OAAuB,EAAA;QAC5C,IAAK,MAAM,GAAG,IAAI,MAAM,CAAC,IAAI,CAAC,OAAO,CAAC,CAAE;YACtC,MAAM,SAAS,GAAG,GAAG,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC;YACjC,MAAM,IAAI,GAAG;gBAAC,MAAM;gBAAE,+BAA+B;aAAC,CAAC;YACvD,0EAA0E;YAC1E,qEAAqE;YACrE,uBAAuB;YACvB,0EAA0E;YAC1E,4DAA4D;YAC5D,uEAAuE;YACvE,sBAAsB;YACtB,MAAM,MAAM,GAAG,SAAS,CACJ,GAAG,EAAC,GAAG,CAAC,EAAE;gBACT,IAAI,GAAG,CAAC,UAAU,CAAC,GAAG,CAAC,EAAE;oBACvB,OAAO,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;iBACrB;gBACD,OAAO,GAAG,CAAC;YACb,CAAC,CAAC,CACD,MAAM,EAAC,GAAG,CAAC,EAAE,AAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,CAAC,CAClC,IAAI,CAAC,GAAG,CAAC,CAAC;YAC9B,IAAI,MAAM,KAAK,GAAG,EAAE;gBAClB,OAAO,CAAC,MAAM,CAAC,GAAG,OAAO,CAAC,GAAG,CAAC,CAAC;gBAC/B,OAAO,OAAO,CAAC,GAAG,CAAC,CAAC;aACrB;SACF;IACH,CAAC;IAED;;;OAGG,CACO,aAAa,GAAA;QACrB,MAAM,SAAS,GAAG,IAAI,CAAC,SAAS,EAAE,CAAC;QACnC,MAAM,WAAW,GAA6B,CAAA,CAAE,CAAC;QACjD,WAAW,CAAC,WAAW,CAAC,GAAG,IAAI,CAAC,YAAY,EAAE,CAAC;QAC/C,WAAW,CAAC,QAAQ,CAAC,GAAG,SAAS,CAAC;QAClC,WAAW,CAAC,cAAc,CAAC,GAAG,CAAA,YAAA,EAAe,uSAAa,EAAE,CAAC;QAC7D,0DAA0D;QAC1D,YAAY;QACZ,WAAW,CAAC,SAAS,CAAC,GAAG,eAAe,CAAC;QACzC,OAAO,WAAW,CAAC;IACrB,CAAC;IAED;;;;;;;;;;OAUG,CACH,kCAAkC;IAClC,MAAM,CAAC,MAAY,EAAE,YAAY,GAAG,IAAI,EAAA;QACtC,MAAM,WAAW,OAAG,wUAAmB,EAAC,IAAI,CAAC,aAAa,EAAE,CAAe,CAAC;QAC5E,OAAO,YAAY,CAAC,CAAC,CAAC,IAAI,CAAC,SAAS,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC;IAClE,CAAC;IAED;;;;;;;;;;;;OAYG,CACM,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;QACnD,WAAO,iPAAI,EAAC,GAAG,EAAE;YACf,MAAM,GAAG,aAAa,CAAC,uSAAM,CAAC,MAAM,CAAC,CAAC;YACtC,MAAM,QAAQ,GAAG,IAAI,mTAAQ,EAAE,CAAC;YAChC,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBAC3C,QAAQ,CAAC,GAAG,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;aACzC;YACD,WAAO,kTAAO,EAAC,IAAI,CAAC,OAAO,EAAE,QAAQ,EAAE,MAAM,CAAsB,CAAC;QACtE,CAAC,CAAC,CAAC;IACL,CAAC;IAED;;;;;;;;OAQG,CACM,WAAW,CAAC,MAAuB,EAAE,IAAsB,EAAA;QAElE,WAAO,iPAAI,EAAC,GAAG,EAAE;YACf,MAAM,GAAG,aAAa,CAAC,uSAAM,CAAC,MAAM,CAAC,CAAC;YACtC,IAAI,KAAe,CAAC;YACpB,IAAI,IAAI,IAAI,IAAI,EAAE;gBAChB,KAAK,GAAG,aAAa,CAAC,6SAAY,CAAC,IAAI,EAAE,MAAM,CAAC,MAAM,CAAC,CAAC;aACzD,MAAM;gBACL,KAAK,GAAG,aAAa,CAAC,uSAAM,CAAC,IAAI,CAAC,CAAC;aACpC;YACD,oDAAoD;YACpD,OAAO,IAAI,CAAC,gBAAgB,CAAC,MAAM,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;QACjD,CAAC,CAAC,CAAC;IACL,CAAC;IAED;;;;;;;;OAQG,CACM,kBAAkB,CAAC,UAAyB,EAAA;QACnD,MAAM,WAAW,GAAG,WAAW,CAAC,mTAAkB,CAAC,UAAU,CAAC,CAAC;QAC/D,IAAI,WAAW,CAAC,MAAM,KAAK,IAAI,CAAC,WAAW,CAAC,MAAM,EAAE;YAClD,MAAM,IAAI,ySAAU,CAChB,CAAA,4BAAA,EAA+B,UAAU,CAAA,EAAA,CAAI,GAC7C,CAAA,UAAA,EAAa,IAAI,CAAC,WAAW,CAAC,MAAM,CAAA,eAAA,CAAiB,CAAC,CAAC;SAC5D;QAED,kCAAkC;QAClC,MAAM,oBAAoB,GAAgC,CAAA,CAAE,CAAC;QAC7D,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,WAAW,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;YAC3C,MAAM,KAAK,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;YAClC,MAAM,UAAU,GAAG,WAAW,CAAC,CAAC,CAAC,CAAC;YAClC,uDAAuD;YACvD,oDAAoD;YACpD,MAAM,QAAQ,GAAG,KAAK,CAAC,IAAI,GAAG,MAAM,CAAC;YACrC,oBAAoB,CAAC,QAAQ,CAAC,GAAG,UAAU,CAAC;SAC7C;QAED,MAAM,SAAS,GAAG,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,YAAY,CAAC,CACzB,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,QAAQ,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CACzB,IAAI,CAAC,aAAa,CAAC,qTAAoB,CAAC,CAAC;QAChE,sCAAsC;QACtC,IAAI,SAAS,CAAC,MAAM,GAAG,CAAC,EAAE;YACxB,KAAK,MAAM,KAAK,IAAI,SAAS,CAAE;gBAC7B,MAAM,KAAK,GAAG,IAAI,CAAC,YAAY,CAAC,KAAK,CAAC,CAAC;gBACvC,KAAK,MAAM,IAAI,IAAI,KAAK,CAAE;oBACxB,+CAA+C;oBAC/C,MAAM,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC;oBACjC,IAAI,IAAI,CAAC,WAAW,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,EAAE,CAAC,CAAC,OAAO,CAAC,KAAK,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,EAAE;wBAE5D,SAAS;qBACV;oBACD,8DAA8D;oBAC9D,MAAM,WAAW,GAAY,EAAE,CAAC;oBAChC,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;wBAClD,MAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;wBAC3C,MAAM,SAAS,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;wBACtC,MAAM,WAAW,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;wBAC1C,MAAM,QAAQ,GAAG,GAAG,YAAY,CAAC,IAAI,CAAA,CAAA,EAAI,SAAS,CAAA,CAAA,EAAI,WAAW,EAAE,CAAC;wBACpE,MAAM,UAAU,GAAG,oBAAoB,CAAC,QAAQ,CAAC,CAAC;wBAClD,WAAW,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;qBAC9B;oBAED,MAAM,WAAW,GAAG,KAAK,CAAC,kBAAkB,CACxC,aAAa,CAAC,iTAAgB,CAAC,WAAW,CAAC,CAAC,CAAC;oBAEjD,MAAM,YAAY,GAAG,WAAW,CAAC,mTAAkB,CAAC,WAAW,CAAC,CAAC;oBACjE,MAAM,SAAS,GAAG,KAAK,CAAC,YAAY,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC;oBACnD,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,YAAY,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;wBAC5C,MAAM,QAAQ,GAAG,GAAG,KAAK,CAAC,IAAI,CAAA,CAAA,EAAI,SAAS,CAAA,CAAA,EAAI,CAAC,EAAE,CAAC;wBACnD,oBAAoB,CAAC,QAAQ,CAAC,GAAG,YAAY,CAAC,CAAC,CAAC,CAAC;qBAClD;iBACF;aACF;SACF;QAED,sDAAsD;QACtD,MAAM,YAAY,GAAY,EAAE,CAAC;QACjC,MAAM,eAAe,GAAa,EAAE,CAAC;QACrC,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,YAAY,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;YACjD,MAAM,KAAK,GAAG,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC;YACnC,MAAM,SAAS,GAAG,IAAI,CAAC,uBAAuB,CAAC,CAAC,CAAC,CAAC;YAClD,MAAM,WAAW,GAAG,IAAI,CAAC,yBAAyB,CAAC,CAAC,CAAC,CAAC;YACtD,MAAM,QAAQ,GAAG,GAAG,KAAK,CAAC,IAAI,CAAA,CAAA,EAAI,SAAS,CAAA,CAAA,EAAI,WAAW,EAAE,CAAC;YAC7D,eAAe,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;SAChC;QAED,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,eAAe,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;YAC/C,MAAM,GAAG,GAAG,eAAe,CAAC,CAAC,CAAC,CAAC;YAC/B,aAAa,CAAC,uSAAM,CAAC,GAAG,IAAI,oBAAoB,CAAC,CAAC;YAClD,YAAY,CAAC,IAAI,CAAC,oBAAoB,CAAC,GAAG,CAAC,CAAC,CAAC;SAC9C;QAED,mCAAmC;QACnC,OAAO,aAAa,CAAC,iTAAgB,CAAC,YAAY,CAAC,CAAC;IACtD,CAAC;IAED;;;;;;;;;OASG,CACO,gBAAgB,CAAC,MAAgB,EAAE,KAAgB,EAAA;QAE3D,IAAI,KAAK,IAAI,IAAI,EAAE;YACjB,KAAK,GAAG,aAAa,CAAC,6SAAY,CAAC,IAAI,EAAE,MAAM,CAAC,MAAM,CAAC,CAAC;SACzD;QAED,iDAAiD;QACjD,kCAAkC;QAClC,8CAA8C;QAC9C,qDAAqD;QACrD,iDAAiD;QACjD,MAAM,SAAS,GAA2C,CAAA,CAAE,CAAC;QAC7D,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YAC3C,MAAM,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;YACzB,MAAM,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;YACpB,MAAM,IAAI,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC;YACtB,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG;gBAAC,CAAC;gBAAE,IAAI;aAAC,CAAC;SAC7B;QAED,MAAM,SAAS,GAAG,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,YAAY,CAAC,CACzB,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,QAAQ,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CACzB,IAAI,CAAC,aAAa,CAAC,qTAAoB,CAAC,CAAC;QAChE,KAAK,MAAM,KAAK,IAAI,SAAS,CAAE;YAC7B,MAAM,KAAK,GAAG,IAAI,CAAC,YAAY,CAAC,KAAK,CAAC,CAAC;YACvC,KAAK,MAAM,IAAI,IAAI,KAAK,CAAE;gBACxB,+CAA+C;gBAC/C,MAAM,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC;gBACjC,MAAM,qBAAqB,GAAG,IAAI,CAAC,YAAY,CAAC;gBAChD,MAAM,sBAAsB,GAAG,IAAI,CAAC,aAAa,CAAC;gBAElD,4DAA4D;gBAC5D,uCAAuC;gBACvC,gCAAgC;gBAChC,MAAM,YAAY,GAAG,IAAI,KAAK,EAAoB,CAAC;gBACnD,KAAK,MAAM,CAAC,IAAI,qBAAqB,CAAE;oBACrC,IAAI,CAAC,CAAC,EAAE,IAAI,SAAS,EAAE;wBACrB,YAAY,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;qBACpC;iBACF;gBACD,IAAI,YAAY,CAAC,MAAM,KAAK,qBAAqB,CAAC,MAAM,EAAE;oBACxD,4DAA4D;oBAC5D,IAAI,MAAM,GAAW,CAAA,CAAE,CAAC;oBACxB,IAAI,eAAyB,CAAC;oBAC9B,IAAI,aAAuB,CAAC;oBAC5B,IAAI,aAAuB,CAAC;oBAC5B,IAAI,WAAqB,CAAC;oBAC1B,aAAa;oBACb,IAAI,IAAI,CAAC,QAAQ,IAAI,IAAI,EAAE;wBACzB,MAAM,GAAG,IAAI,CAAC,QAAQ,CAAC;qBACxB;oBACD,IAAI,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;wBAC7B,MAAM,CAAC,cAAc,EAAE,YAAY,CAAC,GAAG,YAAY,CAAC,CAAC,CAAC,CAAC;wBACvD,IAAI,MAAM,CAAC,MAAM,CAAC,IAAI,IAAI,EAAE;4BAC1B,MAAM,CAAC,MAAM,CAAC,GAAG,YAAY,CAAC;yBAC/B;wBACD,aAAa,GACT,aAAa,CAAC,uSAAM,CAAC,KAAK,CAAC,IAAI,CAAC,cAAc,EAAE,MAAM,CAAC,CAAC,CAAC;wBAC7D,WAAW,GAAG,aAAa,CAAC,uSAAM,CAC9B,KAAK,CAAC,WAAW,CAAC,cAAc,EAAE,YAAY,CAAC,CAAC,CAAC;wBACrD,eAAe,GAAG;4BAAC,cAAc;yBAAC,CAAC;wBACnC,aAAa,GAAG;4BAAC,YAAY;yBAAC,CAAC;qBAChC,MAAM;wBACL,eAAe,GAAG,YAAY,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;wBAC9C,aAAa,GAAG,YAAY,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;wBAC5C,IAAI,MAAM,CAAC,MAAM,CAAC,IAAI,IAAI,EAAE;4BAC1B,MAAM,CAAC,MAAM,CAAC,GAAG,aAAa,CAAC;yBAChC;wBACD,aAAa,GACT,aAAa,CAAC,uSAAM,CAAC,KAAK,CAAC,IAAI,CAAC,eAAe,EAAE,MAAM,CAAC,CAAC,CAAC;wBAC9D,WAAW,GAAG,aAAa,CAAC,uSAAM,CAC9B,KAAK,CAAC,WAAW,CAAC,eAAe,EAAE,aAAa,CAAC,CAAC,CAAC;qBACxD;oBAED,IAAI,KAAK,CAAC,mBAAmB,EAAE;wBAC7B,MAAM,IAAI,kTAAmB,CACzB,8DAA8D,GAC9D,2DAA2D,CAAC,CAAC;qBAClE;oBACD,mDAAmD;oBAEnD,qBAAqB;oBACrB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,sBAAsB,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;wBACtD,MAAM,CAAC,GAAG,sBAAsB,CAAC,CAAC,CAAC,CAAC;wBACpC,MAAM,CAAC,GAAG,aAAa,CAAC,CAAC,CAAC,CAAC;wBAC3B,MAAM,IAAI,GAAG,WAAW,CAAC,CAAC,CAAC,CAAC;wBAC5B,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG;4BAAC,CAAC;4BAAE,IAAI;yBAAC,CAAC;qBAC7B;iBACF;aACF;SACF;QAED,MAAM,aAAa,GAAa,EAAE,CAAC;QACnC,MAAM,WAAW,GAAa,EAAE,CAAC;QACjC,MAAM,YAAY,GAAY,EAAE,CAAC;QACjC,KAAK,MAAM,CAAC,IAAI,IAAI,CAAC,OAAO,CAAE;YAC5B,aAAa,CAAC,uSAAM,CAChB,CAAC,CAAC,EAAE,IAAI,SAAS,EAAE,CAAA,yBAAA,EAA4B,CAAC,CAAC,IAAI,CAAA,GAAA,EAAM,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;YACvE,MAAM,CAAC,MAAM,EAAE,IAAI,CAAC,GAAG,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;YACvC,YAAY,CAAC,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC;YAChC,aAAa,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;YAC3B,WAAW,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SACxB;QAED,8CAA8C;QAC9C,OAAO;YAAC,aAAa;YAAE,WAAW;YAAE,YAAY;SAAC,CAAC;IACpD,CAAC;IAED;;;;;;;OAOG,CACK,sBAAsB,CAAC,MAAe,EAAA;QAC5C,MAAM,iBAAiB,GAAgC,CAAA,CAAE,CAAC;QAC1D,IAAI,SAAiB,CAAC;QACtB,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;YAC/B,SAAS,GAAG,KAAK,YAAY,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;YAC/C,IAAK,IAAI,iBAAiB,GAAG,CAAC,EACzB,iBAAiB,GAAG,KAAK,CAAC,YAAY,CAAC,MAAM,EAAE,iBAAiB,EAAE,CAAE;gBACvE,MAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,iBAAiB,CAAC,CAAC;gBAC5D,IAAI,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;oBACpC,8BAA8B;oBAC9B,iBAAiB,CAAC,OAAO,CAAC,GAAG,SAAS,CAAC;oBACvC,SAAS,IAAI,CAAC,CAAC;iBAChB;aACF;SACF;QACD,OAAO,iBAAiB,CAAC;IAC3B,CAAC;IAwBD,QAAQ,CAAC,WAA2B,EAAE,KAAc,EAAA;QAClD,IAAI,KAAK,IAAI,IAAI,EAAE;YACjB,OAAO,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,CAAC;SAC9B,MAAM;YACL,IAAI,WAAW,IAAI,IAAI,EAAE;gBACvB,MAAM,IAAI,ySAAU,CAAC,4CAA4C,CAAC,CAAC;aACpE;YACD,IAAI,OAAO,WAAW,KAAK,QAAQ,EAAE;gBACnC,OAAO,IAAI,CAAC,SAAS,CAAC,WAAW,CAAC,CAAC;aACpC;SACF;QAED,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;YAC/B,IAAI,KAAK,CAAC,IAAI,KAAK,WAAW,EAAE;gBAC9B,OAAO,KAAK,CAAC;aACd;SACF;QACD,MAAM,IAAI,ySAAU,CAAC,CAAA,eAAA,EAAkB,WAAW,EAAE,CAAC,CAAC;IACxD,CAAC;IAED,SAAS,CAAC,KAAa,EAAA;QACrB,IAAI,IAAI,CAAC,MAAM,CAAC,MAAM,IAAI,KAAK,EAAE;YAC/B,MAAM,IAAI,ySAAU,CAChB,CAAA,qCAAA,EAAwC,KAAK,CAAA,iBAAA,CAAmB,GAChE,CAAA,IAAA,EAAO,IAAI,CAAC,MAAM,CAAC,MAAM,CAAA,UAAA,CAAY,CAAC,CAAC;SAC5C,MAAM;YACL,OAAO,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC;SAC3B;IACH,CAAC;IAED;;;;OAIG,CACM,eAAe,GAAA;QACtB,sEAAsE;QACtE,yEAAyE;QACzE,yEAAyE;QACzE,wBAAwB;QACxB,WAAO,iPAAI,EAAC,GAAG,EAAE;YACf,MAAM,MAAM,GAAa,EAAE,CAAC;YAC5B,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;gBAC/B,IAAK,IAAI,SAAS,GAAG,CAAC,EAAE,SAAS,GAAG,KAAK,CAAC,YAAY,CAAC,MAAM,EACxD,EAAE,SAAS,CAAE;oBAChB,MAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC;oBACpD,IAAI,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;wBACpC,MAAM,CAAC,IAAI,CAAC,GAAG,KAAK,CAAC,eAAe,EAAE,CAAC,CAAC;qBACzC;iBACF;aACF;YACD,wDAAwD;YACxD,OAAO,MAAM,CAAC;QAChB,CAAC,CAAC,CAAC;IACL,CAAC;IAEQ,SAAS,GAAA;QAChB,MAAM,MAAM,GAA6B;YAAC,IAAI,EAAE,IAAI,CAAC,IAAI;QAAA,CAAC,CAAC;QAE3D,sDAAsD;QACtD,0DAA0D;QAC1D,2CAA2C;QAC3C,MAAM,iBAAiB,GACnB,IAAI,CAAC,sBAAsB,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;QAE7C,gDAAgD;QAChD,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;YAC/B,MAAM,cAAc,GAAG,KAAK,CAAC,YAAY,EAAE,CAAC;YAC5C,MAAM,WAAW,GAAG,KAAK,CAAC,SAAS,EAAE,CAAC;YACtC,MAAM,oBAAoB,GAAG,EAAE,CAAC;YAChC,IAAK,IAAI,iBAAiB,GAAG,CAAC,EACzB,iBAAiB,GAAG,KAAK,CAAC,YAAY,CAAC,MAAM,EAAE,iBAAiB,EAAE,CAAE;gBACvE,MAAM,IAAI,GAAG,KAAK,CAAC,YAAY,CAAC,iBAAiB,CAAC,CAAC;gBACnD,MAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,iBAAiB,CAAC,CAAC;gBAC5D,IAAI,MAAM,GAAG,CAAA,CAAE,CAAC;gBAChB,IAAI,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;oBACpC,qCAAqC;oBACrC,+BAA+B;oBAC/B,IAAI,IAAI,CAAC,QAAQ,EAAE;wBACjB,IAAI;4BACF,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;4BAC9B,MAAM,GAAG,IAAI,CAAC,QAAQ,CAAC;yBACxB,CAAC,OAAO,GAAG,EAAE;4BACZ,OAAO,CAAC,IAAI,CACR,CAAA,MAAA,EAAS,KAAK,CAAC,IAAI,CAAA,YAAA,CAAc,GACjC,CAAA,oCAAA,CAAsC,GACtC,GAAG,IAAI,CAAC,QAAQ,CAAA,4BAAA,CAA8B,GAC9C,CAAA,0CAAA,CAA4C,GAC5C,CAAA,iCAAA,CAAmC,CAAC,CAAC;4BACzC,MAAM,GAAG,CAAA,CAAE,CAAC;yBACb;qBACF;oBACD,IAAI,IAAI,CAAC,aAAa,CAAC,MAAM,GAAG,CAAC,EAAE;wBACjC,MAAM,QAAQ,GAAG,EAAE,CAAC;wBACpB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;4BAClD,MAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;4BAC3C,MAAM,SAAS,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;4BACtC,MAAM,WAAW,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;4BAC1C,MAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,YAAY,EAAE,SAAS,CAAC,CAAC;4BAC3D,IAAI,YAAY,GAAG,iBAAiB,CAAC,OAAO,CAAC,CAAC;4BAC9C,IAAI,YAAY,IAAI,IAAI,EAAE;gCACxB,YAAY,GAAG,CAAC,CAAC;6BAClB;4BACD,QAAQ,CAAC,IAAI,CACT;gCAAC,YAAY,CAAC,IAAI;gCAAE,YAAY;gCAAE,WAAW;gCAAE,MAAM;6BAAC,CAAC,CAAC;yBAC7D;wBACD,oBAAoB,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;qBACrC;iBACF;aACF;YACD,MAAM,IAAI,GAA6B,CAAA,CAAE,CAAC;YAC1C,IAAI,CAAC,MAAM,CAAC,GAAG,KAAK,CAAC,IAAI,CAAC;YAC1B,IAAI,CAAC,WAAW,CAAC,GAAG,cAAc,CAAC;YACnC,IAAI,CAAC,QAAQ,CAAC,GAAG,WAAW,CAAC;YAC7B,IAAI,CAAC,cAAc,CAAC,GAAG,oBAAoB,CAAC;YAC5C,YAAY,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SACzB;QACD,MAAM,CAAC,QAAQ,CAAC,GAAG,YAAY,CAAC;QAChC,uCAAuC;QACvC,MAAM,WAAW,GAAG,EAAE,CAAC;QACvB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,WAAW,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;YAChD,MAAM,KAAK,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;YAClC,MAAM,SAAS,GAAG,IAAI,CAAC,sBAAsB,CAAC,CAAC,CAAC,CAAC;YAEjD,MAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC;YACpD,IAAI,CAAC,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;gBACrC,SAAS;aACV;YACD,IAAI,YAAY,GAAG,iBAAiB,CAAC,OAAO,CAAC,CAAC;YAC9C,IAAI,YAAY,KAAK,IAAI,IAAI,YAAY,KAAK,SAAS,EAAE;gBACvD,YAAY,GAAG,CAAC,CAAC;aAClB;YACD,MAAM,WAAW,GAAG,IAAI,CAAC,wBAAwB,CAAC,CAAC,CAAC,CAAC;YACrD,WAAW,CAAC,IAAI,CAAC;gBAAC,KAAK,CAAC,IAAI;gBAAE,YAAY;gBAAE,WAAW;aAAC,CAAC,CAAC;SAC3D;QACD,MAAM,CAAC,aAAa,CAAC,GAAG,WAAW,CAAC;QAEpC,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,YAAY,CAAC,MAAM,EAAE,CAAC,EAAE,CAAE;YACjD,MAAM,KAAK,GAAG,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC;YACnC,MAAM,SAAS,GAAG,IAAI,CAAC,uBAAuB,CAAC,CAAC,CAAC,CAAC;YAElD,MAAM,OAAO,GAAG,SAAS,CAAC,OAAO,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC;YACpD,IAAI,CAAC,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE;gBACrC,SAAS;aACV;YACD,IAAI,YAAY,GAAG,iBAAiB,CAAC,OAAO,CAAC,CAAC;YAC9C,IAAI,YAAY,KAAK,IAAI,IAAI,YAAY,KAAK,SAAS,EAAE;gBACvD,YAAY,GAAG,CAAC,CAAC;aAClB;YACD,MAAM,WAAW,GAAG,IAAI,CAAC,yBAAyB,CAAC,CAAC,CAAC,CAAC;YACtD,YAAY,CAAC,IAAI,CAAC;gBAAC,KAAK,CAAC,IAAI;gBAAE,YAAY;gBAAE,WAAW;aAAC,CAAC,CAAC;SAC5D;QACD,MAAM,CAAC,cAAc,CAAC,GAAG,YAAY,CAAC;QACtC,OAAO,MAAM,CAAC;IAChB,CAAC;IAED;;;;;;;;;;;OAWG,CACH,gBAAA,EAAkB,CAClB,MAAM,CAAU,UAAU,CACtB,GAA6C,EAC7C,MAAgC,EAChC,gBAAgB,CAAA,CAA8B,EAC9C,cAAc,GAAG,KAAK,EAAA;QACxB,iCAAiC;QACjC,mCAAmC;QACnC,MAAM,aAAa,GAAiC,CAAA,CAAE,CAAC;QAEvD,wCAAwC;QACxC,yCAAyC;QACzC,oDAAoD;QACpD,qDAAqD;QACrD,wDAAwD;QACxD,MAAM,gBAAgB,GAAkD,CAAA,CAAE,CAAC;QAC3E,SAAS,kBAAkB,CACvB,KAAY,EAAE,QAAkC;YAClD,IAAI,CAAC,CAAC,KAAK,CAAC,IAAI,IAAI,gBAAgB,CAAC,EAAE;gBACrC,gBAAgB,CAAC,KAAK,CAAC,IAAI,CAAC,GAAG;oBAAC,QAAQ;iBAAC,CAAC;aAC3C,MAAM;gBACL,gBAAgB,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;aAC7C;QACH,CAAC;QAED,SAAS,WAAW,CAAC,KAAY,EAAE,QAAkC;YACnE,MAAM,YAAY,GAAqB,EAAE,CAAC;YAC1C,IAAI,MAAM,CAAC;YACX,KAAK,MAAM,SAAS,IAAI,QAAQ,CAAE;gBAChC,MAAM,gBAAgB,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;gBACtC,MAAM,gBAAgB,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;gBACtC,MAAM,kBAAkB,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;gBAExC,MAAM,GAAG,SAAS,CAAC,CAAC,CAAC,IAAI,IAAI,CAAC,CAAC,CAC3B,CAAA,CAAE,CAAC,CAAC,CACJ,SAAS,CAAC,CAAC,CAA6B,CAAC;gBAC7C,IAAI,CAAC,CAAC,gBAAgB,IAAI,aAAa,CAAC,EAAE;oBACxC,kBAAkB,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;oBACpC,OAAO;iBACR;gBACD,MAAM,YAAY,GAAG,aAAa,CAAC,gBAAgB,CAAC,CAAC;gBACrD,IAAI,YAAY,CAAC,YAAY,CAAC,MAAM,IAAI,gBAAgB,EAAE;oBACxD,kBAAkB,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;oBACpC,OAAO;iBACR;gBACD,MAAM,WAAW,GAAG,YAAY,CAAC,YAAY,CAAC,gBAAgB,CAAC,CAAC;gBAChE,YAAY,CAAC,IAAI,CAAC,WAAW,CAAC,aAAa,CAAC,kBAAkB,CAAC,CAAC,CAAC;aAClE;YACD,mDAAmD;YACnD,oCAAoC;YACpC,8CAA8C;YAC9C,IAAI,YAAY,CAAC,MAAM,GAAG,CAAC,EAAE;gBAC3B,KAAK,CAAC,KAAK,CACP,aAAa,CAAC,iTAAgB,CAAC,YAAY,CAAC,EAC5C,MAAM,CAAC,CAAC,CAAE,gBAAgB;aAC/B;QACH,CAAC;QAED;;;;;WAKG,CACH,SAAS,YAAY,CAAC,SAAwC;YAC5D,MAAM,SAAS,GAAG,SAAS,CAAC,MAAM,CAAW,CAAC;YAC9C,qBAAqB;YACrB,MAAM,KAAK,OACP,2TAAgB,EACZ,SAAS,EACT,MAAM,CAAC,eAAe,CAAC,IAAI,IAAI,CAAC,CAAC,CAC7B,MAAM,CAAC,eAAe,CAA6B,CAAC,CAAC,CACrD,CAAA,CAAE,CAAU,CAAC;YACzB,KAAK,CAAC,4BAA4B,CAAC,cAAc,CAAC,CAAC;YACnD,aAAa,CAAC,SAAS,CAAC,GAAG,KAAK,CAAC;YACjC,uBAAuB;YACvB,MAAM,gBAAgB,GAClB,SAAS,CAAC,cAAc,CAA+B,CAAC;YAC5D,gBAAgB,CAAC,OAAO,EAAC,QAAQ,CAAC,EAAE;gBAClC,IAAI,CAAC,CAAC,QAAQ,YAAY,KAAK,CAAC,EAAE;oBAChC,MAAM,IAAI,ySAAU,CAChB,CAAA,sDAAA,EACI,QAAQ,EAAE,CAAC,CAAC;iBACrB;gBACD,iDAAiD;gBACjD,yDAAyD;gBACzD,0DAA0D;gBAC1D,sCAAsC;gBACtC,kBAAkB,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;YACtC,CAAC,CAAC,CAAC;QACL,CAAC;QAED,iEAAiE;QACjE,MAAM,IAAI,GAAG,MAAM,CAAC,MAAM,CAAC,CAAC;QAC5B,MAAM,gBAAgB,GAAG,MAAM,CAAC,QAAQ,CAA+B,CAAC;QACxE,KAAK,MAAM,SAAS,IAAI,gBAAgB,CAAE;YACxC,YAAY,CAAC,SAAS,CAAC,CAAC;SACzB;QAED,iDAAiD;QACjD,yDAAyD;QACzD,yDAAyD;QACzD,6CAA6C;QAC7C,MAAO,CAAC,aAAa,CAAC,8SAAa,CAAC,gBAAgB,CAAC,CAAE;YACrD,KAAK,MAAM,SAAS,IAAI,gBAAgB,CAAE;gBACxC,MAAM,KAAK,GAAG,aAAa,CAAC,SAAS,CAAC,MAAM,CAAW,CAAC,CAAC;gBACzD,IAAI,KAAK,CAAC,IAAI,IAAI,gBAAgB,EAAE;oBAClC,MAAM,+BAA+B,GAAG,gBAAgB,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;oBACrE,OAAO,gBAAgB,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;oBACpC,KAAK,MAAM,QAAQ,IAAI,+BAA+B,CAAE;wBACtD,WAAW,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;qBAC9B;iBACF;aACF;SACF;QAED,MAAM,YAAY,GAAqB,EAAE,CAAC;QAC1C,MAAM,aAAa,GAAqB,EAAE,CAAC;QAC3C,MAAM,qBAAqB,GACvB,MAAM,CAAC,aAAa,CAA+B,CAAC;QACxD,KAAK,MAAM,SAAS,IAAI,qBAAqB,CAAE;YAC7C,MAAM,SAAS,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YACzC,MAAM,SAAS,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YACzC,MAAM,WAAW,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YAC3C,aAAa,CAAC,uSAAM,CAAC,SAAS,IAAI,aAAa,CAAC,CAAC;YACjD,MAAM,KAAK,GAAG,aAAa,CAAC,SAAS,CAAC,CAAC;YACvC,MAAM,kBAAkB,GAAG,KAAK,CAAC,YAAY,CAAC,SAAS,CAAC,CAAC,aAAa,CAAC;YACvE,YAAY,CAAC,IAAI,CAAC,kBAAkB,CAAC,WAAW,CAAC,CAAC,CAAC;SACpD;QACD,MAAM,sBAAsB,GACxB,MAAM,CAAC,cAAc,CAA+B,CAAC;QACzD,KAAK,MAAM,SAAS,IAAI,sBAAsB,CAAE;YAC9C,MAAM,SAAS,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YACzC,MAAM,SAAS,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YACzC,MAAM,WAAW,GAAG,SAAS,CAAC,CAAC,CAAW,CAAC;YAC3C,aAAa,CAAC,uSAAM,CAAC,SAAS,IAAI,aAAa,CAAC,CAAC;YACjD,MAAM,KAAK,GAAG,aAAa,CAAC,SAAS,CAAC,CAAC;YACvC,MAAM,kBAAkB,GAAG,KAAK,CAAC,YAAY,CAAC,SAAS,CAAC,CAAC,aAAa,CAAC;YACvE,aAAa,CAAC,IAAI,CAAC,kBAAkB,CAAC,WAAW,CAAC,CAAC,CAAC;SACrD;QACD,OAAO,IAAI,GAAG,CAAC;YAAC,MAAM,EAAE,YAAY;YAAE,OAAO,EAAE,aAAa;YAAE,IAAI;QAAA,CAAC,CAAC,CAAC;IACvE,CAAC;IAED;;;;;OAKG,CACH,IAAa,QAAQ,GAAA;QACnB,oEAAoE;QACpE,kDAAkD;QAClD,IAAI,IAAI,CAAC,SAAS,EAAE;YAClB,MAAM,IAAI,ySAAU,CAChB,4DAA4D,GAC5D,6DAA6D,GAC7D,iEAAiE,CAAC,CAAC;SACxE;QACD,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;YAC/B,IAAI,KAAK,CAAC,QAAQ,EAAE;gBAClB,OAAO,IAAI,CAAC;aACb;SACF;QACD,OAAO,KAAK,CAAC;IACf,CAAC;IAED;;;;;OAKG,CACM,WAAW,GAAA;YAClB,iPAAI,EAAC,GAAG,EAAE;YACR,IAAI,CAAC,MAAM,CAAC,OAAO,EAAC,KAAK,CAAC,EAAE;gBAC1B,wBAAwB;gBACxB,IAAI,KAAK,CAAC,QAAQ,EAAE;oBAClB,KAAK,CAAC,WAAW,EAAE,CAAC;iBACrB;YACD,uBAAuB;YACzB,CAAC,CAAC,CAAC;QACL,CAAC,CAAC,CAAC;IACL,CAAC;CACF"}},
    {"offset": {"line": 2866, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-layers@4.22.0_@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-layers/dist/engine/training_utils.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-layers/src/engine/training_utils.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport {argMax, clone, dispose, mul, reshape, Tensor, Tensor1D, tensor1d, tidy} from '@tensorflow/tfjs-core';\n\n/**\n * For multi-class classification problems, this object is designed to store a\n * mapping from class index to the \"weight\" of the class, where higher weighted\n * classes have larger impact on loss, accuracy, and other metrics.\n *\n * This is useful for cases in which you want the model to \"pay more attention\"\n * to examples from an under-represented class, e.g., in unbalanced datasets.\n */\nexport type ClassWeight = {\n  [classIndex: number]: number\n};\n\n/**\n * Class weighting for a model with multiple outputs.\n *\n * This object maps each output name to a class-weighting object.\n */\nexport type ClassWeightMap = {\n  [outputName: string]: ClassWeight\n};\n\nfunction standardizeSampleOrClassWeights(\n    xWeight: ClassWeight|ClassWeight[]|ClassWeightMap, outputNames: string[],\n    weightType: 'sampleWeight'|'classWeight'): ClassWeight[] {\n  const numOutputs = outputNames.length;\n  if (xWeight == null || (Array.isArray(xWeight) && xWeight.length === 0)) {\n    return outputNames.map(name => null);\n  }\n  if (numOutputs === 1) {\n    if (Array.isArray(xWeight) && xWeight.length === 1) {\n      return xWeight;\n    } else if (typeof xWeight === 'object' && outputNames[0] in xWeight) {\n      return [(xWeight as ClassWeightMap)[outputNames[0]]];\n    } else {\n      return [xWeight as ClassWeight];\n    }\n  }\n  if (Array.isArray(xWeight)) {\n    if (xWeight.length !== numOutputs) {\n      throw new Error(\n          `Provided ${weightType} is an array of ${xWeight.length} ` +\n          `element(s), but the model has ${numOutputs} outputs. ` +\n          `Make sure a set of weights is provided for each model output.`);\n    }\n    return xWeight;\n  } else if (\n      typeof xWeight === 'object' && Object.keys(xWeight).length > 0 &&\n      typeof (xWeight as ClassWeightMap)[Object.keys(xWeight)[0]] ===\n          'object') {\n    const output: ClassWeight[] = [];\n    outputNames.forEach(outputName => {\n      if (outputName in xWeight) {\n        output.push((xWeight as ClassWeightMap)[outputName]);\n      } else {\n        output.push(null);\n      }\n    });\n    return output;\n  } else {\n    throw new Error(\n        `The model has multiple (${numOutputs}) outputs, ` +\n        `so ${weightType} must be either an array with ` +\n        `${numOutputs} elements or an object with ${outputNames} keys. ` +\n        `Provided ${weightType} not understood: ${JSON.stringify(xWeight)}`);\n  }\n}\n\n/**\n * Standardize class weighting objects.\n *\n * This function takes a single class-weighting object, an array of them,\n * or a map from output name to class-weighting object. It compares it to the\n * output name(s) of the model, base on which it outputs an array of\n * class-weighting objects of which the length matches the number of outputs.\n *\n * @param classWeight Input class-weighting object(s).\n * @param outputNames All output name(s) of the model.\n * @return An array of class-weighting objects. The length of the array matches\n *   the model's number of outputs.\n */\nexport function standardizeClassWeights(\n    classWeight: ClassWeight|ClassWeight[]|ClassWeightMap,\n    outputNames: string[]): ClassWeight[] {\n  return standardizeSampleOrClassWeights(\n      classWeight, outputNames, 'classWeight');\n}\n\nexport function standardizeSampleWeights(\n    classWeight: ClassWeight|ClassWeight[]|ClassWeightMap,\n    outputNames: string[]): ClassWeight[] {\n  return standardizeSampleOrClassWeights(\n      classWeight, outputNames, 'sampleWeight');\n}\n\n/**\n * Standardize by-sample and/or by-class weights for training.\n *\n * Note that this function operates on one model output at a time. For a model\n * with multiple outputs, you must call this function multiple times.\n *\n * @param y The target tensor that the by-sample and/or by-class weight is for.\n *     The values of y are assumed to encode the classes, either directly\n *     as an integer index, or as one-hot encoding.\n * @param sampleWeight By-sample weights.\n * @param classWeight By-class weights: an object mapping class indices\n *     (integers) to a weight (float) to apply to the model's loss for the\n *     samples from this class during training. This can be useful to tell the\n *     model to \"pay more attention\" to samples from an under-represented class.\n * @param sampleWeightMode The mode for the sample weights.\n * @return A Promise of weight tensor, of which the size of the first dimension\n *     matches that of `y`.\n */\nexport async function standardizeWeights(\n    y: Tensor, sampleWeight?: Tensor, classWeight?: ClassWeight,\n    sampleWeightMode?: 'temporal'): Promise<Tensor> {\n  if (sampleWeight != null || sampleWeightMode != null) {\n    // TODO(cais): Once 'temporal' mode is implemented, document it in the doc\n    // string.\n    throw new Error('Support sampleWeight is not implemented yet');\n  }\n\n  if (classWeight != null) {\n    // Apply class weights per sample.\n    const yClasses: Tensor1D = tidy(() => {\n      if (y.shape.length === 1) {\n        // Assume class indices.\n        return clone(y) as Tensor1D;\n      } else if (y.shape.length === 2) {\n        if (y.shape[1] > 1) {\n          // Assume one-hot encoding of classes.\n          const axis = 1;\n          return argMax(y, axis);\n        } else if (y.shape[1] === 1) {\n          // Class index.\n          return reshape(y, [y.shape[0]]);\n        } else {\n          throw new Error(\n              `Encountered unexpected last-dimension size (${y.shape[1]}) ` +\n              `during handling of class weights. The size is expected to be ` +\n              `>= 1.`);\n        }\n      } else {\n        throw new Error(\n            `Unexpected rank of target (y) tensor (${y.rank}) during ` +\n            `handling of class weights. The rank is expected to be 1 or 2.`);\n      }\n    });\n\n    const yClassIndices = Array.from(await yClasses.data());\n    dispose(yClasses);\n    const classSampleWeight: number[] = [];\n    yClassIndices.forEach(classIndex => {\n      if (classWeight[classIndex] == null) {\n        throw new Error(\n            `classWeight must contain all classes in the training data. ` +\n            `The class ${classIndex} exists in the data but not in ` +\n            `classWeight`);\n      } else {\n        classSampleWeight.push(classWeight[classIndex]);\n      }\n    });\n\n    return tensor1d(classSampleWeight, 'float32');\n  } else {\n    return null;\n  }\n}\n\n/**\n * Apply per-sample weights on the loss values from a number of samples.\n *\n * @param losses Loss tensor of shape `[batchSize]`.\n * @param sampleWeights Per-sample weight tensor of shape `[batchSize]`.\n * @returns Tensor of the same shape as`losses`.\n */\nexport function computeWeightedLoss(losses: Tensor, sampleWeights: Tensor) {\n  return mul(losses, sampleWeights);\n}\n"],"names":[],"mappings":";;;;;;;;;;AAAA;;;;;;;;GAQG;;;;;;AAEH,OAAO,EAAC,MAAM,EAAE,KAAK,EAAE,OAAO,EAAE,GAAG,EAAE,OAAO,EAAoB,QAAQ,EAAE,IAAI,EAAC,MAAM,uBAAuB,CAAC;;AAuB7G,SAAS,+BAA+B,CACpC,OAAiD,EAAE,WAAqB,EACxE,UAAwC;IAC1C,MAAM,UAAU,GAAG,WAAW,CAAC,MAAM,CAAC;IACtC,IAAI,OAAO,IAAI,IAAI,IAAI,AAAC,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,OAAO,CAAC,MAAM,KAAK,CAAC,CAAC,CAAE;QACvE,OAAO,WAAW,CAAC,GAAG,EAAC,IAAI,CAAC,EAAE,AAAC,IAAI,CAAC,CAAC;KACtC;IACD,IAAI,UAAU,KAAK,CAAC,EAAE;QACpB,IAAI,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,OAAO,CAAC,MAAM,KAAK,CAAC,EAAE;YAClD,OAAO,OAAO,CAAC;SAChB,MAAM,IAAI,OAAO,OAAO,KAAK,QAAQ,IAAI,WAAW,CAAC,CAAC,CAAC,IAAI,OAAO,EAAE;YACnE,OAAO;gBAAE,OAA0B,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;aAAC,CAAC;SACtD,MAAM;YACL,OAAO;gBAAC,OAAsB;aAAC,CAAC;SACjC;KACF;IACD,IAAI,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,EAAE;QAC1B,IAAI,OAAO,CAAC,MAAM,KAAK,UAAU,EAAE;YACjC,MAAM,IAAI,KAAK,CACX,CAAA,SAAA,EAAY,UAAU,CAAA,gBAAA,EAAmB,OAAO,CAAC,MAAM,CAAA,CAAA,CAAG,GAC1D,CAAA,8BAAA,EAAiC,UAAU,CAAA,UAAA,CAAY,GACvD,CAAA,6DAAA,CAA+D,CAAC,CAAC;SACtE;QACD,OAAO,OAAO,CAAC;KAChB,MAAM,IACH,OAAO,OAAO,KAAK,QAAQ,IAAI,MAAM,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,MAAM,GAAG,CAAC,IAC9D,OAAQ,OAA0B,CAAC,MAAM,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,KACvD,QAAQ,EAAE;QAChB,MAAM,MAAM,GAAkB,EAAE,CAAC;QACjC,WAAW,CAAC,OAAO,EAAC,UAAU,CAAC,EAAE;YAC/B,IAAI,UAAU,IAAI,OAAO,EAAE;gBACzB,MAAM,CAAC,IAAI,CAAE,OAA0B,CAAC,UAAU,CAAC,CAAC,CAAC;aACtD,MAAM;gBACL,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;aACnB;QACH,CAAC,CAAC,CAAC;QACH,OAAO,MAAM,CAAC;KACf,MAAM;QACL,MAAM,IAAI,KAAK,CACX,CAAA,wBAAA,EAA2B,UAAU,CAAA,WAAA,CAAa,GAClD,CAAA,GAAA,EAAM,UAAU,CAAA,8BAAA,CAAgC,GAChD,GAAG,UAAU,CAAA,4BAAA,EAA+B,WAAW,CAAA,OAAA,CAAS,GAChE,CAAA,SAAA,EAAY,UAAU,CAAA,iBAAA,EAAoB,IAAI,CAAC,SAAS,CAAC,OAAO,CAAC,EAAE,CAAC,CAAC;KAC1E;AACH,CAAC;AAeK,SAAU,uBAAuB,CACnC,WAAqD,EACrD,WAAqB;IACvB,OAAO,+BAA+B,CAClC,WAAW,EAAE,WAAW,EAAE,aAAa,CAAC,CAAC;AAC/C,CAAC;AAEK,SAAU,wBAAwB,CACpC,WAAqD,EACrD,WAAqB;IACvB,OAAO,+BAA+B,CAClC,WAAW,EAAE,WAAW,EAAE,cAAc,CAAC,CAAC;AAChD,CAAC;AAoBM,KAAK,UAAU,kBAAkB,CACpC,CAAS,EAAE,YAAqB,EAAE,WAAyB,EAC3D,gBAA6B;IAC/B,IAAI,YAAY,IAAI,IAAI,IAAI,gBAAgB,IAAI,IAAI,EAAE;QACpD,0EAA0E;QAC1E,UAAU;QACV,MAAM,IAAI,KAAK,CAAC,6CAA6C,CAAC,CAAC;KAChE;IAED,IAAI,WAAW,IAAI,IAAI,EAAE;QACvB,kCAAkC;QAClC,MAAM,QAAQ,OAAa,iPAAI,EAAC,GAAG,EAAE;YACnC,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,KAAK,CAAC,EAAE;gBACxB,wBAAwB;gBACxB,WAAO,uPAAK,EAAC,CAAC,CAAa,CAAC;aAC7B,MAAM,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,KAAK,CAAC,EAAE;gBAC/B,IAAI,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,GAAG,CAAC,EAAE;oBAClB,sCAAsC;oBACtC,MAAM,IAAI,GAAG,CAAC,CAAC;oBACf,WAAO,0PAAM,EAAC,CAAC,EAAE,IAAI,CAAC,CAAC;iBACxB,MAAM,IAAI,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC,EAAE;oBAC3B,eAAe;oBACf,WAAO,2PAAO,EAAC,CAAC,EAAE;wBAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC;qBAAC,CAAC,CAAC;iBACjC,MAAM;oBACL,MAAM,IAAI,KAAK,CACX,CAAA,4CAAA,EAA+C,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAA,EAAA,CAAI,GAC7D,CAAA,6DAAA,CAA+D,GAC/D,CAAA,KAAA,CAAO,CAAC,CAAC;iBACd;aACF,MAAM;gBACL,MAAM,IAAI,KAAK,CACX,CAAA,sCAAA,EAAyC,CAAC,CAAC,IAAI,CAAA,SAAA,CAAW,GAC1D,CAAA,6DAAA,CAA+D,CAAC,CAAC;aACtE;QACH,CAAC,CAAC,CAAC;QAEH,MAAM,aAAa,GAAG,KAAK,CAAC,IAAI,CAAC,MAAM,QAAQ,CAAC,IAAI,EAAE,CAAC,CAAC;YACxD,oPAAO,EAAC,QAAQ,CAAC,CAAC;QAClB,MAAM,iBAAiB,GAAa,EAAE,CAAC;QACvC,aAAa,CAAC,OAAO,EAAC,UAAU,CAAC,EAAE;YACjC,IAAI,WAAW,CAAC,UAAU,CAAC,IAAI,IAAI,EAAE;gBACnC,MAAM,IAAI,KAAK,CACX,CAAA,2DAAA,CAA6D,GAC7D,CAAA,UAAA,EAAa,UAAU,CAAA,+BAAA,CAAiC,GACxD,CAAA,WAAA,CAAa,CAAC,CAAC;aACpB,MAAM;gBACL,iBAAiB,CAAC,IAAI,CAAC,WAAW,CAAC,UAAU,CAAC,CAAC,CAAC;aACjD;QACH,CAAC,CAAC,CAAC;QAEH,WAAO,6PAAQ,EAAC,iBAAiB,EAAE,SAAS,CAAC,CAAC;KAC/C,MAAM;QACL,OAAO,IAAI,CAAC;KACb;AACH,CAAC;AASK,SAAU,mBAAmB,CAAC,MAAc,EAAE,aAAqB;IACvE,WAAO,mPAAG,EAAC,MAAM,EAAE,aAAa,CAAC,CAAC;AACpC,CAAC"}},
    {"offset": {"line": 2986, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-layers@4.22.0_@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-layers/src/engine/training_dataset.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {scalar} from '@tensorflow/tfjs-core';\nimport {BaseCallback, configureCallbacks, CustomCallbackArgs, History, ModelLoggingVerbosity, standardizeCallbacks, YieldEveryOptions} from '../base_callbacks';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {disposeTensorsInLogs, UnresolvedLogs} from '../logs';\nimport {TensorOrArrayOrMap} from '../types';\nimport {singletonOrArray, toList} from '../utils/generic_utils';\n\nimport {Dataset, LazyIterator} from './dataset_stub';\nimport {ClassWeight, ClassWeightMap, standardizeClassWeights, standardizeWeights} from './training_utils';\n\n/**\n * Interface for configuring model training based on a dataset object.\n */\nexport interface ModelFitDatasetArgs<T> {\n  /**\n   * (Optional) Total number of steps (batches of samples) before\n   * declaring one epoch finished and starting the next epoch. It should\n   * typically be equal to the number of samples of your dataset divided by\n   * the batch size, so that `fitDataset`() call can utilize the entire dataset.\n   * If it is not provided, use `done` return value in `iterator.next()` as\n   * signal to finish an epoch.\n   */\n  batchesPerEpoch?: number;\n\n  /**\n   * Integer number of times to iterate over the training dataset.\n   */\n  epochs: number;\n\n  /**\n   * Verbosity level.\n   *\n   * Expected to be 0, 1, or 2. Default: 1.\n   *\n   * 0 - No printed message during fit() call.\n   * 1 - In Node.js (tfjs-node), prints the progress bar, together with\n   *     real-time updates of loss and metric values and training speed.\n   *     In the browser: no action. This is the default.\n   * 2 - Not implemented yet.\n   */\n  verbose?: ModelLoggingVerbosity;\n\n  /**\n   * List of callbacks to be called during training.\n   * Can have one or more of the following callbacks:\n   *   - `onTrainBegin(logs)`: called when training starts.\n   *   - `onTrainEnd(logs)`: called when training ends.\n   *   - `onEpochBegin(epoch, logs)`: called at the start of every epoch.\n   *   - `onEpochEnd(epoch, logs)`: called at the end of every epoch.\n   *   - `onBatchBegin(batch, logs)`: called at the start of every batch.\n   *   - `onBatchEnd(batch, logs)`: called at the end of every batch.\n   *   - `onYield(epoch, batch, logs)`: called every `yieldEvery` milliseconds\n   *      with the current epoch, batch and logs. The logs are the same\n   *      as in `onBatchEnd()`. Note that `onYield` can skip batches or\n   *      epochs. See also docs for `yieldEvery` below.\n   */\n  callbacks?: BaseCallback[]|CustomCallbackArgs|CustomCallbackArgs[];\n\n  /**\n   * Data on which to evaluate the loss and any model\n   * metrics at the end of each epoch. The model will not be trained on this\n   * data. This could be any of the following:\n   *\n   *   - An array `[xVal, yVal]`, where the two values may be `tf.Tensor`,\n   *     an array of Tensors, or a map of string to Tensor.\n   *   - Similarly, an array ` [xVal, yVal, valSampleWeights]`\n   *     (not implemented yet).\n   *   - a `Dataset` object with elements of the form `{xs: xVal, ys: yVal}`,\n   *     where `xs` and `ys` are the feature and label tensors, respectively.\n   *\n   * If `validationData` is an Array of Tensor objects, each `tf.Tensor` will be\n   * sliced into batches during validation, using the parameter\n   * `validationBatchSize` (which defaults to 32). The entirety of the\n   * `tf.Tensor` objects will be used in the validation.\n   *\n   * If `validationData` is a dataset object, and the `validationBatches`\n   * parameter is specified, the validation will use `validationBatches` batches\n   * drawn from the dataset object. If `validationBatches` parameter is not\n   * specified, the validation will stop when the dataset is exhausted.\n   *\n   * The model will not be trained on this data.\n   */\n  validationData?: [\n    TensorOrArrayOrMap, TensorOrArrayOrMap\n  ]|[TensorOrArrayOrMap, TensorOrArrayOrMap, TensorOrArrayOrMap]|Dataset<T>;\n\n  /**\n   * Optional batch size for validation.\n   *\n   * Used only if `validationData` is an array of `tf.Tensor` objects, i.e., not\n   * a dataset object.\n   *\n   * If not specified, its value defaults to 32.\n   */\n  validationBatchSize?: number;\n\n  /**\n   * (Optional) Only relevant if `validationData` is specified and is a dataset\n   * object.\n   *\n   * Total number of batches of samples to draw from `validationData` for\n   * validation purpose before stopping at the end of every epoch. If not\n   * specified, `evaluateDataset` will use `iterator.next().done` as signal to\n   * stop validation.\n   */\n  validationBatches?: number;\n\n  /**\n   * Configures the frequency of yielding the main thread to other tasks.\n   *\n   * In the browser environment, yielding the main thread can improve the\n   * responsiveness of the page during training. In the Node.js environment,\n   * it can ensure tasks queued in the event loop can be handled in a timely\n   * manner.\n   *\n   * The value can be one of the following:\n   *   - `'auto'`: The yielding happens at a certain frame rate (currently set\n   *               at 125ms). This is the default.\n   *   - `'batch'`: yield every batch.\n   *   - `'epoch'`: yield every epoch.\n   *   - a `number`: Will yield every `number` milliseconds.\n   *   - `'never'`: never yield. (But yielding can still happen through `await\n   *      nextFrame()` calls in custom callbacks.)\n   */\n  yieldEvery?: YieldEveryOptions;\n\n  /**\n   * Epoch at which to start training (useful for resuming a previous training\n   * run). When this is used, `epochs` is the index of the \"final epoch\".\n   * The model is not trained for a number of iterations given by `epochs`,\n   * but merely until the epoch of index `epochs` is reached.\n   */\n  initialEpoch?: number;\n\n  /**\n   * Optional object mapping class indices (integers) to\n   * a weight (float) to apply to the model's loss for the samples from this\n   * class during training. This can be useful to tell the model to \"pay more\n   * attention\" to samples from an under-represented class.\n   *\n   * If the model has multiple outputs, a class weight can be specified for\n   * each of the outputs by setting this field an array of weight object\n   * or an object that maps model output names (e.g., `model.outputNames[0]`)\n   * to weight objects.\n   */\n  classWeight?: ClassWeight|ClassWeight[]|ClassWeightMap;\n}\n\nexport interface FitDatasetElement {\n  xs: TensorOrArrayOrMap;\n  ys: TensorOrArrayOrMap;\n}\n\n/**\n * Interface for configuring model evaluation based on a dataset object.\n */\nexport interface ModelEvaluateDatasetArgs {\n  /**\n   * Number of batches to draw from the dataset object before ending the\n   * evaluation.\n   */\n  batches?: number;\n\n  /**\n   * Verbosity mode.\n   */\n  verbose?: ModelLoggingVerbosity;\n}\n\n// Default batch size used during tensor-based validation.\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, iteratorOut: {}): {xs: tfc.Tensor[], ys: tfc.Tensor[]} {\n  let xs: TensorOrArrayOrMap;\n  let ys: TensorOrArrayOrMap;\n\n  const iteratorOutObj = iteratorOut as FitDatasetElement;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(\n      xs != null && ys != null,\n      () => 'A Dataset iterator for fitDataset() is expected to generate ' +\n          'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +\n          'values may be `tf.Tensor`, an array of Tensors, or a map of ' +\n          'string to Tensor.  The provided Dataset instead generates ' +\n          `${iteratorOut}`);\n\n  const flattenedXs: tfc.Tensor[] =\n      flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  const flattenedYs: tfc.Tensor[] =\n      flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n\n  const batchSize: number = flattenedXs[0].shape[0];\n\n  tfc.util.assert(\n      flattenedXs.length === model.inputs.length,\n      () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` +\n          `provides ${flattenedXs.length} inputs.  (Expected input keys: ` +\n          `${JSON.stringify(model.inputNames)})`);\n\n  tfc.util.assert(\n      flattenedYs.length === model.outputs.length,\n      () =>\n          `LayersModel has ${model.outputs.length} outputs, but the dataset ` +\n          `provides ${flattenedYs.length} outputs.  (Expected output keys: ` +\n          `${JSON.stringify(model.outputNames)})`);\n\n  for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    tfc.util.assert(\n        flattenedXs[xIndex].shape[0] === batchSize,\n        () => `Batch size mismatch: input ` +\n            `${model.inputNames[xIndex]} has ${\n                  flattenedXs[xIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    tfc.util.assert(\n        flattenedYs[yIndex].shape[0] === batchSize,\n        () => `Batch size mismatch: output ` +\n            `${model.outputNames[yIndex]} has ${\n                  flattenedYs[yIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  return {xs: flattenedXs, ys: flattenedYs};\n}\n\nfunction flattenTensorOrArrayOrMap(\n    inputOrOutput: string, names: string[], values: TensorOrArrayOrMap) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(\n        values.length === names.length,\n        () => `Received an array of ${values.length} Tensors, but expected ${\n            names.length} to match the ${inputOrOutput} keys ${names}.`);\n    return values;\n  } else {\n    const result: tfc.Tensor[] = [];\n    // Check that all the required keys are available.\n    for (const name of names) {\n      if (values[name] == null) {\n        throw new ValueError(\n            `The feature data generated by the dataset lacks the required ` +\n            `${inputOrOutput} key '${name}'.`);\n      }\n      result.push(values[name]);\n    }\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData<T>(\n    data:\n        [\n          tfc.Tensor|tfc.Tensor[], tfc.Tensor|tfc.Tensor[]\n        ]|[tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[],\n           tfc.Tensor | tfc.Tensor[]]):\n    {xs: tfc.Tensor|tfc.Tensor[], ys: tfc.Tensor|tfc.Tensor[]} {\n  if (data.length === 3) {\n    throw new NotImplementedError(\n        'Validation with sample weights is not implemented yet.');\n  }\n  return {xs: data[0], ys: data[1]};\n}\n\nexport async function fitDataset<T>(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, dataset: Dataset<T>,\n    args: ModelFitDatasetArgs<T>): Promise<History> {\n  const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n  tfc.util.assert(\n      model.optimizer != null,\n      () => 'You must compile a model before training/testing. Use ' +\n          'LayersModel.compile(modelCompileConfig).');\n\n  tfc.util.assert(\n      args != null,\n      () => `For fitDataset(), the 2nd argument (config) is required, ` +\n          `but it is not provided in this call.`);\n  tfc.util.assert(\n      args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs),\n      () => `For fitDataset(), config.epochs is expected to be a positive ` +\n          `integer, but got ${args.epochs}`);\n  tfc.util.assert(\n      !hasBatchesPerEpoch ||\n          (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)),\n      () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` +\n          `positive integer if specified, but got ${args.batchesPerEpoch}`);\n  tfc.util.assert(\n      // tslint:disable-next-line:no-any\n      (args as any)['validationSplit'] == null,\n      () => '`validationSplit` is not supported by `fitDataset()`. ' +\n          'Use validationData instead.');\n\n  if (model.isTraining) {\n    throw new Error(\n        'Cannot start training because another fit() call is ongoing.');\n  }\n  model.isTraining = true;\n\n  try {\n    const doValidation = args.validationData != null;\n    let valXs: tfc.Tensor|tfc.Tensor[];\n    let valYs: tfc.Tensor|tfc.Tensor[];\n    if (doValidation) {\n      if (isDatasetObject(args.validationData)) {\n        tfc.util.assert(\n            args.validationBatches == null ||\n                (args.validationBatches > 0 &&\n                 Number.isInteger(args.validationBatches)),\n            () => `For fitDataset() with dataset-based validation, ` +\n                `config.validationBatches is expected not to be provided, ` +\n                `or to be a positive integer, ` +\n                `but got ${args.validationBatches}`);\n      } else {\n        const validationData = standardizeTensorValidationData(\n            args.validationData as\n                    [tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[]] |\n            [\n              tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[],\n              tfc.Tensor | tfc.Tensor[]\n            ]);\n        valXs = validationData.xs;\n        valYs = validationData.ys;\n      }\n    }\n\n    const trainFunction = model.makeTrainFunction();\n    const outLabels = model.getDedupedMetricsNames() as string[];\n\n    let callbackMetrics: string[];\n    if (doValidation) {\n      callbackMetrics =\n          outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n    } else {\n      callbackMetrics = outLabels.slice();\n    }\n\n    const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n    const verbose = args.verbose == null ? 1 : args.verbose;\n    const {callbackList, history} = configureCallbacks(\n        callbacks, verbose, args.epochs, null, null,\n        getStepsPerEpoch(dataset, args),\n        null,  // Batch size determined by the dataset itself.\n        doValidation, callbackMetrics);\n    callbackList.setModel(model);\n    model.history = history;\n\n    await callbackList.onTrainBegin();\n    model.stopTraining_ = false;\n    let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n\n    let dataIterator = await dataset.iterator();\n    while (epoch < args.epochs) {\n      const epochLogs: UnresolvedLogs = {};\n      await callbackList.onEpochBegin(epoch);\n      let stepsDone = 0;\n      let batchIndex = 0;\n      if (!hasBatchesPerEpoch) {\n        dataIterator = await dataset.iterator();\n      }\n      while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n        const iteratorOut = await dataIterator.next();\n\n        // If `batchesPerEpoch` is specified, the dataset should not be\n        // exhausted until all epoches are done.\n        if (hasBatchesPerEpoch && iteratorOut.done) {\n          console.warn(\n              'You provided `batchesPerEpoch` as ' +\n              `${args.batchesPerEpoch}, ` +\n              'but your dataset iterator ran out of data after ' +\n              `${stepsDone} batches; ` +\n              'interrupting training. Make sure that your ' +\n              'dataset can generate at least `batchesPerEpoch * epochs` ' +\n              'batches (in this case, ' +\n              `${args.batchesPerEpoch * args.epochs} batches). ` +\n              'You may need to use the repeat() function when building ' +\n              'your dataset.');\n          break;\n        }\n\n        if (iteratorOut.value != null) {\n          const {xs, ys} =\n              standardizeDataIteratorOutput(model, iteratorOut.value);\n          const batchLogs: UnresolvedLogs = {};\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = xs[0].shape[0];\n\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n\n          const sampleWeights: tfc.Tensor[] = [];\n          if (args.classWeight != null) {\n            const standardClassWeights =\n                standardizeClassWeights(args.classWeight, model.outputNames);\n            for (let i = 0; i < standardClassWeights.length; ++i) {\n              sampleWeights.push(await standardizeWeights(\n                  ys[i], null, standardClassWeights[i]));\n            }\n          }\n\n          // Train on batch.\n          const ins = xs.concat(ys).concat(sampleWeights);\n          const outs = trainFunction(ins);\n          tfc.dispose(ins);\n          for (let i = 0; i < outLabels.length; ++i) {\n            const label = outLabels[i];\n            const out = outs[i];\n            batchLogs[label] = out;\n            tfc.keep(out);\n          }\n\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n\n          batchIndex++;\n          stepsDone++;\n        }\n\n        if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :\n                                 iteratorOut.done) {\n          // Epoch finished. Perform validation.\n          if (doValidation) {\n            let valOuts: tfc.Scalar[];\n            if (isDatasetObject(args.validationData)) {\n              valOuts = toList(await model.evaluateDataset(\n                  args.validationData, {batches: args.validationBatches}));\n            } else {\n              valOuts = toList(model.evaluate(valXs, valYs, {\n                batchSize: args.validationBatchSize == null ?\n                    DEFAULT_VALIDATION_BATCH_SIZE :\n                    args.validationBatchSize,\n                verbose: 0\n              }));\n            }\n            for (let i = 0; i < model.metricsNames.length; ++i) {\n              epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n            }\n          }\n          // Call `break` to exit one epoch lopp after validation is done. If\n          // config.batchesPerEpoch is specified, an epoch while loop will\n          // stop when `stepsDone >= config.batchesPerEpoch`. When\n          // config.batchesPerEpoch is not provided, the following `break` is\n          // required to exit the while lopp after dataset is exhausted.\n          break;\n        }\n\n        if (model.stopTraining_) {\n          break;\n        }\n      }\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      epoch++;\n      if (model.stopTraining_) {\n        break;\n      }\n    }\n    await callbackList.onTrainEnd();\n    await model.history.syncData();\n    return model.history;\n  } finally {\n    model.isTraining = false;\n  }\n}\n\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction getStepsPerEpoch<T>(\n    dataset: Dataset<T>, args: ModelFitDatasetArgs<T>): number {\n  // Attempt to determine # of batches in an epoch.\n  let stepsPerEpoch: number = null;\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n  return stepsPerEpoch;\n}\n\n// Check if provided object is a Dataset object by checking its .iterator\n// element.\nfunction isDatasetObject<T>(\n    dataset:\n        [\n          TensorOrArrayOrMap, TensorOrArrayOrMap\n        ]|[TensorOrArrayOrMap, TensorOrArrayOrMap, TensorOrArrayOrMap]|\n    Dataset<T>): boolean {\n  return (typeof (dataset as Dataset<T>).iterator === 'function');\n}\n\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject<T>(iterator: Dataset<T>|\n                                 LazyIterator<T>): boolean {\n  return (typeof (iterator as LazyIterator<T>).next === 'function');\n}\n\nexport async function evaluateDataset<T>(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, dataset: Dataset<T>|LazyIterator<T>,\n    args: ModelEvaluateDatasetArgs): Promise<tfc.Scalar|tfc.Scalar[]> {\n  args = args || {};\n  const hasBatches = args.batches != null;\n  const f = model.testFunction;\n  let outs: tfc.Scalar[] = [];\n  if (args.verbose > 0) {\n    throw new NotImplementedError('Verbose mode is not implemented yet.');\n  }\n\n  tfc.util.assert(\n      !hasBatches || (args.batches > 0 && Number.isInteger(args.batches)),\n      () => 'Test loop expects `batches` to be a positive integer, but ' +\n          `received ${JSON.stringify(args.batches)}`);\n  const dataIterator = isLazyIteratorObject(dataset) ?\n      dataset as LazyIterator<T>:\n      await (dataset as Dataset<T>).iterator();\n  // Keeps track of number of examples used in this evaluation.\n  let numExamples = 0;\n  let batch = 0;\n\n  while (hasBatches ? batch < args.batches : true) {\n    const iteratorOut = await dataIterator.next();\n    outs = tfc.tidy(() => {\n      if (iteratorOut.value) {\n        // TODO(cais): Once real dataset is available, use\n        //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n        const {xs, ys} =\n            standardizeDataIteratorOutput(model, iteratorOut.value);\n        const xsAndYs = xs.concat(ys);\n        const batchOuts = tfc.tidy(() => f(xsAndYs));\n        tfc.dispose(xsAndYs);\n\n        if (batch === 0) {\n          for (let i = 0; i < batchOuts.length; ++i) {\n            outs.push(scalar(0));\n          }\n        }\n\n        const batchSize = xsAndYs[0].shape[0];\n        for (let i = 0; i < batchOuts.length; ++i) {\n          const batchOut = batchOuts[i];\n          const oldScalar = outs[i];\n          outs[i] =\n              tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n          if (batch > 0) {\n            tfc.dispose(oldScalar);\n          }\n        }\n        tfc.dispose(batchOuts);\n        numExamples += batchSize;\n\n        ++batch;\n      }\n      return outs;\n    });\n\n    if (iteratorOut.done) {\n      if (hasBatches) {\n        console.warn(\n            'Your dataset iterator ran out of data during evaluateDataset(). ' +\n            'Interrupting evalution. Make sure that your ' +\n            'dataset can generate at least `batches` ' +\n            `batches (in this case, ${args.batches} batches). ` +\n            'You may need to use the repeat() function when building ' +\n            'your dataset.');\n      }\n      break;\n    }\n  }\n\n  for (let i = 0; i < outs.length; ++i) {\n    const oldScalar = outs[i];\n    outs[i] = tfc.div(outs[i], numExamples);\n    tfc.dispose(oldScalar);\n  }\n\n  return singletonOrArray(outs);\n}\n"],"names":[],"mappings":";;;;;;AAAA;;;;;;;;GAQG,CAEH;;GAEG;;;;;;AAEH,OAAO,KAAK,GAAG,MAAM,uBAAuB,CAAC;AAC7C,OAAO,EAAC,MAAM,EAAC,MAAM,uBAAuB,CAAC;AAC7C,OAAO,EAAe,kBAAkB,EAAsD,oBAAoB,EAAoB,MAAM,mBAAmB,CAAC;AAChK,OAAO,EAAC,mBAAmB,EAAE,UAAU,EAAC,MAAM,WAAW,CAAC;AAC1D,OAAO,EAAC,oBAAoB,EAAiB,MAAM,SAAS,CAAC;AAE7D,OAAO,EAAC,gBAAgB,EAAE,MAAM,EAAC,MAAM,wBAAwB,CAAC;AAGhE,OAAO,EAA8B,uBAAuB,EAAE,kBAAkB,EAAC,MAAM,kBAAkB,CAAC;;;;;;;;AAiK1G,0DAA0D;AAC1D,MAAM,6BAA6B,GAAG,EAAE,CAAC;AAEzC;;;;;;;;;;;;;GAaG,CACH,SAAS,6BAA6B,CAClC,6DAA6D;AAC7D,eAAe;AACf,kCAAkC;AAClC,KAAU,EAAE,WAAe;IAC7B,IAAI,EAAsB,CAAC;IAC3B,IAAI,EAAsB,CAAC;IAE3B,MAAM,cAAc,GAAG,WAAgC,CAAC;IACxD,EAAE,GAAG,cAAc,CAAC,IAAI,CAAC,CAAC;IAC1B,EAAE,GAAG,cAAc,CAAC,IAAI,CAAC,CAAC;IAC1B,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,EAAE,IAAI,IAAI,IAAI,EAAE,IAAI,IAAI,EACxB,GAAG,CAAG,CAAD,6DAA+D,GAChE,4DAA4D,GAC5D,8DAA8D,GAC9D,4DAA4D,GAC5D,GAAG,WAAW,EAAE,CAAC,CAAC;IAE1B,MAAM,WAAW,GACb,yBAAyB,CAAC,OAAO,EAAE,KAAK,CAAC,UAAU,EAAE,EAAE,CAAC,CAAC;IAC7D,MAAM,WAAW,GACb,yBAAyB,CAAC,QAAQ,EAAE,KAAK,CAAC,WAAW,EAAE,EAAE,CAAC,CAAC;IAE/D,MAAM,SAAS,GAAW,WAAW,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;IAElD,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,WAAW,CAAC,MAAM,KAAK,KAAK,CAAC,MAAM,CAAC,MAAM,EAC1C,GAAG,CAAG,CAAD,AAAC,gBAAA,EAAmB,KAAK,CAAC,MAAM,CAAC,MAAM,CAAA,yBAAA,CAA2B,GACnE,CAAA,SAAA,EAAY,WAAW,CAAC,MAAM,CAAA,gCAAA,CAAkC,GAChE,GAAG,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,UAAU,CAAC,CAAA,CAAA,CAAG,CAAC,CAAC;IAEhD,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,WAAW,CAAC,MAAM,KAAK,KAAK,CAAC,OAAO,CAAC,MAAM,EAC3C,GAAG,CACC,CADC,AACD,gBAAA,EAAmB,KAAK,CAAC,OAAO,CAAC,MAAM,CAAA,0BAAA,CAA4B,GACnE,CAAA,SAAA,EAAY,WAAW,CAAC,MAAM,CAAA,kCAAA,CAAoC,GAClE,GAAG,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,WAAW,CAAC,CAAA,CAAA,CAAG,CAAC,CAAC;IAEjD,IAAK,IAAI,MAAM,GAAG,CAAC,EAAE,MAAM,GAAG,WAAW,CAAC,MAAM,EAAE,MAAM,EAAE,CAAE;QAC1D,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,WAAW,CAAC,MAAM,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,SAAS,EAC1C,GAAG,CAAG,CAAD,AAAC,2BAAA,CAA6B,GAC/B,GAAG,KAAK,CAAC,UAAU,CAAC,MAAM,CAAC,CAAA,KAAA,EACrB,WAAW,CAAC,MAAM,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAA,EAAA,CAAI,GACtC,CAAA,UAAA,EAAa,SAAS,CAAA,gBAAA,EAAmB,KAAK,CAAC,UAAU,CAAC,CAAC,CAAC,CAAA,CAAA,CAAG,CAAC,CAAC;KAC1E;IAED,IAAK,IAAI,MAAM,GAAG,CAAC,EAAE,MAAM,GAAG,WAAW,CAAC,MAAM,EAAE,MAAM,EAAE,CAAE;QAC1D,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,WAAW,CAAC,MAAM,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,SAAS,EAC1C,GAAG,CAAG,CAAD,AAAC,4BAAA,CAA8B,GAChC,GAAG,KAAK,CAAC,WAAW,CAAC,MAAM,CAAC,CAAA,KAAA,EACtB,WAAW,CAAC,MAAM,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAA,EAAA,CAAI,GACtC,CAAA,UAAA,EAAa,SAAS,CAAA,gBAAA,EAAmB,KAAK,CAAC,UAAU,CAAC,CAAC,CAAC,CAAA,CAAA,CAAG,CAAC,CAAC;KAC1E;IAED,OAAO;QAAC,EAAE,EAAE,WAAW;QAAE,EAAE,EAAE,WAAW;IAAA,CAAC,CAAC;AAC5C,CAAC;AAED,SAAS,yBAAyB,CAC9B,aAAqB,EAAE,KAAe,EAAE,MAA0B;IACpE,IAAI,MAAM,YAAY,GAAG,CAAC,8OAAM,EAAE;QAChC,OAAO;YAAC,MAAM;SAAC,CAAC;KACjB,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;QAChC,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,MAAM,CAAC,MAAM,KAAK,KAAK,CAAC,MAAM,EAC9B,GAAG,CAAG,CAAD,AAAC,qBAAA,EAAwB,MAAM,CAAC,MAAM,CAAA,uBAAA,EACvC,KAAK,CAAC,MAAM,CAAA,cAAA,EAAiB,aAAa,CAAA,MAAA,EAAS,KAAK,CAAA,CAAA,CAAG,CAAC,CAAC;QACrE,OAAO,MAAM,CAAC;KACf,MAAM;QACL,MAAM,MAAM,GAAiB,EAAE,CAAC;QAChC,kDAAkD;QAClD,KAAK,MAAM,IAAI,IAAI,KAAK,CAAE;YACxB,IAAI,MAAM,CAAC,IAAI,CAAC,IAAI,IAAI,EAAE;gBACxB,MAAM,IAAI,ySAAU,CAChB,CAAA,6DAAA,CAA+D,GAC/D,GAAG,aAAa,CAAA,MAAA,EAAS,IAAI,CAAA,EAAA,CAAI,CAAC,CAAC;aACxC;YACD,MAAM,CAAC,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC;SAC3B;QACD,OAAO,MAAM,CAAC;KACf;AACH,CAAC;AAED,SAAS,+BAA+B,CACpC,IAIiC;IAEnC,IAAI,IAAI,CAAC,MAAM,KAAK,CAAC,EAAE;QACrB,MAAM,IAAI,kTAAmB,CACzB,wDAAwD,CAAC,CAAC;KAC/D;IACD,OAAO;QAAC,EAAE,EAAE,IAAI,CAAC,CAAC,CAAC;QAAE,EAAE,EAAE,IAAI,CAAC,CAAC,CAAC;IAAA,CAAC,CAAC;AACpC,CAAC;AAEM,KAAK,UAAU,UAAU,CAC5B,6DAA6D;AAC7D,eAAe;AACf,kCAAkC;AAClC,KAAU,EAAE,OAAmB,EAC/B,IAA4B;IAC9B,MAAM,kBAAkB,GAAG,IAAI,CAAC,eAAe,IAAI,IAAI,CAAC;IACxD,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,KAAK,CAAC,SAAS,IAAI,IAAI,EACvB,GAAG,CAAG,CAAD,uDAAyD,GAC1D,0CAA0C,CAAC,CAAC;IAEpD,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,IAAI,IAAI,IAAI,EACZ,GAAG,CAAG,CAAD,AAAC,yDAAA,CAA2D,GAC7D,CAAA,oCAAA,CAAsC,CAAC,CAAC;IAChD,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,IAAI,CAAC,MAAM,IAAI,IAAI,IAAI,IAAI,CAAC,MAAM,GAAG,CAAC,IAAI,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,MAAM,CAAC,EACvE,GAAG,CAAG,CAAA,AAAD,6DAAC,CAA+D,GACjE,CAAA,iBAAA,EAAoB,IAAI,CAAC,MAAM,EAAE,CAAC,CAAC;IAC3C,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,CAAC,kBAAkB,IACd,IAAI,CAAC,eAAe,GAAG,CAAC,IAAI,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,eAAe,CAAC,CAAC,CACxE,GAAG,CAAG,CAAD,AAAC,6DAAA,CAA+D,GACjE,CAAA,uCAAA,EAA0C,IAAI,CAAC,eAAe,EAAE,CAAC,CAAC;IAC1E,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,kCAAkC;IACjC,IAAY,CAAC,iBAAiB,CAAC,IAAI,IAAI,EACxC,GAAG,CAAG,CAAD,uDAAyD,GAC1D,6BAA6B,CAAC,CAAC;IAEvC,IAAI,KAAK,CAAC,UAAU,EAAE;QACpB,MAAM,IAAI,KAAK,CACX,8DAA8D,CAAC,CAAC;KACrE;IACD,KAAK,CAAC,UAAU,GAAG,IAAI,CAAC;IAExB,IAAI;QACF,MAAM,YAAY,GAAG,IAAI,CAAC,cAAc,IAAI,IAAI,CAAC;QACjD,IAAI,KAA8B,CAAC;QACnC,IAAI,KAA8B,CAAC;QACnC,IAAI,YAAY,EAAE;YAChB,IAAI,eAAe,CAAC,IAAI,CAAC,cAAc,CAAC,EAAE;gBACxC,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,IAAI,CAAC,iBAAiB,IAAI,IAAI,IACzB,IAAI,CAAC,iBAAiB,GAAG,CAAC,IAC1B,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,iBAAiB,CAAC,CAAC,CAC9C,GAAG,CAAG,CAAD,AAAC,gDAAA,CAAkD,GACpD,CAAA,yDAAA,CAA2D,GAC3D,CAAA,6BAAA,CAA+B,GAC/B,CAAA,QAAA,EAAW,IAAI,CAAC,iBAAiB,EAAE,CAAC,CAAC;aAC9C,MAAM;gBACL,MAAM,cAAc,GAAG,+BAA+B,CAClD,IAAI,CAAC,cAKJ,CAAC,CAAC;gBACP,KAAK,GAAG,cAAc,CAAC,EAAE,CAAC;gBAC1B,KAAK,GAAG,cAAc,CAAC,EAAE,CAAC;aAC3B;SACF;QAED,MAAM,aAAa,GAAG,KAAK,CAAC,iBAAiB,EAAE,CAAC;QAChD,MAAM,SAAS,GAAG,KAAK,CAAC,sBAAsB,EAAc,CAAC;QAE7D,IAAI,eAAyB,CAAC;QAC9B,IAAI,YAAY,EAAE;YAChB,eAAe,GACX,SAAS,CAAC,KAAK,EAAE,CAAC,MAAM,CAAC,SAAS,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;SAC9D,MAAM;YACL,eAAe,GAAG,SAAS,CAAC,KAAK,EAAE,CAAC;SACrC;QAED,MAAM,SAAS,OAAG,2TAAoB,EAAC,IAAI,CAAC,SAAS,EAAE,IAAI,CAAC,UAAU,CAAC,CAAC;QACxE,MAAM,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,OAAO,CAAC;QACxD,MAAM,EAAC,YAAY,EAAE,OAAO,EAAC,OAAG,yTAAkB,EAC9C,SAAS,EAAE,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE,IAAI,EAAE,IAAI,EAC3C,gBAAgB,CAAC,OAAO,EAAE,IAAI,CAAC,EAC/B,IAAI,EAAG,AACP,YAAY,EAAE,eAAe,CAAC,CAAC,gBADuB;QAE1D,YAAY,CAAC,QAAQ,CAAC,KAAK,CAAC,CAAC;QAC7B,KAAK,CAAC,OAAO,GAAG,OAAO,CAAC;QAExB,MAAM,YAAY,CAAC,YAAY,EAAE,CAAC;QAClC,KAAK,CAAC,aAAa,GAAG,KAAK,CAAC;QAC5B,IAAI,KAAK,GAAG,IAAI,CAAC,YAAY,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,YAAY,CAAC;QAE9D,IAAI,YAAY,GAAG,MAAM,OAAO,CAAC,QAAQ,EAAE,CAAC;QAC5C,MAAO,KAAK,GAAG,IAAI,CAAC,MAAM,CAAE;YAC1B,MAAM,SAAS,GAAmB,CAAA,CAAE,CAAC;YACrC,MAAM,YAAY,CAAC,YAAY,CAAC,KAAK,CAAC,CAAC;YACvC,IAAI,SAAS,GAAG,CAAC,CAAC;YAClB,IAAI,UAAU,GAAG,CAAC,CAAC;YACnB,IAAI,CAAC,kBAAkB,EAAE;gBACvB,YAAY,GAAG,MAAM,OAAO,CAAC,QAAQ,EAAE,CAAC;aACzC;YACD,MAAO,kBAAkB,CAAC,CAAC,CAAC,SAAS,GAAG,IAAI,CAAC,eAAe,CAAC,CAAC,CAAC,IAAI,CAAE;gBACnE,MAAM,WAAW,GAAG,MAAM,YAAY,CAAC,IAAI,EAAE,CAAC;gBAE9C,+DAA+D;gBAC/D,wCAAwC;gBACxC,IAAI,kBAAkB,IAAI,WAAW,CAAC,IAAI,EAAE;oBAC1C,OAAO,CAAC,IAAI,CACR,oCAAoC,GACpC,GAAG,IAAI,CAAC,eAAe,CAAA,EAAA,CAAI,GAC3B,kDAAkD,GAClD,GAAG,SAAS,CAAA,UAAA,CAAY,GACxB,6CAA6C,GAC7C,2DAA2D,GAC3D,yBAAyB,GACzB,GAAG,IAAI,CAAC,eAAe,GAAG,IAAI,CAAC,MAAM,CAAA,WAAA,CAAa,GAClD,0DAA0D,GAC1D,eAAe,CAAC,CAAC;oBACrB,MAAM;iBACP;gBAED,IAAI,WAAW,CAAC,KAAK,IAAI,IAAI,EAAE;oBAC7B,MAAM,EAAC,EAAE,EAAE,EAAE,EAAC,GACV,6BAA6B,CAAC,KAAK,EAAE,WAAW,CAAC,KAAK,CAAC,CAAC;oBAC5D,MAAM,SAAS,GAAmB,CAAA,CAAE,CAAC;oBACrC,SAAS,CAAC,OAAO,CAAC,GAAG,UAAU,CAAC;oBAChC,SAAS,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;oBAEnC,MAAM,YAAY,CAAC,YAAY,CAAC,UAAU,EAAE,SAAS,CAAC,CAAC;oBAEvD,MAAM,aAAa,GAAiB,EAAE,CAAC;oBACvC,IAAI,IAAI,CAAC,WAAW,IAAI,IAAI,EAAE;wBAC5B,MAAM,oBAAoB,OACtB,wUAAuB,EAAC,IAAI,CAAC,WAAW,EAAE,KAAK,CAAC,WAAW,CAAC,CAAC;wBACjE,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,oBAAoB,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;4BACpD,aAAa,CAAC,IAAI,CAAC,UAAM,mUAAkB,EACvC,EAAE,CAAC,CAAC,CAAC,EAAE,IAAI,EAAE,oBAAoB,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;yBAC5C;qBACF;oBAED,kBAAkB;oBAClB,MAAM,GAAG,GAAG,EAAE,CAAC,MAAM,CAAC,EAAE,CAAC,CAAC,MAAM,CAAC,aAAa,CAAC,CAAC;oBAChD,MAAM,IAAI,GAAG,aAAa,CAAC,GAAG,CAAC,CAAC;oBAChC,GAAG,CAAC,gPAAO,CAAC,GAAG,CAAC,CAAC;oBACjB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;wBACzC,MAAM,KAAK,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;wBAC3B,MAAM,GAAG,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;wBACpB,SAAS,CAAC,KAAK,CAAC,GAAG,GAAG,CAAC;wBACvB,GAAG,CAAC,6OAAI,CAAC,GAAG,CAAC,CAAC;qBACf;oBAED,MAAM,YAAY,CAAC,UAAU,CAAC,UAAU,EAAE,SAAS,CAAC,CAAC;wBACrD,iTAAoB,EAAC,SAAS,CAAC,CAAC;oBAEhC,UAAU,EAAE,CAAC;oBACb,SAAS,EAAE,CAAC;iBACb;gBAED,IAAI,kBAAkB,CAAC,CAAC,CAAC,SAAS,IAAI,IAAI,CAAC,eAAe,CAAC,CAAC,CACnC,WAAW,CAAC,IAAI,EAAE;oBACzC,sCAAsC;oBACtC,IAAI,YAAY,EAAE;wBAChB,IAAI,OAAqB,CAAC;wBAC1B,IAAI,eAAe,CAAC,IAAI,CAAC,cAAc,CAAC,EAAE;4BACxC,OAAO,OAAG,qTAAM,EAAC,MAAM,KAAK,CAAC,eAAe,CACxC,IAAI,CAAC,cAAc,EAAE;gCAAC,OAAO,EAAE,IAAI,CAAC,iBAAiB;4BAAA,CAAC,CAAC,CAAC,CAAC;yBAC9D,MAAM;4BACL,OAAO,OAAG,qTAAM,EAAC,KAAK,CAAC,QAAQ,CAAC,KAAK,EAAE,KAAK,EAAE;gCAC5C,SAAS,EAAE,IAAI,CAAC,mBAAmB,IAAI,IAAI,CAAC,CAAC,CACzC,6BAA6B,CAAC,CAAC,CAC/B,IAAI,CAAC,mBAAmB;gCAC5B,OAAO,EAAE,CAAC;6BACX,CAAC,CAAC,CAAC;yBACL;wBACD,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,YAAY,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;4BAClD,SAAS,CAAC,CAAA,IAAA,EAAO,KAAK,CAAC,YAAY,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;yBACxD;qBACF;oBAMD,MAAM;iBACP;gBAED,IAAI,KAAK,CAAC,aAAa,EAAE;oBACvB,MAAM;iBACP;aACF;YACD,MAAM,YAAY,CAAC,UAAU,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC;YAChD,KAAK,EAAE,CAAC;YACR,IAAI,KAAK,CAAC,aAAa,EAAE;gBACvB,MAAM;aACP;SACF;QACD,MAAM,YAAY,CAAC,UAAU,EAAE,CAAC;QAChC,MAAM,KAAK,CAAC,OAAO,CAAC,QAAQ,EAAE,CAAC;QAC/B,OAAO,KAAK,CAAC,OAAO,CAAC;KACtB,QAAS;QACR,KAAK,CAAC,UAAU,GAAG,KAAK,CAAC;KAC1B;AACH,CAAC;AAED,yEAAA,EAA2E,CAC3E,SAAS,gBAAgB,CACrB,OAAmB,EAAE,IAA4B;IACnD,iDAAiD;IACjD,IAAI,aAAa,GAAW,IAAI,CAAC;IACjC,IAAI,IAAI,CAAC,eAAe,IAAI,IAAI,EAAE;QAChC,aAAa,GAAG,IAAI,CAAC,eAAe,CAAC;KACtC,MAAM,IAAI,MAAM,CAAC,QAAQ,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE;QACxC,aAAa,GAAG,OAAO,CAAC,IAAI,CAAC;KAC9B;IACD,OAAO,aAAa,CAAC;AACvB,CAAC;AAED,yEAAyE;AACzE,WAAW;AACX,SAAS,eAAe,CACpB,OAIU;IACZ,OAAO,AAAC,OAAQ,OAAsB,CAAC,QAAQ,KAAK,UAAU,CAAC,CAAC;AAClE,CAAC;AAED,2EAA2E;AAC3E,WAAW;AACX,SAAS,oBAAoB,CAAI,QACe;IAC9C,OAAO,AAAC,OAAQ,QAA4B,CAAC,IAAI,KAAK,UAAU,CAAC,CAAC;AACpE,CAAC;AAEM,KAAK,UAAU,eAAe,CACjC,6DAA6D;AAC7D,eAAe;AACf,kCAAkC;AAClC,KAAU,EAAE,OAAmC,EAC/C,IAA8B;IAChC,IAAI,GAAG,IAAI,IAAI,CAAA,CAAE,CAAC;IAClB,MAAM,UAAU,GAAG,IAAI,CAAC,OAAO,IAAI,IAAI,CAAC;IACxC,MAAM,CAAC,GAAG,KAAK,CAAC,YAAY,CAAC;IAC7B,IAAI,IAAI,GAAiB,EAAE,CAAC;IAC5B,IAAI,IAAI,CAAC,OAAO,GAAG,CAAC,EAAE;QACpB,MAAM,IAAI,kTAAmB,CAAC,sCAAsC,CAAC,CAAC;KACvE;IAED,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,CAAC,UAAU,IAAI,AAAC,IAAI,CAAC,OAAO,GAAG,CAAC,IAAI,MAAM,CAAC,SAAS,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,CACnE,GAAG,CAAG,CAAD,2DAA6D,GAC9D,CAAA,SAAA,EAAY,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,OAAO,CAAC,EAAE,CAAC,CAAC;IACpD,MAAM,YAAY,GAAG,oBAAoB,CAAC,OAAO,CAAC,CAAC,CAAC,CAChD,OAA0B,CAAA,CAAC,CAC3B,MAAO,OAAsB,CAAC,QAAQ,EAAE,CAAC;IAC7C,6DAA6D;IAC7D,IAAI,WAAW,GAAG,CAAC,CAAC;IACpB,IAAI,KAAK,GAAG,CAAC,CAAC;IAEd,MAAO,UAAU,CAAC,CAAC,CAAC,KAAK,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,IAAI,CAAE;QAC/C,MAAM,WAAW,GAAG,MAAM,YAAY,CAAC,IAAI,EAAE,CAAC;QAC9C,IAAI,GAAG,GAAG,CAAC,6OAAI,CAAC,GAAG,EAAE;YACnB,IAAI,WAAW,CAAC,KAAK,EAAE;gBACrB,kDAAkD;gBAClD,+DAA+D;gBAC/D,MAAM,EAAC,EAAE,EAAE,EAAE,EAAC,GACV,6BAA6B,CAAC,KAAK,EAAE,WAAW,CAAC,KAAK,CAAC,CAAC;gBAC5D,MAAM,OAAO,GAAG,EAAE,CAAC,MAAM,CAAC,EAAE,CAAC,CAAC;gBAC9B,MAAM,SAAS,GAAG,GAAG,CAAC,6OAAI,CAAC,GAAG,CAAG,CAAD,AAAE,CAAC,OAAO,CAAC,CAAC,CAAC;gBAC7C,GAAG,CAAC,gPAAO,CAAC,OAAO,CAAC,CAAC;gBAErB,IAAI,KAAK,KAAK,CAAC,EAAE;oBACf,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;wBACzC,IAAI,CAAC,IAAI,KAAC,yPAAM,EAAC,CAAC,CAAC,CAAC,CAAC;qBACtB;iBACF;gBAED,MAAM,SAAS,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;gBACtC,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;oBACzC,MAAM,QAAQ,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;oBAC9B,MAAM,SAAS,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;oBAC1B,IAAI,CAAC,CAAC,CAAC,GACH,GAAG,CAAC,6OAAI,CAAC,GAAG,CAAG,CAAD,EAAI,CAAC,+OAAG,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,GAAG,CAAC,+OAAG,CAAC,SAAS,EAAE,QAAQ,CAAC,CAAC,CAAC,CAAC;oBACnE,IAAI,KAAK,GAAG,CAAC,EAAE;wBACb,GAAG,CAAC,gPAAO,CAAC,SAAS,CAAC,CAAC;qBACxB;iBACF;gBACD,GAAG,CAAC,gPAAO,CAAC,SAAS,CAAC,CAAC;gBACvB,WAAW,IAAI,SAAS,CAAC;gBAEzB,EAAE,KAAK,CAAC;aACT;YACD,OAAO,IAAI,CAAC;QACd,CAAC,CAAC,CAAC;QAEH,IAAI,WAAW,CAAC,IAAI,EAAE;YACpB,IAAI,UAAU,EAAE;gBACd,OAAO,CAAC,IAAI,CACR,kEAAkE,GAClE,8CAA8C,GAC9C,0CAA0C,GAC1C,CAAA,uBAAA,EAA0B,IAAI,CAAC,OAAO,CAAA,WAAA,CAAa,GACnD,0DAA0D,GAC1D,eAAe,CAAC,CAAC;aACtB;YACD,MAAM;SACP;KACF;IAED,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;QACpC,MAAM,SAAS,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;QAC1B,IAAI,CAAC,CAAC,CAAC,GAAG,GAAG,CAAC,+OAAG,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,WAAW,CAAC,CAAC;QACxC,GAAG,CAAC,gPAAO,CAAC,SAAS,CAAC,CAAC;KACxB;IAED,WAAO,+TAAgB,EAAC,IAAI,CAAC,CAAC;AAChC,CAAC"}},
    {"offset": {"line": 3302, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-layers@4.22.0_@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-layers/dist/engine/training_tensors.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-layers/src/engine/training_tensors.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using tf.Tensor objects.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {Tensor, Tensor1D} from '@tensorflow/tfjs-core';\nimport {expandDims, gather, sliceAlongFirstAxis} from '../backend/tfjs_backend';\nimport {BaseCallback, CustomCallbackArgs, ModelLoggingVerbosity, YieldEveryOptions} from '../base_callbacks';\nimport {ClassWeight, ClassWeightMap} from './training_utils';\n\n/**\n * Interface configuration model training based on data as `tf.Tensor`s.\n */\nexport interface ModelFitArgs {\n  /**\n   * Number of samples per gradient update. If unspecified, it\n   * will default to 32.\n   */\n  batchSize?: number;\n\n  /**\n   * Integer number of times to iterate over the training data arrays.\n   */\n  epochs?: number;\n\n  /**\n   * Verbosity level.\n   *\n   * Expected to be 0, 1, or 2. Default: 1.\n   *\n   * 0 - No printed message during fit() call.\n   * 1 - In Node.js (tfjs-node), prints the progress bar, together with\n   *     real-time updates of loss and metric values and training speed.\n   *     In the browser: no action. This is the default.\n   * 2 - Not implemented yet.\n   */\n  verbose?: ModelLoggingVerbosity | 2;\n\n  /**\n   * List of callbacks to be called during training.\n   * Can have one or more of the following callbacks:\n   *   - `onTrainBegin(logs)`: called when training starts.\n   *   - `onTrainEnd(logs)`: called when training ends.\n   *   - `onEpochBegin(epoch, logs)`: called at the start of every epoch.\n   *   - `onEpochEnd(epoch, logs)`: called at the end of every epoch.\n   *   - `onBatchBegin(batch, logs)`: called at the start of every batch.\n   *   - `onBatchEnd(batch, logs)`: called at the end of every batch.\n   *   - `onYield(epoch, batch, logs)`: called every `yieldEvery` milliseconds\n   *      with the current epoch, batch and logs. The logs are the same\n   *      as in `onBatchEnd()`. Note that `onYield` can skip batches or\n   *      epochs. See also docs for `yieldEvery` below.\n   */\n  callbacks?: BaseCallback[]|CustomCallbackArgs|CustomCallbackArgs[];\n\n  /**\n   * Float between 0 and 1: fraction of the training data\n   * to be used as validation data. The model will set apart this fraction of\n   * the training data, will not train on it, and will evaluate the loss and\n   * any model metrics on this data at the end of each epoch.\n   * The validation data is selected from the last samples in the `x` and `y`\n   * data provided, before shuffling.\n   */\n  validationSplit?: number;\n\n  /**\n   * Data on which to evaluate the loss and any model\n   * metrics at the end of each epoch. The model will not be trained on this\n   * data. This could be a tuple [xVal, yVal] or a tuple [xVal, yVal,\n   * valSampleWeights]. The model will not be trained on this data.\n   * `validationData` will override `validationSplit`.\n   */\n  validationData?: [\n    Tensor|Tensor[], Tensor|Tensor[]\n  ]|[Tensor | Tensor[], Tensor|Tensor[], Tensor|Tensor[]];\n\n  /**\n   * Whether to shuffle the training data before each epoch. Has\n   * no effect when `stepsPerEpoch` is not `null`.\n   */\n  shuffle?: boolean;\n\n  /**\n   * Optional object mapping class indices (integers) to\n   * a weight (float) to apply to the model's loss for the samples from this\n   * class during training. This can be useful to tell the model to \"pay more\n   * attention\" to samples from an under-represented class.\n   *\n   * If the model has multiple outputs, a class weight can be specified for\n   * each of the outputs by setting this field an array of weight object\n   * or an object that maps model output names (e.g., `model.outputNames[0]`)\n   * to weight objects.\n   */\n  classWeight?: ClassWeight|ClassWeight[]|ClassWeightMap;\n\n  /**\n   * Optional array of the same length as x, containing\n   * weights to apply to the model's loss for each sample. In the case of\n   * temporal data, you can pass a 2D array with shape (samples,\n   * sequenceLength), to apply a different weight to every timestep of every\n   * sample. In this case you should make sure to specify\n   * sampleWeightMode=\"temporal\" in compile().\n   */\n  sampleWeight?: Tensor;\n\n  /**\n   * Epoch at which to start training (useful for resuming a previous training\n   * run). When this is used, `epochs` is the index of the \"final epoch\".\n   * The model is not trained for a number of iterations given by `epochs`,\n   * but merely until the epoch of index `epochs` is reached.\n   */\n  initialEpoch?: number;\n\n  /**\n   * Total number of steps (batches of samples) before\n   * declaring one epoch finished and starting the next epoch. When training\n   * with Input Tensors such as TensorFlow data tensors, the default `null` is\n   * equal to the number of unique samples in your dataset divided by the\n   * batch size, or 1 if that cannot be determined.\n   */\n  stepsPerEpoch?: number;\n\n  /**\n   * Only relevant if `stepsPerEpoch` is specified. Total number of steps\n   * (batches of samples) to validate before stopping.\n   */\n  validationSteps?: number;\n\n  /**\n   * Configures the frequency of yielding the main thread to other tasks.\n   *\n   * In the browser environment, yielding the main thread can improve the\n   * responsiveness of the page during training. In the Node.js environment,\n   * it can ensure tasks queued in the event loop can be handled in a timely\n   * manner.\n   *\n   * The value can be one of the following:\n   *   - `'auto'`: The yielding happens at a certain frame rate (currently set\n   *               at 125ms). This is the default.\n   *   - `'batch'`: yield every batch.\n   *   - `'epoch'`: yield every epoch.\n   *   - any `number`: yield every `number` milliseconds.\n   *   - `'never'`: never yield. (yielding can still happen through `await\n   *      nextFrame()` calls in custom callbacks.)\n   */\n  yieldEvery?: YieldEveryOptions;\n}\n\nexport function checkBatchSize(batchSize: number) {\n  tfc.util.assert(\n      batchSize > 0 && Number.isInteger(batchSize),\n      () => `batchSize is required to be a positive integer, but got ${\n          batchSize}`);\n}\n\n/**\n * Slice a Tensor or an Array of Tensors, by start and stop indices.\n *\n * Porting Note: The `_slice_arrays` function in PyKeras is covered by this\n *   function and `sliceArraysByIndices()` together.\n *\n * @param arrays: the input.\n * @param start: the starting index (inclusive).\n * @param stop: the stopping index (exclusive).\n * @returns The result of the slicing. If `arrays` is an `Array` of\n *   `tf.Tensor`s, the slicing will be applied to all elements of the `Array`\n *   in the same way.\n */\nexport function sliceArrays(\n    arrays: Tensor|Tensor[], start: number, stop: number): Tensor|Tensor[] {\n  if (arrays == null) {\n    return [null];\n  } else if (Array.isArray(arrays)) {\n    return arrays.map(array => sliceAlongFirstAxis(array, start, stop - start));\n  } else {  // Tensor.\n    return sliceAlongFirstAxis(arrays, start, stop - start);\n  }\n}\n\n/**\n * Slice a Tensor or an Array of Tensors, by random-order indices.\n *\n * Porting Note: The `_slice_arrays` function in PyKeras is covered by this\n *   function and `sliceArrays()` together.\n *\n * @param arrays The input `tf.Tensor` or `Array` of `tf.Tensor`s to slice.\n *   If an `Array` of `tf.Tensor`s, all `tf.Tensor`s will be sliced in the\n *   same fashion.\n * @param indices The indices to use for slicing along the first (batch)\n *   dimension.\n * @returns Result(s) of the slicing.\n */\nexport function sliceArraysByIndices(\n    arrays: Tensor|Tensor[], indices: Tensor1D): Tensor|Tensor[] {\n  return tfc.tidy(() => {\n    if (arrays == null) {\n      return null;\n    } else if (Array.isArray(arrays)) {\n      return arrays.map(\n          array => (sliceArraysByIndices(array, indices) as Tensor));\n    } else {\n      // TODO(cais): indices should be a pre-constructed Tensor1D to avoid\n      //   tensor1d() calls.\n      return gather(\n          arrays,\n          indices.dtype === 'int32' ? indices : tfc.cast(indices, 'int32'));\n    }\n  });\n}\n\n/**\n * Returns a list of batch indices (tuples of indices).\n * @param size: Integer, total size of the data to slice into batches.\n * @param batchSize: Integer, batch size.\n * @returns An Array of [batchStart, batchEnd] tuples. batchStart is\n *   inclusive; batchEnd is exclusive. I.e., each batch consists of indices x\n *   that satisfy batchStart <= x < batchEnd.\n */\nexport function makeBatches(\n    size: number, batchSize: number): Array<[number, number]> {\n  const output: Array<[number, number]> = [];\n  let batchStart = 0;\n  let batchEnd: number = null;\n  while (batchStart < size) {\n    batchEnd = batchStart + batchSize;\n    if (batchEnd >= size) {\n      batchEnd = size;\n    }\n    output.push([batchStart, batchEnd]);\n    batchStart = batchEnd;\n  }\n  return output;\n}\n\n/**\n * Ensure tensors all have a rank of at least 2.\n *\n * If a tensor has a rank of 1, it is dimension-expanded to rank 2.\n * If any tensor has a rank of 0 (i.e., is a scalar), an error will be thrown.\n */\nexport function ensureTensorsRank2OrHigher(tensors: Tensor|Tensor[]): Tensor[] {\n  const outs: Tensor[] = [];\n  if (tensors instanceof Tensor) {\n    tensors = [tensors];\n  }\n\n  // Make Tensors at least 2D.\n  for (let i = 0; i < tensors.length; ++i) {\n    const tensor = tensors[i];\n    if (tensor.rank === 1) {\n      outs.push(expandDims(tensor, 1));\n    } else if (tensor.rank === 0) {\n      throw new Error(\n          'Expected tensor to be at least 1D, but received a 0D tensor ' +\n          '(scalar).');\n    } else {\n      outs.push(tensor);\n    }\n  }\n  return outs;\n}\n\n/**\n * Compare a set of tensors with a reference (old) set, discard the ones\n * in the new set that are not present in the reference set.\n *\n * This method is used for memory clenaup during calls such as\n * LayersModel.fit().\n *\n * @param tensors New set which may contain Tensors not present in\n *   `refTensors`.\n * @param refTensors Reference Tensor set.\n */\n// TODO(cais, kangyizhang): Deduplicate with tfjs-data.\nexport function disposeNewTensors(\n    tensors: Tensor|Tensor[]|{[inputName: string]: Tensor},\n    refTensors: Tensor|Tensor[]|{[inputName: string]: Tensor}): void {\n  if (tensors == null) {\n    return;\n  }\n  const oldTensorIds: number[] = [];\n  if (refTensors instanceof Tensor) {\n    oldTensorIds.push(refTensors.id);\n  } else if (Array.isArray(refTensors)) {\n    refTensors.forEach(t => oldTensorIds.push(t.id));\n  } else if (refTensors != null) {\n    // `oldTensors` is a map from string name to Tensor.\n    for (const name in refTensors) {\n      const oldTensor = refTensors[name];\n      oldTensorIds.push(oldTensor.id);\n    }\n  }\n\n  const tensorsToDispose: Tensor[] = [];\n  if (tensors instanceof Tensor) {\n    if (oldTensorIds.indexOf(tensors.id) === -1) {\n      tensorsToDispose.push(tensors);\n    }\n  } else if (Array.isArray(tensors)) {\n    tensors.forEach(t => {\n      if (oldTensorIds.indexOf(t.id) === -1) {\n        tensorsToDispose.push(t);\n      }\n    });\n  } else if (tensors != null) {\n    // `oldTensors` is a map from string name to Tensor.\n    for (const name in tensors) {\n      const tensor = tensors[name];\n      if (oldTensorIds.indexOf(tensor.id) === -1) {\n        tensorsToDispose.push(tensor);\n      }\n    }\n  }\n\n  tensorsToDispose.forEach(t => {\n    if (!t.isDisposed) {\n      t.dispose();\n    }\n  });\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;AAAA;;;;;;;;GAQG,CAEH;;GAEG;;;AAEH,OAAO,KAAK,GAAG,MAAM,uBAAuB,CAAC;AAC7C,OAAO,EAAC,MAAM,EAAW,MAAM,uBAAuB,CAAC;AACvD,OAAO,EAAC,UAAU,EAAE,MAAM,EAAE,mBAAmB,EAAC,MAAM,yBAAyB,CAAC;;;;AA6I1E,SAAU,cAAc,CAAC,SAAiB;IAC9C,GAAG,CAAC,0QAAI,CAAC,MAAM,CACX,SAAS,GAAG,CAAC,IAAI,MAAM,CAAC,SAAS,CAAC,SAAS,CAAC,EAC5C,GAAG,CAAG,CAAD,AAAC,wDAAA,EACF,SAAS,EAAE,CAAC,CAAC;AACvB,CAAC;AAeK,SAAU,WAAW,CACvB,MAAuB,EAAE,KAAa,EAAE,IAAY;IACtD,IAAI,MAAM,IAAI,IAAI,EAAE;QAClB,OAAO;YAAC,IAAI;SAAC,CAAC;KACf,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;QAChC,OAAO,MAAM,CAAC,GAAG,EAAC,KAAK,CAAC,EAAE,IAAC,mUAAmB,EAAC,KAAK,EAAE,KAAK,EAAE,IAAI,GAAG,KAAK,CAAC,CAAC,CAAC;KAC7E,MAAM,EAAG,UAAU;QAClB,WAAO,mUAAmB,EAAC,MAAM,EAAE,KAAK,EAAE,IAAI,GAAG,KAAK,CAAC,CAAC;KACzD;AACH,CAAC;AAeK,SAAU,oBAAoB,CAChC,MAAuB,EAAE,OAAiB;IAC5C,OAAO,GAAG,CAAC,6OAAI,CAAC,GAAG,EAAE;QACnB,IAAI,MAAM,IAAI,IAAI,EAAE;YAClB,OAAO,IAAI,CAAC;SACb,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;YAChC,OAAO,MAAM,CAAC,GAAG,EACb,KAAK,CAAC,EAAE,AAAE,oBAAoB,CAAC,KAAK,EAAE,OAAO,CAAY,CAAC,CAAC;SAChE,MAAM;YACL,oEAAoE;YACpE,sBAAsB;YACtB,WAAO,sTAAM,EACT,MAAM,EACN,OAAO,CAAC,KAAK,KAAK,OAAO,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,GAAG,CAAC,iPAAI,CAAC,OAAO,EAAE,OAAO,CAAC,CAAC,CAAC;SACvE;IACH,CAAC,CAAC,CAAC;AACL,CAAC;AAUK,SAAU,WAAW,CACvB,IAAY,EAAE,SAAiB;IACjC,MAAM,MAAM,GAA4B,EAAE,CAAC;IAC3C,IAAI,UAAU,GAAG,CAAC,CAAC;IACnB,IAAI,QAAQ,GAAW,IAAI,CAAC;IAC5B,MAAO,UAAU,GAAG,IAAI,CAAE;QACxB,QAAQ,GAAG,UAAU,GAAG,SAAS,CAAC;QAClC,IAAI,QAAQ,IAAI,IAAI,EAAE;YACpB,QAAQ,GAAG,IAAI,CAAC;SACjB;QACD,MAAM,CAAC,IAAI,CAAC;YAAC,UAAU;YAAE,QAAQ;SAAC,CAAC,CAAC;QACpC,UAAU,GAAG,QAAQ,CAAC;KACvB;IACD,OAAO,MAAM,CAAC;AAChB,CAAC;AAQK,SAAU,0BAA0B,CAAC,OAAwB;IACjE,MAAM,IAAI,GAAa,EAAE,CAAC;IAC1B,IAAI,OAAO,YAAY,kPAAM,EAAE;QAC7B,OAAO,GAAG;YAAC,OAAO;SAAC,CAAC;KACrB;IAED,4BAA4B;IAC5B,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,OAAO,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;QACvC,MAAM,MAAM,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;QAC1B,IAAI,MAAM,CAAC,IAAI,KAAK,CAAC,EAAE;YACrB,IAAI,CAAC,IAAI,KAAC,0TAAU,EAAC,MAAM,EAAE,CAAC,CAAC,CAAC,CAAC;SAClC,MAAM,IAAI,MAAM,CAAC,IAAI,KAAK,CAAC,EAAE;YAC5B,MAAM,IAAI,KAAK,CACX,8DAA8D,GAC9D,WAAW,CAAC,CAAC;SAClB,MAAM;YACL,IAAI,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;SACnB;KACF;IACD,OAAO,IAAI,CAAC;AACd,CAAC;AAcK,SAAU,iBAAiB,CAC7B,OAAsD,EACtD,UAAyD;IAC3D,IAAI,OAAO,IAAI,IAAI,EAAE;QACnB,OAAO;KACR;IACD,MAAM,YAAY,GAAa,EAAE,CAAC;IAClC,IAAI,UAAU,YAAY,kPAAM,EAAE;QAChC,YAAY,CAAC,IAAI,CAAC,UAAU,CAAC,EAAE,CAAC,CAAC;KAClC,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,UAAU,CAAC,EAAE;QACpC,UAAU,CAAC,OAAO,EAAC,CAAC,CAAC,EAAG,AAAD,YAAa,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;KAClD,MAAM,IAAI,UAAU,IAAI,IAAI,EAAE;QAC7B,oDAAoD;QACpD,IAAK,MAAM,IAAI,IAAI,UAAU,CAAE;YAC7B,MAAM,SAAS,GAAG,UAAU,CAAC,IAAI,CAAC,CAAC;YACnC,YAAY,CAAC,IAAI,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC;SACjC;KACF;IAED,MAAM,gBAAgB,GAAa,EAAE,CAAC;IACtC,IAAI,OAAO,YAAY,kPAAM,EAAE;QAC7B,IAAI,YAAY,CAAC,OAAO,CAAC,OAAO,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,EAAE;YAC3C,gBAAgB,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;SAChC;KACF,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,EAAE;QACjC,OAAO,CAAC,OAAO,EAAC,CAAC,CAAC,EAAE;YAClB,IAAI,YAAY,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,EAAE;gBACrC,gBAAgB,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;aAC1B;QACH,CAAC,CAAC,CAAC;KACJ,MAAM,IAAI,OAAO,IAAI,IAAI,EAAE;QAC1B,oDAAoD;QACpD,IAAK,MAAM,IAAI,IAAI,OAAO,CAAE;YAC1B,MAAM,MAAM,GAAG,OAAO,CAAC,IAAI,CAAC,CAAC;YAC7B,IAAI,YAAY,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,EAAE;gBAC1C,gBAAgB,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;aAC/B;SACF;KACF;IAED,gBAAgB,CAAC,OAAO,EAAC,CAAC,CAAC,EAAE;QAC3B,IAAI,CAAC,CAAC,CAAC,UAAU,EAAE;YACjB,CAAC,CAAC,OAAO,EAAE,CAAC;SACb;IACH,CAAC,CAAC,CAAC;AACL,CAAC"}},
    {"offset": {"line": 3445, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-layers@4.22.0_@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-layers/src/engine/training.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original Source: engine/training.py */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {io, ModelPredictConfig as ModelPredictArgs, NamedTensorMap, Optimizer, Scalar, scalar, serialization, Tensor, Tensor1D, tensor1d, util} from '@tensorflow/tfjs-core';\n\nimport * as K from '../backend/tfjs_backend';\nimport {BaseCallback, configureCallbacks, History, ModelLoggingVerbosity, standardizeCallbacks} from '../base_callbacks';\nimport {nameScope} from '../common';\nimport {NotImplementedError, RuntimeError, ValueError} from '../errors';\nimport {Shape} from '../keras_format/common';\nimport {LossIdentifier} from '../keras_format/loss_config';\nimport {OptimizerSerialization} from '../keras_format/optimizer_config';\nimport {MetricsIdentifier, TrainingConfig} from '../keras_format/training_config';\nimport {deserialize} from '../layers/serialization';\nimport { disposeTensorsInLogs, UnresolvedLogs } from '../logs';\nimport * as losses from '../losses';\nimport * as Metrics from '../metrics';\nimport * as optimizers from '../optimizers';\nimport {LossOrMetricFn, NamedTensor} from '../types';\nimport {checkUserDefinedMetadata} from '../user_defined_metadata';\nimport {count, pyListRepeat, singletonOrArray, toCamelCase, toSnakeCase, unique} from '../utils/generic_utils';\nimport {printSummary} from '../utils/layer_utils';\nimport {range} from '../utils/math_utils';\nimport {convertPythonicToTs} from '../utils/serialization_utils';\nimport {LayerVariable} from '../variables';\nimport {version} from '../version';\n\nimport {Container, ContainerArgs} from './container';\nimport {Dataset} from './dataset_stub';\nimport {execute, FeedDict} from './executor';\nimport {DisposeResult, SymbolicTensor} from './topology';\nimport {evaluateDataset, fitDataset, ModelEvaluateDatasetArgs, ModelFitDatasetArgs} from './training_dataset';\nimport {checkBatchSize, disposeNewTensors, ensureTensorsRank2OrHigher, makeBatches, ModelFitArgs, sliceArrays, sliceArraysByIndices} from './training_tensors';\nimport {ClassWeight, ClassWeightMap, computeWeightedLoss, standardizeClassWeights, standardizeWeights} from './training_utils';\n\n/**\n * Helper function for polymorphic input data: 1. singleton Tensor.\n */\nexport function isDataTensor(x: Tensor|Tensor[]|{[inputName: string]: Tensor}|\n                             {[inputName: string]: Tensor[]}): boolean {\n  return x instanceof Tensor;\n}\n\n/**\n * Helper function for polymorphic input data: 2. Array of Tensor.\n */\nexport function isDataArray(x: Tensor|Tensor[]|\n                            {[inputName: string]: Tensor}): boolean {\n  return Array.isArray(x);\n}\n\n/**\n * Helper function for polymorphic input data: 3. \"dict\" of Tensor.\n */\nexport function isDataDict(x: Tensor|Tensor[]|\n                           {[inputName: string]: Tensor}): boolean {\n  return !isDataTensor(x) && !isDataArray(x);\n}\n\n/**\n * Normalizes inputs and targets provided by users.\n * @param data User-provided input data (polymorphic).\n * @param names An Array of expected Tensor names.\n * @param shapes Optional Array of expected Tensor shapes.\n * @param checkBatchAxis Whether to check that the batch axis of the arrays\n *   match  the expected value found in `shapes`.\n * @param exceptionPrefix String prefix used for exception formatting.\n * @returns List of standardized input Tensors (one Tensor per model input).\n * @throws ValueError: in case of improperly formatted user data.\n */\nexport function standardizeInputData(\n    data: Tensor|Tensor[]|{[inputName: string]: Tensor}, names: string[],\n    shapes?: Shape[], checkBatchAxis = true, exceptionPrefix = ''): Tensor[] {\n  if (names == null || names.length === 0) {\n    // Check for the case where the model expected no data, but some data got\n    // sent.\n    if (data != null) {\n      let gotUnexpectedData = false;\n      if (isDataArray(data) && (data as Tensor[]).length > 0) {\n        gotUnexpectedData = true;\n      } else if (isDataDict(data)) {\n        for (const key in data) {\n          if (data.hasOwnProperty(key)) {\n            gotUnexpectedData = true;\n            break;\n          }\n        }\n      } else {\n        // `data` is a singleton Tensor in this case.\n        gotUnexpectedData = true;\n      }\n      if (gotUnexpectedData) {\n        throw new ValueError(\n            `Error when checking model ${exceptionPrefix} expected no data, ` +\n            `but got ${data}`);\n      }\n    }\n    return [];\n  }\n  if (data == null) {\n    return names.map(name => null);\n  }\n\n  let arrays: Tensor[];\n  if (isDataDict(data)) {\n    data = data as {[inputName: string]: Tensor};\n    arrays = [];\n    for (const name of names) {\n      if (data[name] == null) {\n        throw new ValueError(\n            `No data provided for \"${name}\". Need data for each key in: ` +\n            `${names}`);\n      }\n      arrays.push(data[name]);\n    }\n  } else if (isDataArray(data)) {\n    data = data as Tensor[];\n    if (data.length !== names.length) {\n      throw new ValueError(\n          `Error when checking model ${exceptionPrefix}: the Array of ` +\n          `Tensors that you are passing to your model is not the size the ` +\n          `model expected. Expected to see ${names.length} Tensor(s), but ` +\n          `instead got the following list of Tensor(s): ${data}`);\n    }\n    arrays = data;\n  } else {\n    data = data as Tensor;\n    if (names.length > 1) {\n      throw new ValueError(\n          `The model ${exceptionPrefix} expects ${names.length} Tensor(s), ` +\n          `but only received one Tensor. Found: Tensor with shape ${\n              data.shape}`);\n    }\n    arrays = [data];\n  }\n\n  arrays = ensureTensorsRank2OrHigher(arrays);\n\n  // Check shape compatibility.\n  if (shapes != null) {\n    for (let i = 0; i < names.length; ++i) {\n      if (shapes[i] == null) {\n        continue;\n      }\n      const array = arrays[i];\n      if (array.shape.length !== shapes[i].length) {\n        throw new ValueError(\n            `Error when checking ${exceptionPrefix}: expected ${names[i]} ` +\n            `to have ${shapes[i].length} dimension(s). but got array with ` +\n            `shape ${array.shape}`);\n      }\n      for (let j = 0; j < shapes[i].length; ++j) {\n        if (j === 0 && !checkBatchAxis) {\n          // Skip the first (batch) axis.\n          continue;\n        }\n        const dim = array.shape[j];\n        const refDim = shapes[i][j];\n        if (refDim != null && refDim >= 0 && dim !== refDim) {\n          throw new ValueError(\n              `${exceptionPrefix} expected a batch of elements where each ` +\n              `example has shape [${shapes[i].slice(1, shapes[i].length)}] ` +\n              `(i.e.,tensor shape [*,${\n                  shapes[i].slice(1, shapes[i].length)}])` +\n              ` but the ${exceptionPrefix} received an input with ${\n                  array.shape[0]}` +\n              ` examples, each with shape [${\n                  array.shape.slice(1, array.shape.length)}]` +\n              ` (tensor shape [${array.shape}])`);\n        }\n      }\n    }\n  }\n  return arrays;\n}\n\n/**\n * User input validation for Tensors.\n * @param inputs `Array` of `tf.Tensor`s for inputs.\n * @param targets `Array` of `tf.Tensor`s for targets.\n * @param weights Optional `Array` of `tf.Tensor`s for sample weights.\n * @throws ValueError: in case of incorrectly formatted data.\n */\nexport function checkArrayLengths(\n    inputs: Tensor[], targets: Tensor[], weights?: Tensor[]) {\n  const setX = unique(inputs.map(input => input.shape[0]));\n  setX.sort();\n  const setY = unique(targets.map(target => target.shape[0]));\n  setY.sort();\n  // TODO(cais): Check `weights` as well.\n  if (setX.length > 1) {\n    throw new ValueError(\n        `All input Tensors (x) should have the same number of samples. ` +\n        `Got array shapes: ` +\n        `${JSON.stringify(inputs.map(input => input.shape))}`);\n  }\n  if (setY.length > 1) {\n    throw new ValueError(\n        `All target Tensors (y) should have the same number of samples. ` +\n        `Got array shapes: ` +\n        `${JSON.stringify(targets.map(target => target.shape))}`);\n  }\n  if (setX.length > 0 && setY.length > 0 && !util.arraysEqual(setX, setY)) {\n    throw new ValueError(\n        `Input Tensors should have the same number of samples as target ` +\n        `Tensors. Found ${setX[0]} input sample(s) and ${setY[0]} target ` +\n        `sample(s).`);\n  }\n}\n\n/**\n * Validation on the compatibility of targes and loss functions.\n *\n * This helps prevent users from using loss functions incorrectly.\n *\n * @param targets `Array` of `tf.Tensor`s of targets.\n * @param lossFns `Array` of loss functions.\n * @param outputShapes `Array` of shapes of model outputs.\n */\nfunction checkLossAndTargetCompatibility(\n    targets: Tensor[], lossFns: LossOrMetricFn[], outputShapes: Shape[]) {\n  // TODO(cais): Dedicated test coverage?\n  const keyLosses = [\n    losses.meanSquaredError, losses.binaryCrossentropy,\n    losses.categoricalCrossentropy\n  ];\n  for (let i = 0; i < targets.length; ++i) {\n    const y = targets[i];\n    const loss = lossFns[i];\n    const shape = outputShapes[i];\n    if (loss == null) {\n      continue;\n    }\n    if (loss === losses.categoricalCrossentropy) {\n      if (y.shape[y.shape.length - 1] === 1) {\n        throw new ValueError(\n            `You are passing a target array of shape ${y.shape} while using ` +\n            `a loss 'categorical_crossentropy'. 'categorical_crossentropy'` +\n            `expects targets to be binary matrices (1s and 0s) of shape ` +\n            `[samples, classes].`);\n        // TODO(cais): Example code in error message.\n      }\n    }\n    if (keyLosses.indexOf(loss) !== -1) {\n      const slicedYShape = y.shape.slice(1);\n      const slicedShape = shape.slice(1);\n      for (let j = 0; j < slicedYShape.length; ++j) {\n        const targetDim = slicedYShape[j];\n        const outDim = slicedShape[j];\n        if (outDim != null && targetDim !== outDim) {\n          throw new ValueError(\n              `A target Tensor with shape ${y.shape} was passed for an ` +\n              `output of shape ${shape}, while using a loss function that ` +\n              `expects targets to have the same shape as the output.`);\n        }\n      }\n    }\n  }\n}\n\n/**\n * Check inputs provided by the user.\n *\n * Porting Note: This corresponds to _standardize_input_data() in Python\n *   Keras. Because of the strong typing in TF.js, we do not need to convert\n *   the data. Specifically:\n *   1) in PyKeras, `data` can be `DataFrame` instances from pandas, for\n *      example. We don't need to worry about that here because there is no\n *      widely popular javascript/typesdcript equivalent of pandas (so far).\n *      If one becomes available in the future, we can add support.\n *   2) in PyKeras, inputs can be Python dict. But here we are stipulating\n * that the data is either a single `tf.Tensor` or an Array of `tf.Tensor`s. We\n * may add support for `Object` data inputs in the future when the need\n * arises.\n *\n * Instead, we perform basic checks for number of parameters and shapes.\n *\n * @param data: The input data.\n * @param names: Name for the inputs, from the model.\n * @param shapes: Expected shapes for the input data, from the model.\n * @param checkBatchAxis: Whether the size along the batch axis (i.e., the\n *   first dimension) will be checked for matching.\n * @param exceptionPrefix: Execption prefix message, used in generating error\n *   messages.\n * @throws ValueError: on incorrect number of inputs or mismatches in shapes.\n */\nfunction checkInputData(\n    data: Tensor|Tensor[], names: string[], shapes?: Shape[],\n    checkBatchAxis = true, exceptionPrefix = '') {\n  let arrays: Tensor[];\n  if (Array.isArray(data)) {\n    if (data.length !== names.length) {\n      throw new ValueError(\n          `Error when checking model ${exceptionPrefix}: the Array of ` +\n          `Tensors that you are passing to your model is not the size the ` +\n          `the model expected. Expected to see ${names.length} Tensor(s),` +\n          ` but instead got ${data.length} Tensors(s).`);\n    }\n    arrays = data;\n  } else {\n    if (names.length > 1) {\n      throw new ValueError(\n          `The model expects ${names.length} ${exceptionPrefix} Tensors, ` +\n          `but only received one Tensor. Found: array with shape ` +\n          `${JSON.stringify(data.shape)}.`);\n    }\n    arrays = [data];\n  }\n\n  if (shapes != null) {\n    for (let i = 0; i < names.length; ++i) {\n      if (shapes[i] == null) {\n        continue;\n      }\n      const array = arrays[i];\n      if (array.shape.length !== shapes[i].length) {\n        throw new ValueError(\n            `Error when checking ${exceptionPrefix}: expected ${names[i]} ` +\n            `to have ${shapes[i].length} dimension(s), but got array with ` +\n            `shape ${JSON.stringify(array.shape)}`);\n      }\n      for (let j = 0; j < shapes[i].length; ++j) {\n        if (j === 0 && !checkBatchAxis) {\n          continue;\n        }\n        const dim = array.shape[j];\n        const refDim = shapes[i][j];\n        if (refDim != null) {\n          if (refDim !== dim) {\n            throw new ValueError(\n                `Error when checking ${exceptionPrefix}: expected ` +\n                `${names[i]} to have shape ${JSON.stringify(shapes[i])} but ` +\n                `got array with shape ${JSON.stringify(array.shape)}.`);\n          }\n        }\n      }\n    }\n  }\n}\n\n/**\n * Maps metric functions to model outputs.\n * @param metrics An shortcut strings name, metric function, `Array` or dict\n *   (`Object`) of metric functions.\n * @param outputNames An `Array` of the names of model outputs.\n * @returns An `Array` (one entry per model output) of `Array` of metric\n *   functions. For instance, if the model has 2 outputs, and for the first\n *   output we want to compute `binaryAccuracy` and `binaryCrossentropy`,\n *   and just `binaryAccuracy` for the second output, the `Array` would look\n *   like:\n *     `[[binaryAccuracy, binaryCrossentropy],  [binaryAccuracy]]`\n * @throws TypeError: incompatible metrics format.\n */\nexport function collectMetrics(\n    metrics: string|LossOrMetricFn|Array<string|LossOrMetricFn>|\n    {[outputName: string]: string | LossOrMetricFn},\n    outputNames: string[]): Array<Array<string|LossOrMetricFn>> {\n  if (metrics == null || Array.isArray(metrics) && metrics.length === 0) {\n    return outputNames.map(name => []);\n  }\n\n  let wrappedMetrics: Array<string|LossOrMetricFn>|\n      {[outputName: string]: string | LossOrMetricFn};\n  if (typeof metrics === 'string' || typeof metrics === 'function') {\n    wrappedMetrics = [metrics];\n  } else if (Array.isArray(metrics) || typeof metrics === 'object') {\n    wrappedMetrics = metrics as Array<string|LossOrMetricFn>|\n        {[outputName: string]: string} | {[outputName: string]: LossOrMetricFn};\n  } else {\n    throw new TypeError(\n        'Type of metrics argument not understood. Expected an string,' +\n        `function, Array, or Object, found: ${metrics}`);\n  }\n\n  if (Array.isArray(wrappedMetrics)) {\n    // We then apply all metrics to all outputs.\n    return outputNames.map(\n        name => wrappedMetrics as Array<string|LossOrMetricFn>);\n  } else {\n    // In this case, metrics is a dict.\n    const nestedMetrics: Array<Array<string|LossOrMetricFn>> = [];\n    for (const name of outputNames) {\n      let outputMetrics: string|LossOrMetricFn|Array<string|LossOrMetricFn> =\n          wrappedMetrics.hasOwnProperty(name) ? wrappedMetrics[name] : [];\n      if (!Array.isArray(outputMetrics)) {\n        outputMetrics = [outputMetrics];\n      }\n      nestedMetrics.push(outputMetrics);\n    }\n    return nestedMetrics;\n  }\n}\n\nexport interface ModelEvaluateArgs {\n  /**\n   * Batch size (Integer). If unspecified, it will default to 32.\n   */\n  batchSize?: number;\n\n  /**\n   * Verbosity mode.\n   */\n  verbose?: ModelLoggingVerbosity;\n\n  /**\n   * Tensor of weights to weight the contribution of different samples to the\n   * loss and metrics.\n   */\n  sampleWeight?: Tensor;\n\n  /**\n   * integer: total number of steps (batches of samples)\n   * before declaring the evaluation round finished. Ignored with the default\n   * value of `undefined`.\n   */\n  steps?: number;\n}\n\n/**\n * Configuration for calls to `LayersModel.compile()`.\n */\nexport interface ModelCompileArgs {\n  /**\n   * An instance of `tf.train.Optimizer` or a string name for an Optimizer.\n   */\n  optimizer: string|Optimizer;\n\n  /**\n   * Object function(s) or name(s) of object function(s).\n   * If the model has multiple outputs, you can use a different loss\n   * on each output by passing a dictionary or an Array of losses.\n   * The loss value that will be minimized by the model will then be the sum\n   * of all individual losses.\n   */\n  loss: string|string[]|{[outputName: string]: string}|LossOrMetricFn|\n      LossOrMetricFn[]|{[outputName: string]: LossOrMetricFn};\n\n  /**\n   * List of metrics to be evaluated by the model during training and testing.\n   * Typically you will use `metrics=['accuracy']`.\n   * To specify different metrics for different outputs of a multi-output\n   * model, you could also pass a dictionary.\n   */\n  metrics?: string|LossOrMetricFn|Array<string|LossOrMetricFn>|\n      {[outputName: string]: string | LossOrMetricFn};\n\n  // TODO(cais): Add lossWeights, sampleWeightMode, weightedMetrics, and\n  //   targetTensors.\n}\n\nconst LAYERS_MODEL_FORMAT_NAME = 'layers-model';\n\n/**\n * A `tf.LayersModel` is a directed, acyclic graph of `tf.Layer`s plus methods\n * for training, evaluation, prediction and saving.\n *\n * `tf.LayersModel` is the basic unit of training, inference and evaluation in\n * TensorFlow.js. To create a `tf.LayersModel`, use `tf.LayersModel`.\n *\n * See also:\n *   `tf.Sequential`, `tf.loadLayersModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport class LayersModel extends Container implements tfc.InferenceModel {\n  // The class name is 'Model' rather than 'LayersModel' for backwards\n  // compatibility since this class name shows up in the serialization format.\n  /** @nocollapse */\n  static className = 'Model';\n  protected optimizer_: Optimizer;\n  // Whether the model instance owns the optimizer: `true` if and only if\n  // `optimizer` is created from a string parameter during `compile()` call.\n  protected isOptimizerOwned: boolean;\n\n  loss: string|string[]|{[outputName: string]: string}|LossOrMetricFn|\n      LossOrMetricFn[]|{[outputName: string]: LossOrMetricFn};\n  lossFunctions: LossOrMetricFn[];\n\n  // TODO(cais): These private variables should probably not have the string\n  //   'feed' in their names, because we are not dealing with a symbolic\n  //   backend.\n  private feedOutputShapes: Shape[];\n  private feedLossFns: LossOrMetricFn[];\n  private collectedTrainableWeights: LayerVariable[];\n  private testFunction: (data: Tensor[]) => Scalar[];\n  history: History;\n\n  // A public property that can be set by Callbacks to order early stopping\n  // during `fit()` calls.\n  protected stopTraining_: boolean;\n  protected isTraining: boolean;\n\n  metrics: string|LossOrMetricFn|Array<string|LossOrMetricFn>|\n      {[outputName: string]: string | LossOrMetricFn};\n  metricsNames: string[];\n  // Porting Note: `metrics_tensors` in PyKeras is a symbolic tensor. But given\n  //   the imperative nature of tfjs-core, `metricsTensors` is a\n  //   TypeScript function here.\n  //   Also note that due to the imperative nature of tfjs-core, `metricsTensor`\n  //   here needs an output index to keep track of which output of the\n  //   LayersModel a metric belongs to. This is unlike `metrics_tensors` in\n  //   PyKeras, which is a `list` of symbolic tensors, each of which has\n  //   implicit \"knowledge\" of the outputs it depends on.\n  metricsTensors: Array<[LossOrMetricFn, number]>;\n\n  // User defind metadata (if any).\n  private userDefinedMetadata: {};\n\n  constructor(args: ContainerArgs) {\n    super(args);\n    this.isTraining = false;\n  }\n\n  /**\n   * Print a text summary of the model's layers.\n   *\n   * The summary includes\n   * - Name and type of all layers that comprise the model.\n   * - Output shape(s) of the layers\n   * - Number of weight parameters of each layer\n   * - If the model has non-sequential-like topology, the inputs each layer\n   *   receives\n   * - The total number of trainable and non-trainable parameters of the model.\n   *\n   * ```js\n   * const input1 = tf.input({shape: [10]});\n   * const input2 = tf.input({shape: [20]});\n   * const dense1 = tf.layers.dense({units: 4}).apply(input1);\n   * const dense2 = tf.layers.dense({units: 8}).apply(input2);\n   * const concat = tf.layers.concatenate().apply([dense1, dense2]);\n   * const output =\n   *     tf.layers.dense({units: 3, activation: 'softmax'}).apply(concat);\n   *\n   * const model = tf.model({inputs: [input1, input2], outputs: output});\n   * model.summary();\n   * ```\n   *\n   * @param lineLength Custom line length, in number of characters.\n   * @param positions Custom widths of each of the columns, as either\n   *   fractions of `lineLength` (e.g., `[0.5, 0.75, 1]`) or absolute number\n   *   of characters (e.g., `[30, 50, 65]`). Each number corresponds to\n   *   right-most (i.e., ending) position of a column.\n   * @param printFn Custom print function. Can be used to replace the default\n   *   `console.log`. For example, you can use `x => {}` to mute the printed\n   *   messages in the console.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  summary(\n      lineLength?: number, positions?: number[],\n      printFn:\n          // tslint:disable-next-line:no-any\n      (message?: any, ...optionalParams: any[]) => void = console.log) {\n    if (!this.built) {\n      throw new ValueError(\n          `This model has never been called, thus its weights have not been ` +\n          `created yet. So no summary can be displayed. Build the model ` +\n          `first (e.g., by calling it on some test data).`);\n    }\n    printSummary(this, lineLength, positions, printFn);\n  }\n\n  /**\n   * Configures and prepares the model for training and evaluation.  Compiling\n   * outfits the model with an optimizer, loss, and/or metrics.  Calling `fit`\n   * or `evaluate` on an un-compiled model will throw an error.\n   *\n   * @param args a `ModelCompileArgs` specifying the loss, optimizer, and\n   * metrics to be used for fitting and evaluating this model.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  compile(args: ModelCompileArgs): void {\n    if (args.loss == null) {\n      args.loss = [];\n    }\n    this.loss = args.loss;\n\n    if (typeof args.optimizer === 'string') {\n      this.optimizer_ = optimizers.getOptimizer(args.optimizer);\n      this.isOptimizerOwned = true;\n    } else {\n      if (!(args.optimizer instanceof Optimizer)) {\n        throw new ValueError(\n            `User-defined optimizer must be an instance of tf.Optimizer.`);\n      }\n      this.optimizer_ = args.optimizer;\n      this.isOptimizerOwned = false;\n    }\n\n    // TODO(cais): Add lossWeights.\n    // TODO(cais): Add sampleWeightMode.\n\n    // Prepare loss functions.\n    let lossFunctions: LossOrMetricFn[] = [];\n    if (!Array.isArray(args.loss) && typeof args.loss !== 'string' &&\n        typeof args.loss !== 'function') {\n      args.loss = args.loss as {[outputName: string]: string};\n      for (const name in args.loss) {\n        if (this.outputNames.indexOf(name) === -1) {\n          throw new ValueError(\n              `Unknown entry in loss dictionary: \"${name}\". ` +\n              `Only expected the following keys: ${this.outputNames}`);\n        }\n      }\n      for (const name of this.outputNames) {\n        if (args.loss[name] == null) {\n          console.warn(\n              `Output \"${name}\" is missing from loss dictionary. We assume ` +\n              `this was done on purpose, and we will not be expecting data ` +\n              `to be passed to ${name} during training`);\n        }\n        lossFunctions.push(losses.get(args.loss[name]));\n      }\n    } else if (Array.isArray(args.loss)) {\n      if (args.loss.length !== this.outputs.length) {\n        throw new ValueError(\n            `When passing an Array as loss, it should have one entry per ` +\n            `model output. The model has ${this.outputs.length} output(s), ` +\n            `but you passed loss=${args.loss}.`);\n      }\n      const theLosses = args.loss as Array<string|LossOrMetricFn>;\n      lossFunctions = theLosses.map(l => losses.get(l));\n    } else {\n      const lossFunction = losses.get(args.loss);\n      this.outputs.forEach(_ => {\n        lossFunctions.push(lossFunction);\n      });\n    }\n\n    this.lossFunctions = lossFunctions;\n\n    this.feedOutputNames = [];\n    this.feedOutputShapes = [];\n    this.feedLossFns = [];\n    for (let i = 0; i < this.outputs.length; ++i) {\n      // TODO(cais): Logic for skipping target(s).\n      const shape = this.internalOutputShapes[i];\n      const name = this.outputNames[i];\n      this.feedOutputNames.push(name);\n      this.feedOutputShapes.push(shape);\n      this.feedLossFns.push(this.lossFunctions[i]);\n    }\n\n    // TODO(cais): Add logic for output masks.\n    // TODO(cais): Add logic for sample weights.\n    const skipTargetIndices: number[] = [];\n\n    // Prepare metrics.\n    this.metrics = args.metrics;\n    // TODO(cais): Add weightedMetrics.\n    this.metricsNames = ['loss'];\n    this.metricsTensors = [];\n\n    // Compute total loss.\n    // Porting Note: In PyKeras, metrics_tensors are symbolic tensor objects.\n    //   Here, metricsTensors are TypeScript functions. This difference is due\n    //   to the difference in symbolic/imperative property of the backends.\n    nameScope('loss', () => {\n      for (let i = 0; i < this.outputs.length; ++i) {\n        if (skipTargetIndices.indexOf(i) !== -1) {\n          continue;\n        }\n        // TODO(cais): Add weightedLoss, sampleWeight and mask.\n        //   The following line should be weightedLoss\n        const weightedLoss = this.lossFunctions[i];\n        if (this.outputs.length > 1) {\n          this.metricsTensors.push([weightedLoss, i]);\n          this.metricsNames.push(this.outputNames[i] + '_loss');\n        }\n      }\n\n      // Porting Note: Due to the imperative nature of the backend, we calculate\n      //   the regularizer penalties in the totalLossFunction, instead of here.\n    });\n\n    const nestedMetrics = collectMetrics(args.metrics, this.outputNames);\n    // TODO(cais): Add nestedWeightedMetrics.\n\n    /**\n     * Helper function used in loop below.\n     */\n    const appendMetric =\n        (outputIndex: number, metricName: string,\n         metricTensor: LossOrMetricFn) => {\n          if (this.outputNames.length > 1) {\n            metricName = this.outputNames[outputIndex] + '_' + metricName;\n          }\n          this.metricsNames.push(metricName);\n          this.metricsTensors.push([metricTensor, outputIndex]);\n        };\n\n    nameScope('metric', () => {\n      for (let i = 0; i < this.outputs.length; ++i) {\n        if (skipTargetIndices.indexOf(i) !== -1) {\n          continue;\n        }\n        const outputMetrics = nestedMetrics[i];\n        // TODO(cais): Add weights and outputWeightedMetrics.\n\n        // TODO(cais): Add optional arg `weights` to the following function.\n        const handleMetrics = (metrics: Array<string|LossOrMetricFn>) => {\n          const metricNamePrefix = '';\n          let metricName: string;\n          let accFn: LossOrMetricFn;\n          let weightedMetricFn: LossOrMetricFn;\n          //  TODO(cais): Use 'weights_' for weighted metrics.\n\n          for (const metric of metrics) {\n            if (typeof metric === 'string' &&\n                ['accuracy', 'acc', 'crossentropy', 'ce'].indexOf(metric) !==\n                    -1) {\n              const outputShape = this.internalOutputShapes[i];\n\n              if (outputShape[outputShape.length - 1] === 1 ||\n                  this.lossFunctions[i] === losses.binaryCrossentropy) {\n                // case: binary accuracy/crossentropy.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.binaryAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.binaryCrossentropy;\n                }\n              } else if (\n                  this.lossFunctions[i] ===\n                  losses.sparseCategoricalCrossentropy) {\n                // case: categorical accuracy / crossentropy with sparse\n                // targets.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.sparseCategoricalAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.sparseCategoricalCrossentropy;\n                }\n              } else {\n                // case: categorical accuracy / crossentropy.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.categoricalAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.categoricalCrossentropy;\n                }\n              }\n              let suffix: string;\n              if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                suffix = 'acc';\n              } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                suffix = 'ce';\n              }\n              // TODO(cais): Add weighting actually.\n              weightedMetricFn = accFn;\n              metricName = metricNamePrefix + suffix;\n            } else {\n              const metricFn = Metrics.get(metric);\n              // TODO(cais): Add weighting actually.\n              weightedMetricFn = metricFn;\n              metricName =\n                  metricNamePrefix + Metrics.getLossOrMetricName(metric);\n            }\n\n            // TODO(cais): Add weighting and masking to metricResult.\n            let metricResult: LossOrMetricFn;\n            nameScope(metricName, () => {\n              metricResult = weightedMetricFn;\n            });\n            appendMetric(i, metricName, metricResult);\n          }\n        };\n\n        handleMetrics(outputMetrics);\n        // TODO(cais): Call handleMetrics with weights.\n      }\n    });\n\n    // Porting Notes: Given the imperative backend of tfjs-core,\n    //   there is no need for constructing the symbolic graph and placeholders.\n    this.collectedTrainableWeights = this.trainableWeights;\n  }\n\n  /**\n   * Check trainable weights count consistency.\n   *\n   * This will raise a warning if `this.trainableWeights` and\n   * `this.collectedTrainableWeights` are inconsistent (i.e., have different\n   * numbers of parameters).\n   * Inconsistency will typically arise when one modifies `model.trainable`\n   * without calling `model.compile()` again.\n   */\n  protected checkTrainableWeightsConsistency(): void {\n    if (this.collectedTrainableWeights == null) {\n      return;\n    }\n    if (this.trainableWeights.length !==\n        this.collectedTrainableWeights.length) {\n      console.warn(\n          'Discrepancy between trainableweights and collected trainable ' +\n          'weights. Did you set `model.trainable` without calling ' +\n          '`model.compile()` afterwards?');\n    }\n  }\n\n  /**\n   * Returns the loss value & metrics values for the model in test mode.\n   *\n   * Loss and metrics are specified during `compile()`, which needs to happen\n   * before calls to `evaluate()`.\n   *\n   * Computation is done in batches.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * const result = model.evaluate(\n   *     tf.ones([8, 10]), tf.ones([8, 1]), {batchSize: 4});\n   * result.print();\n   * ```\n   *\n   * @param x `tf.Tensor` of test data, or an `Array` of `tf.Tensor`s if the\n   * model has multiple inputs.\n   * @param y `tf.Tensor` of target data, or an `Array` of `tf.Tensor`s if the\n   * model has multiple outputs.\n   * @param args A `ModelEvaluateArgs`, containing optional fields.\n   *\n   * @return `Scalar` test loss (if the model has a single output and no\n   *   metrics) or `Array` of `Scalar`s (if the model has multiple outputs\n   *   and/or metrics). The attribute `model.metricsNames`\n   *   will give you the display labels for the scalar outputs.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  evaluate(\n      x: Tensor|Tensor[], y: Tensor|Tensor[],\n      args: ModelEvaluateArgs = {}): Scalar|Scalar[] {\n    const batchSize = args.batchSize == null ? 32 : args.batchSize;\n    checkBatchSize(batchSize);\n\n    // TODO(cais): Standardize `config.sampleWeights` as well.\n    // Validate user data.\n    const checkBatchAxis = true;\n    const standardizedOuts =\n        this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n    try {\n      // TODO(cais): If uses `useLearningPhase`, set the corresponding element\n      // of the input to 0.\n      const ins = standardizedOuts[0].concat(standardizedOuts[1]);\n      this.makeTestFunction();\n      const f = this.testFunction;\n      const testOuts =\n          this.testLoop(f, ins, batchSize, args.verbose, args.steps);\n      return singletonOrArray(testOuts);\n    } finally {\n      disposeNewTensors(standardizedOuts[0], x);\n      disposeNewTensors(standardizedOuts[1], y);\n    }\n  }\n\n  // TODO(cais): Add code snippet below once real dataset objects are\n  //   available.\n  /**\n   * Evaluate model using a dataset object.\n   *\n   * Note: Unlike `evaluate()`, this method is asynchronous (`async`).\n   *\n   * @param dataset A dataset object. Its `iterator()` method is expected\n   *   to generate a dataset iterator object, the `next()` method of which\n   *   is expected to produce data batches for evaluation. The return value\n   *   of the `next()` call ought to contain a boolean `done` field and a\n   *   `value` field. The `value` field is expected to be an array of two\n   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n   *   case is for models with exactly one input and one output (e.g.\n   *   a sequential model). The latter case is for models with multiple\n   *   inputs and/or multiple outputs. Of the two items in the array, the\n   *   first is the input feature(s) and the second is the output target(s).\n   * @param args A configuration object for the dataset-based evaluation.\n   * @returns Loss and metric values as an Array of `Scalar` objects.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async evaluateDataset(dataset: Dataset<{}>, args?: ModelEvaluateDatasetArgs):\n      Promise<Scalar|Scalar[]> {\n    this.makeTestFunction();\n    return evaluateDataset(this, dataset, args);\n  }\n\n  /**\n   * Get number of samples provided for training, evaluation or prediction.\n   *\n   * @param ins Input `tf.Tensor`.\n   * @param batchSize Integer batch size, optional.\n   * @param steps Total number of steps (batches of samples) before\n   * declaring loop finished. Optional.\n   * @param stepsName The public API's parameter name for `steps`.\n   * @returns Number of samples provided.\n   */\n  private checkNumSamples(\n      ins: Tensor|Tensor[], batchSize?: number, steps?: number,\n      stepsName = 'steps'): number {\n    let numSamples: number;\n    if (steps != null) {\n      numSamples = null;\n      if (batchSize != null) {\n        throw new ValueError(\n            `If ${stepsName} is set, batchSize must be null or undefined.` +\n            `Got batchSize = ${batchSize}`);\n      }\n    } else if (ins != null) {\n      if (Array.isArray(ins)) {\n        numSamples = ins[0].shape[0];\n      } else {\n        numSamples = ins.shape[0];\n      }\n    } else {\n      throw new ValueError(\n          `Either the input data should have a defined shape, or ` +\n          `${stepsName} shoud be specified.`);\n    }\n    return numSamples;\n  }\n\n  /**\n   * Execute internal tensors of the model with input data feed.\n   * @param inputs Input data feed. Must match the inputs of the model.\n   * @param outputs Names of the output tensors to be fetched. Must match\n   *   names of the SymbolicTensors that belong to the graph.\n   * @returns Fetched values for `outputs`.\n   */\n  execute(inputs: Tensor|Tensor[]|NamedTensorMap, outputs: string|string[]):\n      Tensor|Tensor[] {\n    if (Array.isArray(outputs) && outputs.length === 0) {\n      throw new ValueError(\n          '`outputs` is an empty Array, which is not allowed.');\n    }\n\n    const outputsIsArray = Array.isArray(outputs);\n    const outputNames =\n        (outputsIsArray ? outputs : [outputs]);\n    const outputSymbolicTensors = this.retrieveSymbolicTensors(outputNames);\n\n    // Format the input into a FeedDict.\n    const feedDict = new FeedDict();\n    if (inputs instanceof Tensor) {\n      inputs = [inputs];\n    }\n    if (Array.isArray(inputs)) {\n      if (inputs.length !== this.inputs.length) {\n        throw new ValueError(\n            `The number of inputs provided (${inputs.length}) ` +\n            `does not match the number of inputs of this model ` +\n            `(${this.inputs.length}).`);\n      }\n      for (let i = 0; i < this.inputs.length; ++i) {\n        feedDict.add(this.inputs[i], inputs[i]);\n      }\n    } else {\n      for (const input of this.inputs) {\n        const tensorValue = inputs[input.name];\n        if (tensorValue == null) {\n          throw new ValueError(\n              `No value is provided for the model's input ${input.name}`);\n        }\n        feedDict.add(input, tensorValue);\n      }\n    }\n\n    // Run execution.\n    const executeOutputs = execute(outputSymbolicTensors, feedDict) as Tensor[];\n    return outputsIsArray ? executeOutputs : executeOutputs[0];\n  }\n\n  /**\n   * Retrieve the model's internal symbolic tensors from symbolic-tensor names.\n   */\n  private retrieveSymbolicTensors(symbolicTensorNames: string[]):\n      SymbolicTensor[] {\n    const outputSymbolicTensors: SymbolicTensor[] =\n        pyListRepeat(null, symbolicTensorNames.length);\n    let outputsRemaining = symbolicTensorNames.length;\n    for (const layer of this.layers) {\n      const layerOutputs: SymbolicTensor[] =\n          Array.isArray(layer.output) ? layer.output : [layer.output];\n      const layerOutputNames = layerOutputs.map(output => output.name);\n      for (let i = 0; i < symbolicTensorNames.length; ++i) {\n        const index = layerOutputNames.indexOf(symbolicTensorNames[i]);\n        if (index !== -1) {\n          outputSymbolicTensors[i] = layerOutputs[index];\n          outputsRemaining--;\n        }\n        if (outputsRemaining === 0) {\n          break;\n        }\n      }\n      if (outputsRemaining === 0) {\n        break;\n      }\n    }\n\n    if (outputsRemaining > 0) {\n      const remainingNames: string[] = [];\n      outputSymbolicTensors.forEach((tensor, i) => {\n        if (tensor == null) {\n          remainingNames.push(symbolicTensorNames[i]);\n        }\n      });\n      throw new ValueError(\n          `Cannot find SymbolicTensors for output name(s): ` +\n          `${JSON.stringify(remainingNames)}`);\n    }\n    return outputSymbolicTensors;\n  }\n\n  /**\n   * Helper method to loop over some data in batches.\n   *\n   * Porting Note: Not using the functional approach in the Python equivalent\n   *   due to the imperative backend.\n   * Porting Note: Does not support step mode currently.\n   *\n   * @param ins: input data\n   * @param batchSize: integer batch size.\n   * @param verbose: verbosity model\n   * @returns: Predictions as `tf.Tensor` (if a single output) or an `Array` of\n   *   `tf.Tensor` (if multipe outputs).\n   */\n  private predictLoop(ins: Tensor|Tensor[], batchSize = 32, verbose = false):\n      Tensor|Tensor[] {\n    return tfc.tidy(() => {\n      const numSamples = this.checkNumSamples(ins);\n      if (verbose) {\n        throw new NotImplementedError(\n            'Verbose predictLoop() is not implemented yet.');\n      }\n\n      // Sample-based predictions.\n      // Porting Note: Tensor currently does not support sliced assignments as\n      //   in numpy, e.g., x[1:3] = y. Therefore we use concatenation while\n      //   iterating over the batches.\n\n      const batches = makeBatches(numSamples, batchSize);\n      const outsBatches: Tensor[][] = this.outputs.map(output => []);\n\n      // TODO(cais): Can the scope() be pushed down inside the for loop?\n      for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n        const batchOuts = tfc.tidy(() => {\n          const batchStart = batches[batchIndex][0];\n          const batchEnd = batches[batchIndex][1];\n          // TODO(cais): Take care of the case of the last element is a flag for\n          //   training/test.\n          const insBatch = sliceArrays(ins, batchStart, batchEnd);\n\n          // Construct the feeds for execute();\n          const feeds = [];\n          if (Array.isArray(insBatch)) {\n            for (let i = 0; i < insBatch.length; ++i) {\n              feeds.push({key: this.inputs[i], value: insBatch[i]});\n            }\n          } else {\n            feeds.push({key: this.inputs[0], value: insBatch});\n          }\n          const feedDict = new FeedDict(feeds);\n          return execute(this.outputs, feedDict) as Tensor[];\n        });\n        batchOuts.forEach((batchOut, i) => outsBatches[i].push(batchOut));\n      }\n      return singletonOrArray(\n          outsBatches.map(batches => tfc.concat(batches, 0)));\n    });\n  }\n\n  /**\n   * Generates output predictions for the input samples.\n   *\n   * Computation is done in batches.\n   *\n   * Note: the \"step\" mode of predict() is currently not supported.\n   *   This is because the TensorFlow.js core backend is imperative only.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.predict(tf.ones([8, 10]), {batchSize: 4}).print();\n   * ```\n   *\n   * @param x The input data, as a Tensor, or an `Array` of `tf.Tensor`s if\n   *   the model has multiple inputs.\n   * @param args A `ModelPredictArgs` object containing optional fields.\n   *\n   * @return Prediction results as a `tf.Tensor`(s).\n   *\n   * @exception ValueError In case of mismatch between the provided input data\n   *   and the model's expectations, or in case a stateful model receives a\n   *   number of samples that is not a multiple of the batch size.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  predict(x: Tensor|Tensor[], args: ModelPredictArgs = {}): Tensor|Tensor[] {\n    const xsRank2OrHigher = ensureTensorsRank2OrHigher(x);\n    checkInputData(\n        xsRank2OrHigher, this.inputNames, this.feedInputShapes, false);\n    try {\n      // TODO(cais): Take care of stateful models.\n      //   if (this.stateful) ...\n      // TODO(cais): Take care of the learning_phase boolean flag.\n      //   if (this.useLearningPhase) ...\n      const batchSize = args.batchSize == null ? 32 : args.batchSize;\n      checkBatchSize(batchSize);\n      return this.predictLoop(xsRank2OrHigher, batchSize);\n    } finally {\n      disposeNewTensors(xsRank2OrHigher, x);\n    }\n  }\n\n  /**\n   * Returns predictions for a single batch of samples.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.predictOnBatch(tf.ones([8, 10])).print();\n   * ```\n   * @param x: Input samples, as a Tensor (for models with exactly one\n   *   input) or an array of Tensors (for models with more than one input).\n   * @return Tensor(s) of predictions\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  predictOnBatch(x: Tensor|Tensor[]): Tensor|Tensor[] {\n    checkInputData(x, this.inputNames, this.feedInputShapes, true);\n    // TODO(cais): Take care of the learning_phase boolean flag.\n    //   if (this.useLearningPhase) ...\n    const batchSize = (Array.isArray(x) ? x[0] : x).shape[0];\n    return this.predictLoop(x, batchSize);\n  }\n\n  protected standardizeUserDataXY(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|{[inputName: string]: Tensor}, checkBatchAxis = true,\n      batchSize?: number): [Tensor[], Tensor[]] {\n    // TODO(cais): Add sampleWeight, classWeight\n    if (this.optimizer_ == null) {\n      throw new RuntimeError(\n          'You must compile a model before training/testing. Use ' +\n          'LayersModel.compile(modelCompileArgs).');\n    }\n    const outputShapes: Shape[] = [];\n    for (let i = 0; i < this.feedOutputShapes.length; ++i) {\n      const outputShape = this.feedOutputShapes[i];\n      const lossFn = this.feedLossFns[i];\n      if (lossFn === losses.sparseCategoricalCrossentropy) {\n        outputShapes.push(\n            outputShape.slice(0, outputShape.length - 1).concat([1]));\n      } else {\n        // Porting Note: Because of strong typing `lossFn` must be a function.\n        outputShapes.push(outputShape);\n      }\n    }\n    x = standardizeInputData(\n        x, this.feedInputNames, this.feedInputShapes, false, 'input');\n    y = standardizeInputData(\n        y, this.feedOutputNames, outputShapes, false, 'target');\n    // TODO(cais): Standardize sampleWeights & classWeights.\n    checkArrayLengths(x, y, null);\n    // TODO(cais): Check sampleWeights as well.\n    checkLossAndTargetCompatibility(y, this.feedLossFns, this.feedOutputShapes);\n    if (this.stateful && batchSize != null && batchSize > 0) {\n      if (x[0].shape[0] % batchSize !== 0) {\n        throw new ValueError(\n            `In a stateful network, you should only pass inputs with a ` +\n            `number of samples that is divisible by the batch size ` +\n            `${batchSize}. Found: ${x[0].shape[0]} sample(s).`);\n      }\n    }\n    return [x, y];\n  }\n\n  protected async standardizeUserData(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      sampleWeight?: Tensor|Tensor[]|{[outputName: string]: Tensor},\n      classWeight?: ClassWeight|ClassWeight[]|ClassWeightMap,\n      checkBatchAxis = true,\n      batchSize?: number): Promise<[Tensor[], Tensor[], Tensor[]]> {\n    const [standardXs, standardYs] =\n        this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n    // TODO(cais): Handle sampleWeights.\n    if (sampleWeight != null) {\n      throw new Error('sample weight is not supported yet.');\n    }\n\n    let standardSampleWeights: Tensor[] = null;\n    if (classWeight != null) {\n      const classWeights =\n          standardizeClassWeights(classWeight, this.outputNames);\n      standardSampleWeights = [];\n      for (let i = 0; i < classWeights.length; ++i) {\n        standardSampleWeights.push(\n            await standardizeWeights(standardYs[i], null, classWeights[i]));\n      }\n    }\n\n    // TODO(cais): Deal with the case of model.stateful == true.\n    return [standardXs, standardYs, standardSampleWeights];\n  }\n\n  /**\n   * Loop over some test data in batches.\n   * @param f A Function returning a list of tensors.\n   * @param ins Array of tensors to be fed to `f`.\n   * @param batchSize Integer batch size or `null` / `undefined`.\n   * @param verbose verbosity mode.\n   * @param steps Total number of steps (batches of samples) before\n   * declaring test finished. Ignored with the default value of `null` /\n   * `undefined`.\n   * @returns Array of Scalars.\n   */\n  private testLoop(\n      f: (data: Tensor[]) => Scalar[], ins: Tensor[], batchSize?: number,\n      verbose = 0, steps?: number): Scalar[] {\n    return tfc.tidy(() => {\n      const numSamples = this.checkNumSamples(ins, batchSize, steps, 'steps');\n      const outs: Scalar[] = [];\n      if (verbose > 0) {\n        throw new NotImplementedError('Verbose mode is not implemented yet.');\n      }\n      // TODO(cais): Use `indicesForConversionToDense' to prevent slow down.\n      if (steps != null) {\n        throw new NotImplementedError(\n            'steps mode in testLoop() is not implemented yet');\n      } else {\n        const batches = makeBatches(numSamples, batchSize);\n        const indexArray = tensor1d(range(0, numSamples));\n        for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n          const batchStart = batches[batchIndex][0];\n          const batchEnd = batches[batchIndex][1];\n          const batchIds =\n              K.sliceAlongFirstAxis(\n                  indexArray, batchStart, batchEnd - batchStart) as Tensor1D;\n          // TODO(cais): In ins, train flag can be a number, instead of an\n          //   Tensor? Do we need to handle this in tfjs-layers?\n          const insBatch = sliceArraysByIndices(ins, batchIds) as Scalar[];\n          const batchOuts = f(insBatch);\n          if (batchIndex === 0) {\n            for (let i = 0; i < batchOuts.length; ++i) {\n              outs.push(scalar(0));\n            }\n          }\n          for (let i = 0; i < batchOuts.length; ++i) {\n            const batchOut = batchOuts[i];\n            outs[i] =\n                tfc.add(outs[i], tfc.mul(batchEnd - batchStart, batchOut));\n          }\n        }\n        for (let i = 0; i < outs.length; ++i) {\n          outs[i] = tfc.div(outs[i], numSamples);\n        }\n      }\n      return outs;\n    });\n  }\n\n  protected getDedupedMetricsNames(): string[] {\n    const outLabels = this.metricsNames;\n    // Rename duplicated metrics names (can happen with an output layer\n    // shared among multiple dataflows).\n    const dedupedOutLabels = [];\n    for (let i = 0; i < outLabels.length; ++i) {\n      const label = outLabels[i];\n      let newLabel = label;\n      if (count(outLabels, label) > 1) {\n        const dupIndex = count(outLabels.slice(0, i), label);\n        newLabel += `_${dupIndex}`;\n      }\n      dedupedOutLabels.push(newLabel);\n    }\n    return dedupedOutLabels;\n  }\n\n  /**\n   * Creates a function that performs the following actions:\n   *\n   * 1. computes the losses\n   * 2. sums them to get the total loss\n   * 3. call the optimizer computes the gradients of the LayersModel's\n   *    trainable weights w.r.t. the total loss and update the variables\n   * 4. calculates the metrics\n   * 5. returns the values of the losses and metrics.\n   */\n  protected makeTrainFunction(): (data: Tensor[]) => Scalar[] {\n    return (data: Tensor[]) => {\n      const lossValues: Scalar[] = [];\n\n      const inputs = data.slice(0, this.inputs.length);\n      const targets = data.slice(\n          this.inputs.length, this.inputs.length + this.outputs.length);\n      const sampleWeights = data.slice(\n          this.inputs.length + this.outputs.length,\n          this.inputs.length + this.outputs.length * 2);\n\n      const metricsValues: Scalar[] = [];\n\n      // Create a function that computes the total loss based on the\n      // inputs. This function is used for obtaining gradients through\n      // backprop.\n      const totalLossFunction = () => {\n        const feeds = [];\n        for (let i = 0; i < this.inputs.length; ++i) {\n          feeds.push({key: this.inputs[i], value: inputs[i]});\n        }\n        const feedDict = new FeedDict(feeds);\n        const outputs =\n            execute(this.outputs, feedDict, {'training': true}) as Tensor[];\n        // TODO(cais): Take care of the case of multiple outputs from a\n        //   single layer?\n\n        let totalLoss: Tensor;\n        for (let i = 0; i < this.lossFunctions.length; ++i) {\n          const lossFunction = this.lossFunctions[i];\n          let loss = lossFunction(targets[i], outputs[i]);\n          if (sampleWeights[i] != null) {\n            loss = computeWeightedLoss(loss, sampleWeights[i]);\n          }\n\n          // TODO(cais): push Scalar instead.\n          const meanLoss: Scalar = tfc.mean(loss);\n          // TODO(cais): Use a scope() instead, to avoid ownership.\n          lossValues.push(meanLoss);\n          if (i === 0) {\n            totalLoss = loss;\n          } else {\n            totalLoss = tfc.add(totalLoss, loss);\n          }\n        }\n\n        // Compute the metrics.\n        // TODO(cais): These should probably be calculated outside\n        //   totalLossFunction to benefit speed?\n        for (let i = 0; i < this.metricsTensors.length; ++i) {\n          let weightedMetric: Scalar;\n\n          if (this.outputs.length > 1 && i < this.outputs.length) {\n            weightedMetric = lossValues[i];\n          } else {\n            const metric = this.metricsTensors[i][0];\n            const outputIndex = this.metricsTensors[i][1];\n            weightedMetric =\n                tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n          }\n\n          tfc.keep(weightedMetric);\n          // TODO(cais): Use a scope() instead, to avoid ownership.\n          metricsValues.push(weightedMetric);\n        }\n\n        totalLoss = tfc.mean(totalLoss);\n\n        // Add regularizer penalties.\n        this.calculateLosses().forEach(regularizerLoss => {\n          totalLoss = tfc.add(totalLoss, regularizerLoss);\n        });\n\n        return totalLoss as Scalar;\n      };\n\n      const variables = this.collectedTrainableWeights.map(\n          param => param.read() as tfc.Variable);\n      const returnCost = true;\n      const totalLossValue =\n          this.optimizer_.minimize(totalLossFunction, returnCost, variables);\n\n      return [totalLossValue].concat(metricsValues);\n    };\n  }\n\n  /**\n   * Create a function which, when invoked with an array of `tf.Tensor`s as a\n   * batch of inputs, returns the prespecified loss and metrics of the model\n   * under the batch of input data.\n   */\n  private makeTestFunction() {\n    this.testFunction = (data: Tensor[]) => {\n      return tfc.tidy(() => {\n        const valOutputs: Scalar[] = [];\n        let totalLoss: Scalar;\n        const inputs = data.slice(0, this.inputs.length);\n        const targets = data.slice(\n            this.inputs.length, this.inputs.length + this.outputs.length);\n        const feeds = [];\n        for (let i = 0; i < this.inputs.length; ++i) {\n          feeds.push({key: this.inputs[i], value: inputs[i]});\n        }\n        const feedDict = new FeedDict(feeds);\n        const outputs = execute(this.outputs, feedDict) as Tensor[];\n        // Compute total loss.\n        for (let i = 0; i < this.lossFunctions.length; ++i) {\n          const lossFunction = this.lossFunctions[i];\n          // TODO(cais): Add sample weighting and replace the simple\n          // averaging.\n          const loss: Scalar = tfc.mean(lossFunction(targets[i], outputs[i]));\n          if (i === 0) {\n            totalLoss = loss;\n          } else {\n            totalLoss = tfc.add(totalLoss, loss);\n          }\n          valOutputs.push(totalLoss);\n        }\n        // Compute the metrics.\n        for (let i = 0; i < this.metricsTensors.length; ++i) {\n          const metric = this.metricsTensors[i][0];\n          const outputIndex = this.metricsTensors[i][1];\n          // TODO(cais): Replace K.mean() with a proper weighting function.\n          const meanMetric =\n              tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n          valOutputs.push(meanMetric as Scalar);\n        }\n        return valOutputs;\n      });\n    };\n  }\n\n  /**\n   * Trains the model for a fixed number of epochs (iterations on a\n   * dataset).\n   *\n   * ```js\n   * const model = tf.sequential({\n   *     layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * for (let i = 1; i < 5 ; ++i) {\n   *   const h = await model.fit(tf.ones([8, 10]), tf.ones([8, 1]), {\n   *       batchSize: 4,\n   *       epochs: 3\n   *   });\n   *   console.log(\"Loss after Epoch \" + i + \" : \" + h.history.loss[0]);\n   * }\n   * ```\n   *\n   * @param x `tf.Tensor` of training data, or an array of `tf.Tensor`s if the\n   * model has multiple inputs. If all inputs in the model are named, you\n   * can also pass a dictionary mapping input names to `tf.Tensor`s.\n   * @param y `tf.Tensor` of target (label) data, or an array of `tf.Tensor`s if\n   * the model has multiple outputs. If all outputs in the model are named,\n   * you can also pass a dictionary mapping output names to `tf.Tensor`s.\n   * @param args A `ModelFitArgs`, containing optional fields.\n   *\n   * @return A `History` instance. Its `history` attribute contains all\n   *   information collected during training.\n   *\n   * @exception ValueError In case of mismatch between the provided input\n   * data and what the model expects.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async fit(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      args: ModelFitArgs = {}): Promise<History> {\n    if (this.isTraining) {\n      throw new Error(\n          'Cannot start training because another fit() call is ongoing.');\n    }\n    this.isTraining = true;\n    let inputs: Tensor[];\n    let targets: Tensor[];\n    let originalInputs: Tensor[];\n    let originalTargets: Tensor[];\n    let inputValX: Tensor|Tensor[];\n    let inputValY: Tensor|Tensor[];\n    let valX: Tensor|Tensor[];\n    let valY: Tensor|Tensor[];\n    let sampleWeights: Tensor[];\n    try {\n      const batchSize = args.batchSize == null ? 32 : args.batchSize;\n      checkBatchSize(batchSize);\n\n      // Validate user data.\n      // TODO(cais): Support sampleWeight.\n      const checkBatchAxis = false;\n      const standardizedOuts =\n          await this.standardizeUserData(\n              x, y, args.sampleWeight, args.classWeight, checkBatchAxis,\n              batchSize) as [Tensor[], Tensor[], Tensor[]];\n      inputs = standardizedOuts[0];\n      targets = standardizedOuts[1];\n      sampleWeights = standardizedOuts[2];\n\n      // Prepare validation data.\n      let doValidation = false;\n      let valIns: Tensor[];\n      if (args.validationData != null && args.validationData.length > 0) {\n        doValidation = true;\n        if (args.validationData.length === 2) {\n          // config.validationData consists of valX and valY.\n          inputValX = args.validationData[0];\n          inputValY = args.validationData[1];\n        } else if (args.validationData.length === 3) {\n          throw new NotImplementedError(\n              'validationData including sample weights is not supported yet.');\n        } else {\n          throw new ValueError(\n              `When passing validation data, it must contain 2 (valX, valY) ` +\n              `or 3 (valX, valY, valSampleWeight) items; ` +\n              `${args.validationData} is invalid.`);\n        }\n\n        const checkBatchAxis = true;\n        const valStandardized =\n            await this.standardizeUserData(\n                inputValX, inputValY, null, /** Unused sample weights. */\n                null,                       /** Unused class weights. */\n                checkBatchAxis, batchSize) as [Tensor[], Tensor[], Tensor[]];\n        valX = valStandardized[0];\n        valY = valStandardized[1];\n        valIns = valX.concat(valY);\n        // TODO(cais): Add useLearningPhase data properly.\n      } else if (\n          args.validationSplit != null && args.validationSplit > 0 &&\n          args.validationSplit < 1) {\n        doValidation = true;\n        // Porting Note: In tfjs-layers, inputs[0] is always a Tensor.\n        const splitAt =\n            Math.floor(inputs[0].shape[0] * (1 - args.validationSplit));\n        const originalBatchSize = inputs[0].shape[0];\n        valX = sliceArrays(inputs, splitAt, originalBatchSize) as Tensor[];\n        originalInputs = inputs;\n        inputs = sliceArrays(inputs, 0, splitAt) as Tensor[];\n        valY = sliceArrays(targets, splitAt, originalBatchSize) as Tensor[];\n        originalTargets = targets;\n        targets = sliceArrays(targets, 0, splitAt) as Tensor[];\n        // TODO(cais): Once sampleWeights becomes available, slice it to get\n        //   valSampleWeights.\n        valIns = valX.concat(valY);\n\n        // TODO(cais): Add useLearningPhase data properly.\n      } else if (args.validationSteps != null) {\n        doValidation = true;\n        // TODO(cais): Add useLearningPhase.\n      }\n\n      const ins = inputs.concat(targets).concat(sampleWeights);\n\n      this.checkTrainableWeightsConsistency();\n\n      // TODO(cais): Handle use_learning_phase and learning_phase?\n\n      // Porting Note: Here we see a key deviation of tfjs-layers from\n      // Keras.\n      //  Due to the imperative nature of tfjs-layers' backend (tfjs-core),\n      //  we do not construct symbolic computation graphs to embody the\n      //  training process. Instead, we define a function that performs the\n      //  training action. In PyKeras, the data (inputs and targets) are fed\n      //  through graph placeholders. In tfjs-layers, the data are fed as\n      //  function arguments. Since the function are defined below in the\n      //  scope, we don't have equivalents of PyKeras's\n      //  `_make_train_funciton`.\n      const trainFunction = this.makeTrainFunction();\n      const outLabels = this.getDedupedMetricsNames();\n\n      let valFunction: (data: Tensor[]) => Scalar[];\n      let callbackMetrics: string[];\n      if (doValidation) {\n        this.makeTestFunction();\n        valFunction = this.testFunction;\n        callbackMetrics =\n            outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n      } else {\n        valFunction = null;\n        valIns = [];\n        callbackMetrics = outLabels.slice();\n      }\n\n      const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n      const out = await this.fitLoop(\n          trainFunction, ins, outLabels, batchSize, args.epochs,\n          args.verbose, callbacks, valFunction, valIns, args.shuffle,\n          callbackMetrics, args.initialEpoch, null, null);\n      return out;\n    } finally {\n      this.isTraining = false;\n      // Memory clean up.\n      disposeNewTensors(inputs, x);\n      disposeNewTensors(targets, y);\n      disposeNewTensors(originalInputs, x);\n      disposeNewTensors(originalTargets, y);\n      disposeNewTensors(valX as Tensor[], inputValX);\n      disposeNewTensors(valY as Tensor[], inputValY);\n      if (sampleWeights != null) {\n        tfc.dispose(sampleWeights);\n      }\n    }\n    // TODO(cais): Add value to outLabels.\n  }\n\n  /**\n   * Abstract fit function for `f(ins)`.\n   * @param f A Function returning a list of tensors. For training, this\n   *   function is expected to perform the updates to the variables.\n   * @param ins List of tensors to be fed to `f`.\n   * @param outLabels List of strings, display names of the outputs of `f`.\n   * @param batchSize Integer batch size or `== null` if unknown. Default : 32.\n   * @param epochs Number of times to iterate over the data. Default : 1.\n   * @param verbose Verbosity mode: 0, 1, or 2. Default: 1.\n   * @param callbacks List of callbacks to be called during training.\n   * @param valF Function to call for validation.\n   * @param valIns List of tensors to be fed to `valF`.\n   * @param shuffle Whether to shuffle the data at the beginning of every\n   * epoch. Default : true.\n   * @param callbackMetrics List of strings, the display names of the metrics\n   *   passed to the callbacks. They should be the concatenation of the\n   *   display names of the outputs of `f` and the list of display names\n   *   of the outputs of `valF`.\n   * @param initialEpoch Epoch at which to start training (useful for\n   *   resuming a previous training run). Default : 0.\n   * @param stepsPerEpoch Total number of steps (batches on samples) before\n   *   declaring one epoch finished and starting the next epoch. Ignored with\n   *   the default value of `undefined` or `null`.\n   * @param validationSteps Number of steps to run validation for (only if\n   *   doing validation from data tensors). Not applicable for tfjs-layers.\n   * @returns A `History` object.\n   */\n  async fitLoop(\n      f: (data: Tensor[]) => Scalar[], ins: Tensor[], outLabels?:\n      string[], batchSize?: number, epochs?: number, verbose?: number,\n      callbacks?: BaseCallback[], valF?: (data: Tensor[]) => Scalar[], valIns?:\n      Tensor[], shuffle?: boolean|string, callbackMetrics?: string[],\n      initialEpoch?: number, stepsPerEpoch?: number, validationSteps?: number):\n      Promise<History> {\n    if (batchSize == null) {\n      batchSize = 32;\n    }\n    if (epochs == null) {\n      epochs = 1;\n    }\n    if (shuffle == null) {\n      shuffle = true;\n    }\n    if (initialEpoch == null) {\n      initialEpoch = 0;\n    }\n\n    // TODO(cais): Change const to let below when implementing validation.\n    let doValidation = false;\n    if (valF != null && valIns != null) {\n      doValidation = true;\n      // TODO(cais): verbose message.\n    }\n    if (validationSteps != null) {\n      doValidation = true;\n      if (stepsPerEpoch == null) {\n        throw new ValueError(\n            'Can only use `validationSteps` when doing step-wise training, ' +\n            'i.e., `stepsPerEpoch` must be set.');\n      }\n    }\n\n    const numTrainSamples =\n        this.checkNumSamples(ins, batchSize, stepsPerEpoch, 'steps_per_epoch');\n    let indexArray: number[];\n    if (numTrainSamples != null) {\n      indexArray = range(0, numTrainSamples);\n    }\n\n    if (verbose == null) {\n      verbose = 1;\n    }\n\n    const {callbackList, history} = configureCallbacks(\n        callbacks, verbose, epochs, initialEpoch, numTrainSamples,\n        stepsPerEpoch, batchSize, doValidation, callbackMetrics);\n    callbackList.setModel(this);\n    this.history = history;\n    await callbackList.onTrainBegin();\n    this.stopTraining_ = false;\n    // TODO(cais): Take care of callbacks.validation_data as in PyKeras.\n    // TODO(cais): Pre-convert feeds for performance as in PyKeras.\n\n    for (let epoch = initialEpoch; epoch < epochs; ++epoch) {\n      await callbackList.onEpochBegin(epoch);\n      const epochLogs: UnresolvedLogs = {};\n      if (stepsPerEpoch != null) {\n        throw new NotImplementedError(\n            'stepsPerEpoch mode is not implemented yet.');\n      } else {\n        if (shuffle === 'batch') {\n          throw new NotImplementedError('batch shuffling is not implemneted'\n                                        + ' yet');\n        } else if (shuffle) {\n          util.shuffle(indexArray);\n        }\n        // Convert the potentially shuffled indices to Tensor1D, to avoid the\n        // cost of repeated creation of Array1Ds later on.\n        const epochIndexArray1D = tensor1d(indexArray);\n\n        const batches = makeBatches(numTrainSamples, batchSize);\n        for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n          const batchLogs: UnresolvedLogs = {};\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n\n          tfc.tidy(() => {\n            const batchStart = batches[batchIndex][0];\n            const batchEnd = batches[batchIndex][1];\n            const batchIds = K.sliceAlongFirstAxis(\n                                 epochIndexArray1D, batchStart,\n                                 batchEnd - batchStart) as Tensor1D;\n            batchLogs['batch'] = batchIndex;\n            batchLogs['size'] = batchEnd - batchStart;\n\n            // TODO(cais): In ins, train flag can be a number, instead of an\n            //   Tensor? Do we need to handle this in tfjs-layers?\n            const insBatch = sliceArraysByIndices(ins, batchIds) as Tensor[];\n            const outs = f(insBatch);\n            for (let i = 0; i < outLabels.length; ++i) {\n              const label = outLabels[i];\n              const out = outs[i];\n              batchLogs[label] = out;\n              tfc.keep(out);\n              // TODO(cais): Use scope() to avoid ownership.\n            }\n\n            if (batchIndex === batches.length - 1) {  // Last batch.\n              if (doValidation) {\n                const valOuts = this.testLoop(valF, valIns, batchSize);\n                // Porting Notes: In tfjs-layers, valOuts is always an Array.\n                for (let i = 0; i < outLabels.length; ++i) {\n                  const label = outLabels[i];\n                  const out = valOuts[i];\n                  tfc.keep(out);\n                  // TODO(cais): Use scope() to avoid ownership.\n                  epochLogs['val_' + label] = out;\n                }\n              }\n            }\n          });\n\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n\n          if (this.stopTraining_) {\n            break;\n          }\n          // TODO(cais): return outs as list of Tensor.\n        }\n\n        epochIndexArray1D.dispose();\n      }\n      // TODO(cais): Run validation at the end of the epoch.\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      if (this.stopTraining_) {\n        break;\n      }\n    }\n    await callbackList.onTrainEnd();\n\n    await this.history.syncData();\n    return this.history;\n  }\n\n  // TODO(cais): Add code snippet below when it's possible to instantiate\n  //   actual dataset objects.\n  /**\n   * Trains the model using a dataset object.\n   *\n   * @param dataset A dataset object. Its `iterator()` method is expected\n   *   to generate a dataset iterator object, the `next()` method of which\n   *   is expected to produce data batches for training. The return value\n   *   of the `next()` call ought to contain a boolean `done` field and a\n   *   `value` field. The `value` field is expected to be an array of two\n   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n   *   case is for models with exactly one input and one output (e.g.\n   *   a sequential model). The latter case is for models with multiple\n   *   inputs and/or multiple outputs.\n   *   Of the two items in the array, the first is the input feature(s) and\n   *   the second is the output target(s).\n   * @param args A `ModelFitDatasetArgs`, containing optional fields.\n   *\n   * @return A `History` instance. Its `history` attribute contains all\n   *   information collected during training.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async fitDataset<T>(dataset: Dataset<T>, args: ModelFitDatasetArgs<T>):\n      Promise<History> {\n    return fitDataset(this, dataset, args);\n  }\n\n  /**\n   * Runs a single gradient update on a single batch of data.\n   *\n   * This method differs from `fit()` and `fitDataset()` in the following\n   * regards:\n   *   - It operates on exactly one batch of data.\n   *   - It returns only the loss and metric values, instead of\n   *     returning the batch-by-batch loss and metric values.\n   *   - It doesn't support fine-grained options such as verbosity and\n   *     callbacks.\n   *\n   * @param x Input data. It could be one of the following:\n   *   - A `tf.Tensor`, or an Array of `tf.Tensor`s (in case the model has\n   *     multiple inputs).\n   *   - An Object mapping input names to corresponding `tf.Tensor` (if the\n   *     model has named inputs).\n   * @param y Target data. It could be either a `tf.Tensor` or multiple\n   *   `tf.Tensor`s. It should be consistent with `x`.\n   * @returns Training loss or losses (in case the model has\n   *   multiple outputs), along with metrics (if any), as numbers.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async trainOnBatch(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|\n      {[inputName: string]: Tensor}): Promise<number|number[]> {\n    // TODO(cais): Support sampleWeight and classWeight.\n    // TODO(cais): Support Dataset objects.\n    const standardizeOut = await this.standardizeUserData(x, y);\n    const inputs = standardizeOut[0];\n    const targets = standardizeOut[1];\n    const trainFunction = this.makeTrainFunction();\n    const losses = trainFunction(inputs.concat(targets));\n    const lossValues: number[] = [];\n    for (const loss of losses) {\n      const v = await loss.data();\n      lossValues.push(v[0]);\n    }\n    tfc.dispose(losses);\n    disposeNewTensors(standardizeOut[0], x);\n    disposeNewTensors(standardizeOut[1], y);\n    return singletonOrArray(lossValues);\n  }\n\n  /**\n   * Extract weight values of the model.\n   *\n   * @param config: An instance of `io.SaveConfig`, which specifies\n   * model-saving options such as whether only trainable weights are to be\n   * saved.\n   * @returns A `NamedTensorMap` mapping original weight names (i.e.,\n   *   non-uniqueified weight names) to their values.\n   */\n  protected getNamedWeights(config?: io.SaveConfig): NamedTensor[] {\n    const namedWeights: NamedTensor[] = [];\n\n    const trainableOnly = config != null && config.trainableOnly;\n    const weights = trainableOnly ? this.trainableWeights : this.weights;\n    const weightValues = this.getWeights(trainableOnly);\n    for (let i = 0; i < weights.length; ++i) {\n      if (trainableOnly && !weights[i].trainable) {\n        // Optionally skip non-trainable weights.\n        continue;\n      }\n      namedWeights.push(\n          {name: weights[i].originalName, tensor: weightValues[i]});\n    }\n    return namedWeights;\n  }\n\n  /**\n   * Setter used for force stopping of LayersModel.fit() (i.e., training).\n   *\n   * Example:\n   *\n   * ```js\n   * const input = tf.input({shape: [10]});\n   * const output = tf.layers.dense({units: 1}).apply(input);\n   * const model = tf.model({inputs: [input], outputs: [output]});\n   * model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n   * const xs = tf.ones([8, 10]);\n   * const ys = tf.zeros([8, 1]);\n   *\n   * const history = await model.fit(xs, ys, {\n   *   epochs: 10,\n   *   callbacks: {\n   *     onEpochEnd: async (epoch, logs) => {\n   *       if (epoch === 2) {\n   *         model.stopTraining = true;\n   *       }\n   *     }\n   *   }\n   * });\n   *\n   * // There should be only 3 values in the loss array, instead of 10\n   * values,\n   * // due to the stopping after 3 epochs.\n   * console.log(history.history.loss);\n   * ```\n   */\n  set stopTraining(stop: boolean) {\n    this.stopTraining_ = stop;\n  }\n\n  get stopTraining(): boolean {\n    return this.stopTraining_;\n  }\n\n  get optimizer(): Optimizer {\n    return this.optimizer_;\n  }\n\n  set optimizer(optimizer: Optimizer) {\n    if (this.optimizer_ !== optimizer) {\n      this.optimizer_ = optimizer;\n      this.isOptimizerOwned = false;\n    }\n  }\n\n  override dispose(): DisposeResult {\n    const result = super.dispose();\n    if (result.refCountAfterDispose === 0 && this.optimizer != null &&\n        this.isOptimizerOwned) {\n      const numTensorsBeforeOptmizerDisposal = tfc.memory().numTensors;\n      this.optimizer_.dispose();\n      result.numDisposedVariables +=\n          numTensorsBeforeOptmizerDisposal - tfc.memory().numTensors;\n    }\n    return result;\n  }\n\n  private getLossIdentifiers(): LossIdentifier|LossIdentifier[]|\n      {[outputName: string]: LossIdentifier} {\n    let lossNames: LossIdentifier|LossIdentifier[]|\n        {[outputName: string]: LossIdentifier};\n    if (typeof this.loss === 'string') {\n      lossNames = toSnakeCase(this.loss) as LossIdentifier;\n    } else if (Array.isArray(this.loss)) {\n      for (const loss of this.loss) {\n        if (typeof loss !== 'string') {\n          throw new Error('Serialization of non-string loss is not supported.');\n        }\n      }\n      lossNames = (this.loss as string[]).map(name => toSnakeCase(name)) as\n          LossIdentifier[];\n    } else {\n      const outputNames = Object.keys(this.loss);\n      lossNames = {} as {[outputName: string]: LossIdentifier};\n      const losses =\n          this.loss as {[outputName: string]: LossOrMetricFn | string};\n      for (const outputName of outputNames) {\n        if (typeof losses[outputName] === 'string') {\n          lossNames[outputName] =\n              toSnakeCase(losses[outputName] as string) as LossIdentifier;\n        } else {\n          throw new Error('Serialization of non-string loss is not supported.');\n        }\n      }\n    }\n    return lossNames;\n  }\n\n  private getMetricIdentifiers(): MetricsIdentifier[]|\n      {[key: string]: MetricsIdentifier} {\n    if (typeof this.metrics === 'string' ||\n        typeof this.metrics === 'function') {\n      return [toSnakeCase(Metrics.getLossOrMetricName(this.metrics))];\n    } else if (Array.isArray(this.metrics)) {\n      return this.metrics.map(\n          metric => toSnakeCase(Metrics.getLossOrMetricName(metric)));\n    } else {\n      const metricsIdentifiers: {[key: string]: MetricsIdentifier} = {};\n      for (const key in this.metrics) {\n        metricsIdentifiers[key] =\n            toSnakeCase(Metrics.getLossOrMetricName(this.metrics[key]));\n      }\n      return metricsIdentifiers;\n    }\n  }\n\n  protected getTrainingConfig(): TrainingConfig {\n    return {\n      loss: this.getLossIdentifiers(),\n      metrics: this.getMetricIdentifiers(),\n      optimizer_config: {\n        class_name: this.optimizer.getClassName(),\n        config: this.optimizer.getConfig()\n      } as OptimizerSerialization\n    };\n    // TODO(cais): Add weight_metrics when they are supported.\n    // TODO(cais): Add sample_weight_mode when it's supported.\n    // TODO(cais): Add loss_weights when it's supported.\n  }\n\n  loadTrainingConfig(trainingConfig: TrainingConfig) {\n    if (trainingConfig.weighted_metrics != null) {\n      throw new Error('Loading weight_metrics is not supported yet.');\n    }\n    if (trainingConfig.loss_weights != null) {\n      throw new Error('Loading loss_weights is not supported yet.');\n    }\n    if (trainingConfig.sample_weight_mode != null) {\n      throw new Error('Loading sample_weight_mode is not supported yet.');\n    }\n\n    const tsConfig = convertPythonicToTs(trainingConfig.optimizer_config) as\n        serialization.ConfigDict;\n    const optimizer = deserialize(tsConfig) as Optimizer;\n\n    let loss;\n    if (typeof trainingConfig.loss === 'string') {\n      loss = toCamelCase(trainingConfig.loss);\n    } else if (Array.isArray(trainingConfig.loss)) {\n      loss = trainingConfig.loss.map(lossEntry => toCamelCase(lossEntry));\n    } else if (trainingConfig.loss != null) {\n      loss = {} as {[outputName: string]: LossIdentifier};\n      for (const key in trainingConfig.loss) {\n        loss[key] = toCamelCase(trainingConfig.loss[key]) as LossIdentifier;\n      }\n    }\n\n    let metrics;\n    if (Array.isArray(trainingConfig.metrics)) {\n      metrics = trainingConfig.metrics.map(metric => toCamelCase(metric));\n    } else if (trainingConfig.metrics != null) {\n      metrics = {} as {[outputName: string]: MetricsIdentifier};\n      for (const key in trainingConfig.metrics) {\n        metrics[key] = toCamelCase(trainingConfig.metrics[key]);\n      }\n    }\n\n    this.compile({loss, metrics, optimizer});\n  }\n\n  /**\n   * Save the configuration and/or weights of the LayersModel.\n   *\n   * An `IOHandler` is an object that has a `save` method of the proper\n   * signature defined. The `save` method manages the storing or\n   * transmission of serialized data (\"artifacts\") that represent the\n   * model's topology and weights onto or via a specific medium, such as\n   * file downloads, local storage, IndexedDB in the web browser and HTTP\n   * requests to a server. TensorFlow.js provides `IOHandler`\n   * implementations for a number of frequently used saving mediums, such as\n   * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n   * for more details.\n   *\n   * This method also allows you to refer to certain types of `IOHandler`s\n   * as URL-like string shortcuts, such as 'localstorage://' and\n   * 'indexeddb://'.\n   *\n   * Example 1: Save `model`'s topology and weights to browser [local\n   * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n   * then load it back.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * console.log('Prediction from original model:');\n   * model.predict(tf.ones([1, 3])).print();\n   *\n   * const saveResults = await model.save('localstorage://my-model-1');\n   *\n   * const loadedModel = await tf.loadLayersModel('localstorage://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * loadedModel.predict(tf.ones([1, 3])).print();\n   * ```\n   *\n   * Example 2. Saving `model`'s topology and weights to browser\n   * [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API);\n   * then load it back.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * console.log('Prediction from original model:');\n   * model.predict(tf.ones([1, 3])).print();\n   *\n   * const saveResults = await model.save('indexeddb://my-model-1');\n   *\n   * const loadedModel = await tf.loadLayersModel('indexeddb://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * loadedModel.predict(tf.ones([1, 3])).print();\n   * ```\n   *\n   * Example 3. Saving `model`'s topology and weights as two files\n   * (`my-model-1.json` and `my-model-1.weights.bin`) downloaded from\n   * browser.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * const saveResults = await model.save('downloads://my-model-1');\n   * ```\n   *\n   * Example 4. Send  `model`'s topology and weights to an HTTP server.\n   * See the documentation of `tf.io.http` for more details\n   * including specifying request parameters and implementation of the\n   * server.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * const saveResults = await model.save('http://my-server/model/upload');\n   * ```\n   *\n   * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n   * scheme-based string shortcut for `IOHandler`.\n   * @param config Options for saving the model.\n   * @returns A `Promise` of `SaveResult`, which summarizes the result of\n   * the saving, such as byte sizes of the saved artifacts for the model's\n   *   topology and weight values.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n  async save(handlerOrURL: io.IOHandler|string, config?: io.SaveConfig):\n      Promise<io.SaveResult> {\n    if (typeof handlerOrURL === 'string') {\n      const handlers = io.getSaveHandlers(handlerOrURL);\n      if (handlers.length === 0) {\n        throw new ValueError(\n            `Cannot find any save handlers for URL '${handlerOrURL}'`);\n      } else if (handlers.length > 1) {\n        throw new ValueError(\n            `Found more than one (${handlers.length}) save handlers for ` +\n            `URL '${handlerOrURL}'`);\n      }\n      handlerOrURL = handlers[0];\n    }\n    if (handlerOrURL.save == null) {\n      throw new ValueError(\n          'LayersModel.save() cannot proceed because the IOHandler ' +\n          'provided does not have the `save` attribute defined.');\n    }\n\n    const weightDataAndSpecs =\n        await io.encodeWeights(this.getNamedWeights(config));\n\n    const returnString = false;\n    const unusedArg: {} = null;\n    const modelConfig = this.toJSON(unusedArg, returnString);\n    const modelArtifacts: io.ModelArtifacts = {\n      modelTopology: modelConfig,\n      format: LAYERS_MODEL_FORMAT_NAME,\n      generatedBy: `TensorFlow.js tfjs-layers v${version}`,\n      convertedBy: null,\n    };\n\n    const includeOptimizer = config == null ? false : config.includeOptimizer;\n    if (includeOptimizer && this.optimizer != null) {\n      modelArtifacts.trainingConfig = this.getTrainingConfig();\n      const weightType = 'optimizer';\n      const {data: optimizerWeightData, specs: optimizerWeightSpecs} =\n          await io.encodeWeights(await this.optimizer.getWeights(), weightType);\n      weightDataAndSpecs.specs.push(...optimizerWeightSpecs);\n      weightDataAndSpecs.data = io.concatenateArrayBuffers(\n          [weightDataAndSpecs.data, optimizerWeightData]);\n    }\n\n    if (this.userDefinedMetadata != null) {\n      // Check serialized size of user-defined metadata.\n      const checkSize = true;\n      checkUserDefinedMetadata(this.userDefinedMetadata, this.name, checkSize);\n      modelArtifacts.userDefinedMetadata = this.userDefinedMetadata;\n    }\n\n    modelArtifacts.weightData = weightDataAndSpecs.data;\n    modelArtifacts.weightSpecs = weightDataAndSpecs.specs;\n    return handlerOrURL.save(modelArtifacts);\n  }\n\n  /**\n   * Set user-defined metadata.\n   *\n   * The set metadata will be serialized together with the topology\n   * and weights of the model during `save()` calls.\n   *\n   * @param setUserDefinedMetadata\n   */\n  setUserDefinedMetadata(userDefinedMetadata: {}): void {\n    checkUserDefinedMetadata(userDefinedMetadata, this.name);\n    this.userDefinedMetadata = userDefinedMetadata;\n  }\n\n  /**\n   * Get user-defined metadata.\n   *\n   * The metadata is supplied via one of the two routes:\n   *   1. By calling `setUserDefinedMetadata()`.\n   *   2. Loaded during model loading (if the model is constructed\n   *      via `tf.loadLayersModel()`.)\n   *\n   * If no user-defined metadata is available from either of the\n   * two routes, this function will return `undefined`.\n   */\n  getUserDefinedMetadata(): {} {\n    return this.userDefinedMetadata;\n  }\n}\nserialization.registerClass(LayersModel);\n\n/**\n * A `tf.Functional` is an alias to `tf.LayersModel`.\n *\n * See also:\n *   `tf.LayersModel`, `tf.Sequential`, `tf.loadLayersModel`.\n */\n/** @doc {heading: 'Models', subheading: 'Classes'} */\nexport class Functional extends LayersModel {\n  static override className = 'Functional';\n}\nserialization.registerClass(Functional);\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;AAAA;;;;;;;;GAQG,CAEH,uCAAA,EAAyC;;;;;;AAEzC,OAAO,KAAK,GAAG,MAAM,uBAAuB,CAAC;;;;;;;AAC7C,OAAO,EAAC,EAAE,EAA0D,SAAS,EAAU,MAAM,EAAE,aAAa,EAAE,MAAM,EAAY,QAAQ,EAAE,IAAI,EAAC,MAAM,uBAAuB,CAAC;AAE7K,OAAO,KAAK,CAAC,MAAM,yBAAyB,CAAC;AAC7C,OAAO,EAAe,kBAAkB,EAAkC,oBAAoB,EAAC,MAAM,mBAAmB,CAAC;AACzH,OAAO,EAAC,SAAS,EAAC,MAAM,WAAW,CAAC;AACpC,OAAO,EAAC,mBAAmB,EAAE,YAAY,EAAE,UAAU,EAAC,MAAM,WAAW,CAAC;AAKxE,OAAO,EAAC,WAAW,EAAC,MAAM,yBAAyB,CAAC;AACpD,OAAO,EAAE,oBAAoB,EAAkB,MAAM,SAAS,CAAC;AAC/D,OAAO,KAAK,MAAM,MAAM,WAAW,CAAC;AACpC,OAAO,KAAK,OAAO,MAAM,YAAY,CAAC;AACtC,OAAO,KAAK,UAAU,MAAM,eAAe,CAAC;AAE5C,OAAO,EAAC,wBAAwB,EAAC,MAAM,0BAA0B,CAAC;AAClE,OAAO,EAAC,KAAK,EAAE,YAAY,EAAE,gBAAgB,EAAE,WAAW,EAAE,WAAW,EAAE,MAAM,EAAC,MAAM,wBAAwB,CAAC;AAC/G,OAAO,EAAC,YAAY,EAAC,MAAM,sBAAsB,CAAC;AAClD,OAAO,EAAC,KAAK,EAAC,MAAM,qBAAqB,CAAC;AAC1C,OAAO,EAAC,mBAAmB,EAAC,MAAM,8BAA8B,CAAC;AAEjE,OAAO,EAAC,OAAO,EAAC,MAAM,YAAY,CAAC;AAEnC,OAAO,EAAC,SAAS,EAAgB,MAAM,aAAa,CAAC;AAErD,OAAO,EAAC,OAAO,EAAE,QAAQ,EAAC,MAAM,YAAY,CAAC;AAE7C,OAAO,EAAC,eAAe,EAAE,UAAU,EAAgD,MAAM,oBAAoB,CAAC;AAC9G,OAAO,EAAC,cAAc,EAAE,iBAAiB,EAAE,0BAA0B,EAAE,WAAW,EAAgB,WAAW,EAAE,oBAAoB,EAAC,MAAM,oBAAoB,CAAC;AAC/J,OAAO,EAA8B,mBAAmB,EAAE,uBAAuB,EAAE,kBAAkB,EAAC,MAAM,kBAAkB,CAAC;;;;;;;;;;;;;;;;;;;;;;;AAKzH,SAAU,YAAY,CAAC,CAC+B;IAC1D,OAAO,CAAC,YAAY,kPAAM,CAAC;AAC7B,CAAC;AAKK,SAAU,WAAW,CAAC,CAC6B;IACvD,OAAO,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;AAC1B,CAAC;AAKK,SAAU,UAAU,CAAC,CAC6B;IACtD,OAAO,CAAC,YAAY,CAAC,CAAC,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;AAC7C,CAAC;AAaK,SAAU,oBAAoB,CAChC,IAAmD,EAAE,KAAe,EACpE,MAAgB,EAAE,cAAc,GAAG,IAAI,EAAE,eAAe,GAAG,EAAE;IAC/D,IAAI,KAAK,IAAI,IAAI,IAAI,KAAK,CAAC,MAAM,KAAK,CAAC,EAAE;QACvC,yEAAyE;QACzE,QAAQ;QACR,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,iBAAiB,GAAG,KAAK,CAAC;YAC9B,IAAI,WAAW,CAAC,IAAI,CAAC,IAAK,IAAiB,CAAC,MAAM,GAAG,CAAC,EAAE;gBACtD,iBAAiB,GAAG,IAAI,CAAC;aAC1B,MAAM,IAAI,UAAU,CAAC,IAAI,CAAC,EAAE;gBAC3B,IAAK,MAAM,GAAG,IAAI,IAAI,CAAE;oBACtB,IAAI,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,EAAE;wBAC5B,iBAAiB,GAAG,IAAI,CAAC;wBACzB,MAAM;qBACP;iBACF;aACF,MAAM;gBACL,6CAA6C;gBAC7C,iBAAiB,GAAG,IAAI,CAAC;aAC1B;YACD,IAAI,iBAAiB,EAAE;gBACrB,MAAM,IAAI,ySAAU,CAChB,CAAA,0BAAA,EAA6B,eAAe,CAAA,mBAAA,CAAqB,GACjE,CAAA,QAAA,EAAW,IAAI,EAAE,CAAC,CAAC;aACxB;SACF;QACD,OAAO,EAAE,CAAC;KACX;IACD,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,OAAO,KAAK,CAAC,GAAG,EAAC,IAAI,CAAC,EAAE,AAAC,IAAI,CAAC,CAAC;KAChC;IAED,IAAI,MAAgB,CAAC;IACrB,IAAI,UAAU,CAAC,IAAI,CAAC,EAAE;QACpB,IAAI,GAAG,IAAqC,CAAC;QAC7C,MAAM,GAAG,EAAE,CAAC;QACZ,KAAK,MAAM,IAAI,IAAI,KAAK,CAAE;YACxB,IAAI,IAAI,CAAC,IAAI,CAAC,IAAI,IAAI,EAAE;gBACtB,MAAM,IAAI,ySAAU,CAChB,CAAA,sBAAA,EAAyB,IAAI,CAAA,8BAAA,CAAgC,GAC7D,GAAG,KAAK,EAAE,CAAC,CAAC;aACjB;YACD,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC;SACzB;KACF,MAAM,IAAI,WAAW,CAAC,IAAI,CAAC,EAAE;QAC5B,IAAI,GAAG,IAAgB,CAAC;QACxB,IAAI,IAAI,CAAC,MAAM,KAAK,KAAK,CAAC,MAAM,EAAE;YAChC,MAAM,IAAI,ySAAU,CAChB,CAAA,0BAAA,EAA6B,eAAe,CAAA,eAAA,CAAiB,GAC7D,CAAA,+DAAA,CAAiE,GACjE,CAAA,gCAAA,EAAmC,KAAK,CAAC,MAAM,CAAA,gBAAA,CAAkB,GACjE,CAAA,6CAAA,EAAgD,IAAI,EAAE,CAAC,CAAC;SAC7D;QACD,MAAM,GAAG,IAAI,CAAC;KACf,MAAM;QACL,IAAI,GAAG,IAAc,CAAC;QACtB,IAAI,KAAK,CAAC,MAAM,GAAG,CAAC,EAAE;YACpB,MAAM,IAAI,ySAAU,CAChB,CAAA,UAAA,EAAa,eAAe,CAAA,SAAA,EAAY,KAAK,CAAC,MAAM,CAAA,YAAA,CAAc,GAClE,CAAA,uDAAA,EACI,IAAI,CAAC,KAAK,EAAE,CAAC,CAAC;SACvB;QACD,MAAM,GAAG;YAAC,IAAI;SAAC,CAAC;KACjB;IAED,MAAM,OAAG,6UAA0B,EAAC,MAAM,CAAC,CAAC;IAE5C,6BAA6B;IAC7B,IAAI,MAAM,IAAI,IAAI,EAAE;QAClB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YACrC,IAAI,MAAM,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;gBACrB,SAAS;aACV;YACD,MAAM,KAAK,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;YACxB,IAAI,KAAK,CAAC,KAAK,CAAC,MAAM,KAAK,MAAM,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE;gBAC3C,MAAM,IAAI,ySAAU,CAChB,CAAA,oBAAA,EAAuB,eAAe,CAAA,WAAA,EAAc,KAAK,CAAC,CAAC,CAAC,CAAA,CAAA,CAAG,GAC/D,CAAA,QAAA,EAAW,MAAM,CAAC,CAAC,CAAC,CAAC,MAAM,CAAA,kCAAA,CAAoC,GAC/D,CAAA,MAAA,EAAS,KAAK,CAAC,KAAK,EAAE,CAAC,CAAC;aAC7B;YACD,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBACzC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,cAAc,EAAE;oBAE9B,SAAS;iBACV;gBACD,MAAM,GAAG,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;gBAC3B,MAAM,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;gBAC5B,IAAI,MAAM,IAAI,IAAI,IAAI,MAAM,IAAI,CAAC,IAAI,GAAG,KAAK,MAAM,EAAE;oBACnD,MAAM,IAAI,ySAAU,CAChB,GAAG,eAAe,CAAA,yCAAA,CAA2C,GAC7D,CAAA,mBAAA,EAAsB,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAA,EAAA,CAAI,GAC9D,CAAA,sBAAA,EACI,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAA,EAAA,CAAI,GAC5C,CAAA,SAAA,EAAY,eAAe,CAAA,wBAAA,EACvB,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,GACpB,CAAA,4BAAA,EACI,KAAK,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,KAAK,CAAC,KAAK,CAAC,MAAM,CAAC,CAAA,CAAA,CAAG,GAC/C,CAAA,gBAAA,EAAmB,KAAK,CAAC,KAAK,CAAA,EAAA,CAAI,CAAC,CAAC;iBACzC;aACF;SACF;KACF;IACD,OAAO,MAAM,CAAC;AAChB,CAAC;AASK,SAAU,iBAAiB,CAC7B,MAAgB,EAAE,OAAiB,EAAE,OAAkB;IACzD,MAAM,IAAI,OAAG,qTAAM,EAAC,MAAM,CAAC,GAAG,EAAC,KAAK,CAAC,EAAE,AAAC,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IACzD,IAAI,CAAC,IAAI,EAAE,CAAC;IACZ,MAAM,IAAI,OAAG,qTAAM,EAAC,OAAO,CAAC,GAAG,EAAC,MAAM,CAAC,EAAE,AAAC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAC5D,IAAI,CAAC,IAAI,EAAE,CAAC;IACZ,uCAAuC;IACvC,IAAI,IAAI,CAAC,MAAM,GAAG,CAAC,EAAE;QACnB,MAAM,IAAI,ySAAU,CAChB,CAAA,8DAAA,CAAgE,GAChE,CAAA,kBAAA,CAAoB,GACpB,GAAG,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,GAAG,EAAC,KAAK,CAAC,EAAE,AAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC;KAC5D;IACD,IAAI,IAAI,CAAC,MAAM,GAAG,CAAC,EAAE;QACnB,MAAM,IAAI,ySAAU,CAChB,CAAA,+DAAA,CAAiE,GACjE,CAAA,kBAAA,CAAoB,GACpB,GAAG,IAAI,CAAC,SAAS,CAAC,OAAO,CAAC,GAAG,EAAC,MAAM,CAAC,EAAG,AAAD,MAAO,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC;KAC/D;IACD,IAAI,IAAI,CAAC,MAAM,GAAG,CAAC,IAAI,IAAI,CAAC,MAAM,GAAG,CAAC,IAAI,CAAC,8QAAI,CAAC,WAAW,CAAC,IAAI,EAAE,IAAI,CAAC,EAAE;QACvE,MAAM,IAAI,ySAAU,CAChB,CAAA,+DAAA,CAAiE,GACjE,CAAA,eAAA,EAAkB,IAAI,CAAC,CAAC,CAAC,CAAA,qBAAA,EAAwB,IAAI,CAAC,CAAC,CAAC,CAAA,QAAA,CAAU,GAClE,CAAA,UAAA,CAAY,CAAC,CAAC;KACnB;AACH,CAAC;AAED;;;;;;;;GAQG,CACH,SAAS,+BAA+B,CACpC,OAAiB,EAAE,OAAyB,EAAE,YAAqB;IACrE,uCAAuC;IACvC,MAAM,SAAS,GAAG;QAChB,MAAM,CAAC,wSAAgB;QAAE,MAAM,CAAC,0SAAkB;QAClD,MAAM,CAAC,+SAAuB;KAC/B,CAAC;IACF,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,OAAO,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;QACvC,MAAM,CAAC,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;QACrB,MAAM,IAAI,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;QACxB,MAAM,KAAK,GAAG,YAAY,CAAC,CAAC,CAAC,CAAC;QAC9B,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,SAAS;SACV;QACD,IAAI,IAAI,KAAK,MAAM,CAAC,+SAAuB,EAAE;YAC3C,IAAI,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC,EAAE;gBACrC,MAAM,IAAI,ySAAU,CAChB,CAAA,wCAAA,EAA2C,CAAC,CAAC,KAAK,CAAA,aAAA,CAAe,GACjE,CAAA,6DAAA,CAA+D,GAC/D,CAAA,2DAAA,CAA6D,GAC7D,CAAA,mBAAA,CAAqB,CAAC,CAAC;YAC3B,6CAA6C;aAC9C;SACF;QACD,IAAI,SAAS,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;YAClC,MAAM,YAAY,GAAG,CAAC,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;YACtC,MAAM,WAAW,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;YACnC,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,YAAY,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBAC5C,MAAM,SAAS,GAAG,YAAY,CAAC,CAAC,CAAC,CAAC;gBAClC,MAAM,MAAM,GAAG,WAAW,CAAC,CAAC,CAAC,CAAC;gBAC9B,IAAI,MAAM,IAAI,IAAI,IAAI,SAAS,KAAK,MAAM,EAAE;oBAC1C,MAAM,IAAI,ySAAU,CAChB,CAAA,2BAAA,EAA8B,CAAC,CAAC,KAAK,CAAA,mBAAA,CAAqB,GAC1D,CAAA,gBAAA,EAAmB,KAAK,CAAA,mCAAA,CAAqC,GAC7D,CAAA,qDAAA,CAAuD,CAAC,CAAC;iBAC9D;aACF;SACF;KACF;AACH,CAAC;AAED;;;;;;;;;;;;;;;;;;;;;;;;;GAyBG,CACH,SAAS,cAAc,CACnB,IAAqB,EAAE,KAAe,EAAE,MAAgB,EACxD,cAAc,GAAG,IAAI,EAAE,eAAe,GAAG,EAAE;IAC7C,IAAI,MAAgB,CAAC;IACrB,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE;QACvB,IAAI,IAAI,CAAC,MAAM,KAAK,KAAK,CAAC,MAAM,EAAE;YAChC,MAAM,IAAI,ySAAU,CAChB,CAAA,0BAAA,EAA6B,eAAe,CAAA,eAAA,CAAiB,GAC7D,CAAA,+DAAA,CAAiE,GACjE,CAAA,oCAAA,EAAuC,KAAK,CAAC,MAAM,CAAA,WAAA,CAAa,GAChE,CAAA,iBAAA,EAAoB,IAAI,CAAC,MAAM,CAAA,YAAA,CAAc,CAAC,CAAC;SACpD;QACD,MAAM,GAAG,IAAI,CAAC;KACf,MAAM;QACL,IAAI,KAAK,CAAC,MAAM,GAAG,CAAC,EAAE;YACpB,MAAM,IAAI,ySAAU,CAChB,CAAA,kBAAA,EAAqB,KAAK,CAAC,MAAM,CAAA,CAAA,EAAI,eAAe,CAAA,UAAA,CAAY,GAChE,CAAA,sDAAA,CAAwD,GACxD,GAAG,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,KAAK,CAAC,CAAA,CAAA,CAAG,CAAC,CAAC;SACvC;QACD,MAAM,GAAG;YAAC,IAAI;SAAC,CAAC;KACjB;IAED,IAAI,MAAM,IAAI,IAAI,EAAE;QAClB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YACrC,IAAI,MAAM,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;gBACrB,SAAS;aACV;YACD,MAAM,KAAK,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC;YACxB,IAAI,KAAK,CAAC,KAAK,CAAC,MAAM,KAAK,MAAM,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE;gBAC3C,MAAM,IAAI,ySAAU,CAChB,CAAA,oBAAA,EAAuB,eAAe,CAAA,WAAA,EAAc,KAAK,CAAC,CAAC,CAAC,CAAA,CAAA,CAAG,GAC/D,CAAA,QAAA,EAAW,MAAM,CAAC,CAAC,CAAC,CAAC,MAAM,CAAA,kCAAA,CAAoC,GAC/D,CAAA,MAAA,EAAS,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC;aAC7C;YACD,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBACzC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,cAAc,EAAE;oBAC9B,SAAS;iBACV;gBACD,MAAM,GAAG,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;gBAC3B,MAAM,MAAM,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;gBAC5B,IAAI,MAAM,IAAI,IAAI,EAAE;oBAClB,IAAI,MAAM,KAAK,GAAG,EAAE;wBAClB,MAAM,IAAI,ySAAU,CAChB,CAAA,oBAAA,EAAuB,eAAe,CAAA,WAAA,CAAa,GACnD,GAAG,KAAK,CAAC,CAAC,CAAC,CAAA,eAAA,EAAkB,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAA,KAAA,CAAO,GAC7D,CAAA,qBAAA,EAAwB,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,KAAK,CAAC,CAAA,CAAA,CAAG,CAAC,CAAC;qBAC7D;iBACF;aACF;SACF;KACF;AACH,CAAC;AAeK,SAAU,cAAc,CAC1B,OAC+C,EAC/C,WAAqB;IACvB,IAAI,OAAO,IAAI,IAAI,IAAI,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,OAAO,CAAC,MAAM,KAAK,CAAC,EAAE;QACrE,OAAO,WAAW,CAAC,GAAG,EAAC,IAAI,CAAC,EAAE,AAAC,EAAE,CAAC,CAAC;KACpC;IAED,IAAI,cAC+C,CAAC;IACpD,IAAI,OAAO,OAAO,KAAK,QAAQ,IAAI,OAAO,OAAO,KAAK,UAAU,EAAE;QAChE,cAAc,GAAG;YAAC,OAAO;SAAC,CAAC;KAC5B,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,OAAO,OAAO,KAAK,QAAQ,EAAE;QAChE,cAAc,GAAG,OAC0D,CAAC;KAC7E,MAAM;QACL,MAAM,IAAI,SAAS,CACf,8DAA8D,GAC9D,CAAA,mCAAA,EAAsC,OAAO,EAAE,CAAC,CAAC;KACtD;IAED,IAAI,KAAK,CAAC,OAAO,CAAC,cAAc,CAAC,EAAE;QACjC,4CAA4C;QAC5C,OAAO,WAAW,CAAC,GAAG,EAClB,IAAI,CAAC,EAAE,AAAC,cAA8C,CAAC,CAAC;KAC7D,MAAM;QACL,mCAAmC;QACnC,MAAM,aAAa,GAAwC,EAAE,CAAC;QAC9D,KAAK,MAAM,IAAI,IAAI,WAAW,CAAE;YAC9B,IAAI,aAAa,GACb,cAAc,CAAC,cAAc,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,cAAc,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC;YACpE,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,aAAa,CAAC,EAAE;gBACjC,aAAa,GAAG;oBAAC,aAAa;iBAAC,CAAC;aACjC;YACD,aAAa,CAAC,IAAI,CAAC,aAAa,CAAC,CAAC;SACnC;QACD,OAAO,aAAa,CAAC;KACtB;AACH,CAAC;AA2DD,MAAM,wBAAwB,GAAG,cAAc,CAAC;AAEhD;;;;;;;;;;;GAWG,CACH,MAAa,WAAY,SAAQ,qTAAS;IA4CxC,YAAY,IAAmB,CAAA;QAC7B,KAAK,CAAC,IAAI,CAAC,CAAC;QACZ,IAAI,CAAC,UAAU,GAAG,KAAK,CAAC;IAC1B,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OAkCG,CACH,OAAO,CACH,UAAmB,EAAE,SAAoB,EACzC,UAEoD,OAAO,CAAC,GAAG,EAAA;QACjE,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE;YACf,MAAM,IAAI,ySAAU,CAChB,CAAA,iEAAA,CAAmE,GACnE,CAAA,6DAAA,CAA+D,GAC/D,CAAA,8CAAA,CAAgD,CAAC,CAAC;SACvD;YACD,yTAAY,EAAC,IAAI,EAAE,UAAU,EAAE,SAAS,EAAE,OAAO,CAAC,CAAC;IACrD,CAAC;IAED;;;;;;;;;OASG,CACH,OAAO,CAAC,IAAsB,EAAA;QAC5B,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,EAAE;YACrB,IAAI,CAAC,IAAI,GAAG,EAAE,CAAC;SAChB;QACD,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;QAEtB,IAAI,OAAO,IAAI,CAAC,SAAS,KAAK,QAAQ,EAAE;YACtC,IAAI,CAAC,UAAU,GAAG,UAAU,CAAC,oSAAY,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;YAC1D,IAAI,CAAC,gBAAgB,GAAG,IAAI,CAAC;SAC9B,MAAM;YACL,IAAI,CAAC,CAAC,IAAI,CAAC,SAAS,YAAY,sQAAS,CAAC,EAAE;gBAC1C,MAAM,IAAI,ySAAU,CAChB,CAAA,2DAAA,CAA6D,CAAC,CAAC;aACpE;YACD,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,SAAS,CAAC;YACjC,IAAI,CAAC,gBAAgB,GAAG,KAAK,CAAC;SAC/B;QAED,+BAA+B;QAC/B,oCAAoC;QAEpC,0BAA0B;QAC1B,IAAI,aAAa,GAAqB,EAAE,CAAC;QACzC,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,IAAI,OAAO,IAAI,CAAC,IAAI,KAAK,QAAQ,IAC1D,OAAO,IAAI,CAAC,IAAI,KAAK,UAAU,EAAE;YACnC,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAsC,CAAC;YACxD,IAAK,MAAM,IAAI,IAAI,IAAI,CAAC,IAAI,CAAE;gBAC5B,IAAI,IAAI,CAAC,WAAW,CAAC,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;oBACzC,MAAM,IAAI,ySAAU,CAChB,CAAA,mCAAA,EAAsC,IAAI,CAAA,GAAA,CAAK,GAC/C,CAAA,kCAAA,EAAqC,IAAI,CAAC,WAAW,EAAE,CAAC,CAAC;iBAC9D;aACF;YACD,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,WAAW,CAAE;gBACnC,IAAI,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,IAAI,IAAI,EAAE;oBAC3B,OAAO,CAAC,IAAI,CACR,CAAA,QAAA,EAAW,IAAI,CAAA,6CAAA,CAA+C,GAC9D,CAAA,4DAAA,CAA8D,GAC9D,CAAA,gBAAA,EAAmB,IAAI,CAAA,gBAAA,CAAkB,CAAC,CAAC;iBAChD;gBACD,aAAa,CAAC,IAAI,CAAC,MAAM,CAAC,2RAAG,CAAC,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;aACjD;SACF,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE;YACnC,IAAI,IAAI,CAAC,IAAI,CAAC,MAAM,KAAK,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE;gBAC5C,MAAM,IAAI,ySAAU,CAChB,CAAA,4DAAA,CAA8D,GAC9D,CAAA,4BAAA,EAA+B,IAAI,CAAC,OAAO,CAAC,MAAM,CAAA,YAAA,CAAc,GAChE,CAAA,oBAAA,EAAuB,IAAI,CAAC,IAAI,CAAA,CAAA,CAAG,CAAC,CAAC;aAC1C;YACD,MAAM,SAAS,GAAG,IAAI,CAAC,IAAoC,CAAC;YAC5D,aAAa,GAAG,SAAS,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,MAAM,CAAC,2RAAG,CAAC,CAAC,CAAC,CAAC,CAAC;SACnD,MAAM;YACL,MAAM,YAAY,GAAG,MAAM,CAAC,2RAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YAC3C,IAAI,CAAC,OAAO,CAAC,OAAO,EAAC,CAAC,CAAC,EAAE;gBACvB,aAAa,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC;YACnC,CAAC,CAAC,CAAC;SACJ;QAED,IAAI,CAAC,aAAa,GAAG,aAAa,CAAC;QAEnC,IAAI,CAAC,eAAe,GAAG,EAAE,CAAC;QAC1B,IAAI,CAAC,gBAAgB,GAAG,EAAE,CAAC;QAC3B,IAAI,CAAC,WAAW,GAAG,EAAE,CAAC;QACtB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YAC5C,4CAA4C;YAC5C,MAAM,KAAK,GAAG,IAAI,CAAC,oBAAoB,CAAC,CAAC,CAAC,CAAC;YAC3C,MAAM,IAAI,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;YACjC,IAAI,CAAC,eAAe,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YAChC,IAAI,CAAC,gBAAgB,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YAClC,IAAI,CAAC,WAAW,CAAC,IAAI,CAAC,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC,CAAC;SAC9C;QAED,0CAA0C;QAC1C,4CAA4C;QAC5C,MAAM,iBAAiB,GAAa,EAAE,CAAC;QAEvC,mBAAmB;QACnB,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,CAAC;QAC5B,mCAAmC;QACnC,IAAI,CAAC,YAAY,GAAG;YAAC,MAAM;SAAC,CAAC;QAC7B,IAAI,CAAC,cAAc,GAAG,EAAE,CAAC;QAEzB,sBAAsB;QACtB,yEAAyE;QACzE,0EAA0E;QAC1E,uEAAuE;YACvE,wSAAS,EAAC,MAAM,EAAE,GAAG,EAAE;YACrB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBAC5C,IAAI,iBAAiB,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;oBACvC,SAAS;iBACV;gBACD,uDAAuD;gBACvD,8CAA8C;gBAC9C,MAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;gBAC3C,IAAI,IAAI,CAAC,OAAO,CAAC,MAAM,GAAG,CAAC,EAAE;oBAC3B,IAAI,CAAC,cAAc,CAAC,IAAI,CAAC;wBAAC,YAAY;wBAAE,CAAC;qBAAC,CAAC,CAAC;oBAC5C,IAAI,CAAC,YAAY,CAAC,IAAI,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,GAAG,OAAO,CAAC,CAAC;iBACvD;aACF;QAED,0EAA0E;QAC1E,yEAAyE;QAC3E,CAAC,CAAC,CAAC;QAEH,MAAM,aAAa,GAAG,cAAc,CAAC,IAAI,CAAC,OAAO,EAAE,IAAI,CAAC,WAAW,CAAC,CAAC;QACrE,yCAAyC;QAEzC;;WAEG,CACH,MAAM,YAAY,GACd,CAAC,WAAmB,EAAE,UAAkB,EACvC,YAA4B,EAAE,EAAE;YAC/B,IAAI,IAAI,CAAC,WAAW,CAAC,MAAM,GAAG,CAAC,EAAE;gBAC/B,UAAU,GAAG,IAAI,CAAC,WAAW,CAAC,WAAW,CAAC,GAAG,GAAG,GAAG,UAAU,CAAC;aAC/D;YACD,IAAI,CAAC,YAAY,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;YACnC,IAAI,CAAC,cAAc,CAAC,IAAI,CAAC;gBAAC,YAAY;gBAAE,WAAW;aAAC,CAAC,CAAC;QACxD,CAAC,CAAC;YAEN,wSAAS,EAAC,QAAQ,EAAE,GAAG,EAAE;YACvB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBAC5C,IAAI,iBAAiB,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;oBACvC,SAAS;iBACV;gBACD,MAAM,aAAa,GAAG,aAAa,CAAC,CAAC,CAAC,CAAC;gBACvC,qDAAqD;gBAErD,oEAAoE;gBACpE,MAAM,aAAa,GAAG,CAAC,OAAqC,EAAE,EAAE;oBAC9D,MAAM,gBAAgB,GAAG,EAAE,CAAC;oBAC5B,IAAI,UAAkB,CAAC;oBACvB,IAAI,KAAqB,CAAC;oBAC1B,IAAI,gBAAgC,CAAC;oBACrC,oDAAoD;oBAEpD,KAAK,MAAM,MAAM,IAAI,OAAO,CAAE;wBAC5B,IAAI,OAAO,MAAM,KAAK,QAAQ,IAC1B;4BAAC,UAAU;4BAAE,KAAK;4BAAE,cAAc;4BAAE,IAAI;yBAAC,CAAC,OAAO,CAAC,MAAM,CAAC,KACrD,CAAC,CAAC,EAAE;4BACV,MAAM,WAAW,GAAG,IAAI,CAAC,oBAAoB,CAAC,CAAC,CAAC,CAAC;4BAEjD,IAAI,WAAW,CAAC,WAAW,CAAC,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC,IACzC,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,KAAK,MAAM,CAAC,0SAAkB,EAAE;gCACvD,sCAAsC;gCACtC,IAAI;oCAAC,UAAU;oCAAE,KAAK;iCAAC,CAAC,OAAO,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;oCAC9C,KAAK,GAAG,OAAO,CAAC,sSAAc,CAAC;iCAChC,MAAM,IAAI;oCAAC,cAAc;oCAAE,IAAI;iCAAC,CAAC,OAAO,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;oCACxD,KAAK,GAAG,OAAO,CAAC,0SAAkB,CAAC;iCACpC;6BACF,MAAM,IACH,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,KACrB,MAAM,CAAC,qTAA6B,EAAE;gCACxC,wDAAwD;gCACxD,WAAW;gCACX,IAAI;oCAAC,UAAU;oCAAE,KAAK;iCAAC,CAAC,OAAO,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;oCAC9C,KAAK,GAAG,OAAO,CAAC,iTAAyB,CAAC;iCAC3C,MAAM,IAAI;oCAAC,cAAc;oCAAE,IAAI;iCAAC,CAAC,OAAO,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;oCACxD,KAAK,GAAG,OAAO,CAAC,qTAA6B,CAAC;iCAC/C;6BACF,MAAM;gCACL,6CAA6C;gCAC7C,IAAI;oCAAC,UAAU;oCAAE,KAAK;iCAAC,CAAC,OAAO,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;oCAC9C,KAAK,GAAG,OAAO,CAAC,2SAAmB,CAAC;iCACrC,MAAM,IAAI;oCAAC,cAAc;oCAAE,IAAI;iCAAC,CAAC,OAAO,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;oCACxD,KAAK,GAAG,OAAO,CAAC,+SAAuB,CAAC;iCACzC;6BACF;4BACD,IAAI,MAAc,CAAC;4BACnB,IAAI;gCAAC,UAAU;gCAAE,KAAK;6BAAC,CAAC,OAAO,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;gCAC9C,MAAM,GAAG,KAAK,CAAC;6BAChB,MAAM,IAAI;gCAAC,cAAc;gCAAE,IAAI;6BAAC,CAAC,OAAO,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;gCACxD,MAAM,GAAG,IAAI,CAAC;6BACf;4BACD,sCAAsC;4BACtC,gBAAgB,GAAG,KAAK,CAAC;4BACzB,UAAU,GAAG,gBAAgB,GAAG,MAAM,CAAC;yBACxC,MAAM;4BACL,MAAM,QAAQ,GAAG,OAAO,CAAC,2RAAG,CAAC,MAAM,CAAC,CAAC;4BACrC,sCAAsC;4BACtC,gBAAgB,GAAG,QAAQ,CAAC;4BAC5B,UAAU,GACN,gBAAgB,GAAG,OAAO,CAAC,2SAAmB,CAAC,MAAM,CAAC,CAAC;yBAC5D;wBAED,yDAAyD;wBACzD,IAAI,YAA4B,CAAC;4BACjC,wSAAS,EAAC,UAAU,EAAE,GAAG,EAAE;4BACzB,YAAY,GAAG,gBAAgB,CAAC;wBAClC,CAAC,CAAC,CAAC;wBACH,YAAY,CAAC,CAAC,EAAE,UAAU,EAAE,YAAY,CAAC,CAAC;qBAC3C;gBACH,CAAC,CAAC;gBAEF,aAAa,CAAC,aAAa,CAAC,CAAC;YAC7B,+CAA+C;aAChD;QACH,CAAC,CAAC,CAAC;QAEH,4DAA4D;QAC5D,2EAA2E;QAC3E,IAAI,CAAC,yBAAyB,GAAG,IAAI,CAAC,gBAAgB,CAAC;IACzD,CAAC;IAED;;;;;;;;OAQG,CACO,gCAAgC,GAAA;QACxC,IAAI,IAAI,CAAC,yBAAyB,IAAI,IAAI,EAAE;YAC1C,OAAO;SACR;QACD,IAAI,IAAI,CAAC,gBAAgB,CAAC,MAAM,KAC5B,IAAI,CAAC,yBAAyB,CAAC,MAAM,EAAE;YACzC,OAAO,CAAC,IAAI,CACR,+DAA+D,GAC/D,yDAAyD,GACzD,+BAA+B,CAAC,CAAC;SACtC;IACH,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OA8BG,CACH,QAAQ,CACJ,CAAkB,EAAE,CAAkB,EACtC,OAA0B,CAAA,CAAE,EAAA;QAC9B,MAAM,SAAS,GAAG,IAAI,CAAC,SAAS,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,SAAS,CAAC;YAC/D,iUAAc,EAAC,SAAS,CAAC,CAAC;QAE1B,0DAA0D;QAC1D,sBAAsB;QACtB,MAAM,cAAc,GAAG,IAAI,CAAC;QAC5B,MAAM,gBAAgB,GAClB,IAAI,CAAC,qBAAqB,CAAC,CAAC,EAAE,CAAC,EAAE,cAAc,EAAE,SAAS,CAAC,CAAC;QAChE,IAAI;YACF,wEAAwE;YACxE,qBAAqB;YACrB,MAAM,GAAG,GAAG,gBAAgB,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,gBAAgB,CAAC,CAAC,CAAC,CAAC,CAAC;YAC5D,IAAI,CAAC,gBAAgB,EAAE,CAAC;YACxB,MAAM,CAAC,GAAG,IAAI,CAAC,YAAY,CAAC;YAC5B,MAAM,QAAQ,GACV,IAAI,CAAC,QAAQ,CAAC,CAAC,EAAE,GAAG,EAAE,SAAS,EAAE,IAAI,CAAC,OAAO,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;YAC/D,WAAO,+TAAgB,EAAC,QAAQ,CAAC,CAAC;SACnC,QAAS;gBACR,oUAAiB,EAAC,gBAAgB,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;gBAC1C,oUAAiB,EAAC,gBAAgB,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;SAC3C;IACH,CAAC;IAED,mEAAmE;IACnE,eAAe;IACf;;;;;;;;;;;;;;;;;;;OAmBG,CACH,KAAK,CAAC,eAAe,CAAC,OAAoB,EAAE,IAA+B,EAAA;QAEzE,IAAI,CAAC,gBAAgB,EAAE,CAAC;QACxB,WAAO,kUAAe,EAAC,IAAI,EAAE,OAAO,EAAE,IAAI,CAAC,CAAC;IAC9C,CAAC;IAED;;;;;;;;;OASG,CACK,eAAe,CACnB,GAAoB,EAAE,SAAkB,EAAE,KAAc,EACxD,SAAS,GAAG,OAAO,EAAA;QACrB,IAAI,UAAkB,CAAC;QACvB,IAAI,KAAK,IAAI,IAAI,EAAE;YACjB,UAAU,GAAG,IAAI,CAAC;YAClB,IAAI,SAAS,IAAI,IAAI,EAAE;gBACrB,MAAM,IAAI,ySAAU,CAChB,CAAA,GAAA,EAAM,SAAS,CAAA,6CAAA,CAA+C,GAC9D,CAAA,gBAAA,EAAmB,SAAS,EAAE,CAAC,CAAC;aACrC;SACF,MAAM,IAAI,GAAG,IAAI,IAAI,EAAE;YACtB,IAAI,KAAK,CAAC,OAAO,CAAC,GAAG,CAAC,EAAE;gBACtB,UAAU,GAAG,GAAG,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;aAC9B,MAAM;gBACL,UAAU,GAAG,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;aAC3B;SACF,MAAM;YACL,MAAM,IAAI,ySAAU,CAChB,CAAA,sDAAA,CAAwD,GACxD,GAAG,SAAS,CAAA,oBAAA,CAAsB,CAAC,CAAC;SACzC;QACD,OAAO,UAAU,CAAC;IACpB,CAAC;IAED;;;;;;OAMG,CACH,OAAO,CAAC,MAAsC,EAAE,OAAwB,EAAA;QAEtE,IAAI,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,OAAO,CAAC,MAAM,KAAK,CAAC,EAAE;YAClD,MAAM,IAAI,ySAAU,CAChB,oDAAoD,CAAC,CAAC;SAC3D;QAED,MAAM,cAAc,GAAG,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC;QAC9C,MAAM,WAAW,GACb,AAAC,cAAc,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC;YAAC,OAAO;SAAC,CAAC,CAAC;QAC3C,MAAM,qBAAqB,GAAG,IAAI,CAAC,uBAAuB,CAAC,WAAW,CAAC,CAAC;QAExE,oCAAoC;QACpC,MAAM,QAAQ,GAAG,IAAI,mTAAQ,EAAE,CAAC;QAChC,IAAI,MAAM,YAAY,kPAAM,EAAE;YAC5B,MAAM,GAAG;gBAAC,MAAM;aAAC,CAAC;SACnB;QACD,IAAI,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE;YACzB,IAAI,MAAM,CAAC,MAAM,KAAK,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE;gBACxC,MAAM,IAAI,ySAAU,CAChB,CAAA,+BAAA,EAAkC,MAAM,CAAC,MAAM,CAAA,EAAA,CAAI,GACnD,CAAA,kDAAA,CAAoD,GACpD,CAAA,CAAA,EAAI,IAAI,CAAC,MAAM,CAAC,MAAM,CAAA,EAAA,CAAI,CAAC,CAAC;aACjC;YACD,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBAC3C,QAAQ,CAAC,GAAG,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;aACzC;SACF,MAAM;YACL,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;gBAC/B,MAAM,WAAW,GAAG,MAAM,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;gBACvC,IAAI,WAAW,IAAI,IAAI,EAAE;oBACvB,MAAM,IAAI,ySAAU,CAChB,CAAA,2CAAA,EAA8C,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;iBACjE;gBACD,QAAQ,CAAC,GAAG,CAAC,KAAK,EAAE,WAAW,CAAC,CAAC;aAClC;SACF;QAED,iBAAiB;QACjB,MAAM,cAAc,OAAG,kTAAO,EAAC,qBAAqB,EAAE,QAAQ,CAAa,CAAC;QAC5E,OAAO,cAAc,CAAC,CAAC,CAAC,cAAc,CAAC,CAAC,CAAC,cAAc,CAAC,CAAC,CAAC,CAAC;IAC7D,CAAC;IAED;;OAEG,CACK,uBAAuB,CAAC,mBAA6B,EAAA;QAE3D,MAAM,qBAAqB,OACvB,2TAAY,EAAC,IAAI,EAAE,mBAAmB,CAAC,MAAM,CAAC,CAAC;QACnD,IAAI,gBAAgB,GAAG,mBAAmB,CAAC,MAAM,CAAC;QAClD,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;YAC/B,MAAM,YAAY,GACd,KAAK,CAAC,OAAO,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC;gBAAC,KAAK,CAAC,MAAM;aAAC,CAAC;YAChE,MAAM,gBAAgB,GAAG,YAAY,CAAC,GAAG,EAAC,MAAM,CAAC,EAAE,AAAC,MAAM,CAAC,IAAI,CAAC,CAAC;YACjE,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,mBAAmB,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBACnD,MAAM,KAAK,GAAG,gBAAgB,CAAC,OAAO,CAAC,mBAAmB,CAAC,CAAC,CAAC,CAAC,CAAC;gBAC/D,IAAI,KAAK,KAAK,CAAC,CAAC,EAAE;oBAChB,qBAAqB,CAAC,CAAC,CAAC,GAAG,YAAY,CAAC,KAAK,CAAC,CAAC;oBAC/C,gBAAgB,EAAE,CAAC;iBACpB;gBACD,IAAI,gBAAgB,KAAK,CAAC,EAAE;oBAC1B,MAAM;iBACP;aACF;YACD,IAAI,gBAAgB,KAAK,CAAC,EAAE;gBAC1B,MAAM;aACP;SACF;QAED,IAAI,gBAAgB,GAAG,CAAC,EAAE;YACxB,MAAM,cAAc,GAAa,EAAE,CAAC;YACpC,qBAAqB,CAAC,OAAO,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;gBAC1C,IAAI,MAAM,IAAI,IAAI,EAAE;oBAClB,cAAc,CAAC,IAAI,CAAC,mBAAmB,CAAC,CAAC,CAAC,CAAC,CAAC;iBAC7C;YACH,CAAC,CAAC,CAAC;YACH,MAAM,IAAI,ySAAU,CAChB,CAAA,gDAAA,CAAkD,GAClD,GAAG,IAAI,CAAC,SAAS,CAAC,cAAc,CAAC,EAAE,CAAC,CAAC;SAC1C;QACD,OAAO,qBAAqB,CAAC;IAC/B,CAAC;IAED;;;;;;;;;;;;OAYG,CACK,WAAW,CAAC,GAAoB,EAAE,SAAS,GAAG,EAAE,EAAE,OAAO,GAAG,KAAK,EAAA;QAEvE,OAAO,GAAG,CAAC,6OAAI,CAAC,GAAG,EAAE;YACnB,MAAM,UAAU,GAAG,IAAI,CAAC,eAAe,CAAC,GAAG,CAAC,CAAC;YAC7C,IAAI,OAAO,EAAE;gBACX,MAAM,IAAI,kTAAmB,CACzB,+CAA+C,CAAC,CAAC;aACtD;YAED,4BAA4B;YAC5B,wEAAwE;YACxE,qEAAqE;YACrE,gCAAgC;YAEhC,MAAM,OAAO,OAAG,8TAAW,EAAC,UAAU,EAAE,SAAS,CAAC,CAAC;YACnD,MAAM,WAAW,GAAe,IAAI,CAAC,OAAO,CAAC,GAAG,EAAC,MAAM,CAAC,EAAE,AAAC,EAAE,CAAC,CAAC;YAE/D,kEAAkE;YAClE,IAAK,IAAI,UAAU,GAAG,CAAC,EAAE,UAAU,GAAG,OAAO,CAAC,MAAM,EAAE,EAAE,UAAU,CAAE;gBAClE,MAAM,SAAS,GAAG,GAAG,CAAC,6OAAI,CAAC,GAAG,EAAE;oBAC9B,MAAM,UAAU,GAAG,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;oBAC1C,MAAM,QAAQ,GAAG,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;oBACxC,sEAAsE;oBACtE,mBAAmB;oBACnB,MAAM,QAAQ,OAAG,8TAAW,EAAC,GAAG,EAAE,UAAU,EAAE,QAAQ,CAAC,CAAC;oBAExD,qCAAqC;oBACrC,MAAM,KAAK,GAAG,EAAE,CAAC;oBACjB,IAAI,KAAK,CAAC,OAAO,CAAC,QAAQ,CAAC,EAAE;wBAC3B,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,QAAQ,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;4BACxC,KAAK,CAAC,IAAI,CAAC;gCAAC,GAAG,EAAE,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC;gCAAE,KAAK,EAAE,QAAQ,CAAC,CAAC,CAAC;4BAAA,CAAC,CAAC,CAAC;yBACvD;qBACF,MAAM;wBACL,KAAK,CAAC,IAAI,CAAC;4BAAC,GAAG,EAAE,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC;4BAAE,KAAK,EAAE,QAAQ;wBAAA,CAAC,CAAC,CAAC;qBACpD;oBACD,MAAM,QAAQ,GAAG,IAAI,mTAAQ,CAAC,KAAK,CAAC,CAAC;oBACrC,WAAO,kTAAO,EAAC,IAAI,CAAC,OAAO,EAAE,QAAQ,CAAa,CAAC;gBACrD,CAAC,CAAC,CAAC;gBACH,SAAS,CAAC,OAAO,CAAC,CAAC,QAAQ,EAAE,CAAC,EAAE,CAAG,CAAD,UAAY,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC,CAAC;aACnE;YACD,WAAO,+TAAgB,EACnB,WAAW,CAAC,GAAG,EAAC,OAAO,CAAC,EAAE,AAAC,GAAG,CAAC,qPAAM,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QAC1D,CAAC,CAAC,CAAC;IACL,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;OA0BG,CACH,OAAO,CAAC,CAAkB,EAAE,OAAyB,CAAA,CAAE,EAAA;QACrD,MAAM,eAAe,OAAG,6UAA0B,EAAC,CAAC,CAAC,CAAC;QACtD,cAAc,CACV,eAAe,EAAE,IAAI,CAAC,UAAU,EAAE,IAAI,CAAC,eAAe,EAAE,KAAK,CAAC,CAAC;QACnE,IAAI;YACF,4CAA4C;YAC5C,2BAA2B;YAC3B,4DAA4D;YAC5D,mCAAmC;YACnC,MAAM,SAAS,GAAG,IAAI,CAAC,SAAS,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,SAAS,CAAC;gBAC/D,iUAAc,EAAC,SAAS,CAAC,CAAC;YAC1B,OAAO,IAAI,CAAC,WAAW,CAAC,eAAe,EAAE,SAAS,CAAC,CAAC;SACrD,QAAS;gBACR,oUAAiB,EAAC,eAAe,EAAE,CAAC,CAAC,CAAC;SACvC;IACH,CAAC;IAED;;;;;;;;;;;;;;OAcG,CACH,cAAc,CAAC,CAAkB,EAAA;QAC/B,cAAc,CAAC,CAAC,EAAE,IAAI,CAAC,UAAU,EAAE,IAAI,CAAC,eAAe,EAAE,IAAI,CAAC,CAAC;QAC/D,4DAA4D;QAC5D,mCAAmC;QACnC,MAAM,SAAS,GAAG,CAAC,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;QACzD,OAAO,IAAI,CAAC,WAAW,CAAC,CAAC,EAAE,SAAS,CAAC,CAAC;IACxC,CAAC;IAES,qBAAqB,CAC3B,CAAgD,EAChD,CAAgD,EAAE,cAAc,GAAG,IAAI,EACvE,SAAkB,EAAA;QACpB,4CAA4C;QAC5C,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;YAC3B,MAAM,IAAI,2SAAY,CAClB,wDAAwD,GACxD,wCAAwC,CAAC,CAAC;SAC/C;QACD,MAAM,YAAY,GAAY,EAAE,CAAC;QACjC,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,gBAAgB,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YACrD,MAAM,WAAW,GAAG,IAAI,CAAC,gBAAgB,CAAC,CAAC,CAAC,CAAC;YAC7C,MAAM,MAAM,GAAG,IAAI,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;YACnC,IAAI,MAAM,KAAK,MAAM,CAAC,qTAA6B,EAAE;gBACnD,YAAY,CAAC,IAAI,CACb,WAAW,CAAC,KAAK,CAAC,CAAC,EAAE,WAAW,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,MAAM,CAAC;oBAAC,CAAC;iBAAC,CAAC,CAAC,CAAC;aAC/D,MAAM;gBACL,sEAAsE;gBACtE,YAAY,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;aAChC;SACF;QACD,CAAC,GAAG,oBAAoB,CACpB,CAAC,EAAE,IAAI,CAAC,cAAc,EAAE,IAAI,CAAC,eAAe,EAAE,KAAK,EAAE,OAAO,CAAC,CAAC;QAClE,CAAC,GAAG,oBAAoB,CACpB,CAAC,EAAE,IAAI,CAAC,eAAe,EAAE,YAAY,EAAE,KAAK,EAAE,QAAQ,CAAC,CAAC;QAC5D,wDAAwD;QACxD,iBAAiB,CAAC,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC,CAAC;QAC9B,2CAA2C;QAC3C,+BAA+B,CAAC,CAAC,EAAE,IAAI,CAAC,WAAW,EAAE,IAAI,CAAC,gBAAgB,CAAC,CAAC;QAC5E,IAAI,IAAI,CAAC,QAAQ,IAAI,SAAS,IAAI,IAAI,IAAI,SAAS,GAAG,CAAC,EAAE;YACvD,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,GAAG,SAAS,KAAK,CAAC,EAAE;gBACnC,MAAM,IAAI,ySAAU,CAChB,CAAA,0DAAA,CAA4D,GAC5D,CAAA,sDAAA,CAAwD,GACxD,GAAG,SAAS,CAAA,SAAA,EAAY,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAA,WAAA,CAAa,CAAC,CAAC;aACzD;SACF;QACD,OAAO;YAAC,CAAC;YAAE,CAAC;SAAC,CAAC;IAChB,CAAC;IAES,KAAK,CAAC,mBAAmB,CAC/B,CAAgD,EAChD,CAAgD,EAChD,YAA6D,EAC7D,WAAsD,EACtD,cAAc,GAAG,IAAI,EACrB,SAAkB,EAAA;QACpB,MAAM,CAAC,UAAU,EAAE,UAAU,CAAC,GAC1B,IAAI,CAAC,qBAAqB,CAAC,CAAC,EAAE,CAAC,EAAE,cAAc,EAAE,SAAS,CAAC,CAAC;QAChE,oCAAoC;QACpC,IAAI,YAAY,IAAI,IAAI,EAAE;YACxB,MAAM,IAAI,KAAK,CAAC,qCAAqC,CAAC,CAAC;SACxD;QAED,IAAI,qBAAqB,GAAa,IAAI,CAAC;QAC3C,IAAI,WAAW,IAAI,IAAI,EAAE;YACvB,MAAM,YAAY,OACd,wUAAuB,EAAC,WAAW,EAAE,IAAI,CAAC,WAAW,CAAC,CAAC;YAC3D,qBAAqB,GAAG,EAAE,CAAC;YAC3B,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,YAAY,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBAC5C,qBAAqB,CAAC,IAAI,CACtB,UAAM,mUAAkB,EAAC,UAAU,CAAC,CAAC,CAAC,EAAE,IAAI,EAAE,YAAY,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;aACrE;SACF;QAED,4DAA4D;QAC5D,OAAO;YAAC,UAAU;YAAE,UAAU;YAAE,qBAAqB;SAAC,CAAC;IACzD,CAAC;IAED;;;;;;;;;;OAUG,CACK,QAAQ,CACZ,CAA+B,EAAE,GAAa,EAAE,SAAkB,EAClE,OAAO,GAAG,CAAC,EAAE,KAAc,EAAA;QAC7B,OAAO,GAAG,CAAC,6OAAI,CAAC,GAAG,EAAE;YACnB,MAAM,UAAU,GAAG,IAAI,CAAC,eAAe,CAAC,GAAG,EAAE,SAAS,EAAE,KAAK,EAAE,OAAO,CAAC,CAAC;YACxE,MAAM,IAAI,GAAa,EAAE,CAAC;YAC1B,IAAI,OAAO,GAAG,CAAC,EAAE;gBACf,MAAM,IAAI,kTAAmB,CAAC,sCAAsC,CAAC,CAAC;aACvE;YACD,sEAAsE;YACtE,IAAI,KAAK,IAAI,IAAI,EAAE;gBACjB,MAAM,IAAI,kTAAmB,CACzB,iDAAiD,CAAC,CAAC;aACxD,MAAM;gBACL,MAAM,OAAO,OAAG,8TAAW,EAAC,UAAU,EAAE,SAAS,CAAC,CAAC;gBACnD,MAAM,UAAU,OAAG,6PAAQ,MAAC,iTAAK,EAAC,CAAC,EAAE,UAAU,CAAC,CAAC,CAAC;gBAClD,IAAK,IAAI,UAAU,GAAG,CAAC,EAAE,UAAU,GAAG,OAAO,CAAC,MAAM,EAAE,EAAE,UAAU,CAAE;oBAClE,MAAM,UAAU,GAAG,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;oBAC1C,MAAM,QAAQ,GAAG,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;oBACxC,MAAM,QAAQ,GACV,CAAC,CAAC,iUAAmB,CACjB,UAAU,EAAE,UAAU,EAAE,QAAQ,GAAG,UAAU,CAAa,CAAC;oBACnE,gEAAgE;oBAChE,sDAAsD;oBACtD,MAAM,QAAQ,OAAG,uUAAoB,EAAC,GAAG,EAAE,QAAQ,CAAa,CAAC;oBACjE,MAAM,SAAS,GAAG,CAAC,CAAC,QAAQ,CAAC,CAAC;oBAC9B,IAAI,UAAU,KAAK,CAAC,EAAE;wBACpB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;4BACzC,IAAI,CAAC,IAAI,KAAC,yPAAM,EAAC,CAAC,CAAC,CAAC,CAAC;yBACtB;qBACF;oBACD,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;wBACzC,MAAM,QAAQ,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;wBAC9B,IAAI,CAAC,CAAC,CAAC,GACH,GAAG,CAAC,+OAAG,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,GAAG,CAAC,+OAAG,CAAC,QAAQ,GAAG,UAAU,EAAE,QAAQ,CAAC,CAAC,CAAC;qBAChE;iBACF;gBACD,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;oBACpC,IAAI,CAAC,CAAC,CAAC,GAAG,GAAG,CAAC,+OAAG,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,UAAU,CAAC,CAAC;iBACxC;aACF;YACD,OAAO,IAAI,CAAC;QACd,CAAC,CAAC,CAAC;IACL,CAAC;IAES,sBAAsB,GAAA;QAC9B,MAAM,SAAS,GAAG,IAAI,CAAC,YAAY,CAAC;QACpC,mEAAmE;QACnE,oCAAoC;QACpC,MAAM,gBAAgB,GAAG,EAAE,CAAC;QAC5B,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YACzC,MAAM,KAAK,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;YAC3B,IAAI,QAAQ,GAAG,KAAK,CAAC;YACrB,QAAI,oTAAK,EAAC,SAAS,EAAE,KAAK,CAAC,GAAG,CAAC,EAAE;gBAC/B,MAAM,QAAQ,OAAG,oTAAK,EAAC,SAAS,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC;gBACrD,QAAQ,IAAI,CAAA,CAAA,EAAI,QAAQ,EAAE,CAAC;aAC5B;YACD,gBAAgB,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;SACjC;QACD,OAAO,gBAAgB,CAAC;IAC1B,CAAC;IAED;;;;;;;;;OASG,CACO,iBAAiB,GAAA;QACzB,OAAO,CAAC,IAAc,EAAE,EAAE;YACxB,MAAM,UAAU,GAAa,EAAE,CAAC;YAEhC,MAAM,MAAM,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE,IAAI,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;YACjD,MAAM,OAAO,GAAG,IAAI,CAAC,KAAK,CACtB,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,IAAI,CAAC,MAAM,CAAC,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC;YAClE,MAAM,aAAa,GAAG,IAAI,CAAC,KAAK,CAC5B,IAAI,CAAC,MAAM,CAAC,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EACxC,IAAI,CAAC,MAAM,CAAC,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YAElD,MAAM,aAAa,GAAa,EAAE,CAAC;YAEnC,8DAA8D;YAC9D,gEAAgE;YAChE,YAAY;YACZ,MAAM,iBAAiB,GAAG,GAAG,EAAE;gBAC7B,MAAM,KAAK,GAAG,EAAE,CAAC;gBACjB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;oBAC3C,KAAK,CAAC,IAAI,CAAC;wBAAC,GAAG,EAAE,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC;wBAAE,KAAK,EAAE,MAAM,CAAC,CAAC,CAAC;oBAAA,CAAC,CAAC,CAAC;iBACrD;gBACD,MAAM,QAAQ,GAAG,IAAI,mTAAQ,CAAC,KAAK,CAAC,CAAC;gBACrC,MAAM,OAAO,OACT,kTAAO,EAAC,IAAI,CAAC,OAAO,EAAE,QAAQ,EAAE;oBAAC,UAAU,EAAE,IAAI;gBAAA,CAAC,CAAa,CAAC;gBACpE,+DAA+D;gBAC/D,kBAAkB;gBAElB,IAAI,SAAiB,CAAC;gBACtB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;oBAClD,MAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;oBAC3C,IAAI,IAAI,GAAG,YAAY,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;oBAChD,IAAI,aAAa,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;wBAC5B,IAAI,OAAG,oUAAmB,EAAC,IAAI,EAAE,aAAa,CAAC,CAAC,CAAC,CAAC,CAAC;qBACpD;oBAED,mCAAmC;oBACnC,MAAM,QAAQ,GAAW,GAAG,CAAC,iPAAI,CAAC,IAAI,CAAC,CAAC;oBACxC,yDAAyD;oBACzD,UAAU,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;oBAC1B,IAAI,CAAC,KAAK,CAAC,EAAE;wBACX,SAAS,GAAG,IAAI,CAAC;qBAClB,MAAM;wBACL,SAAS,GAAG,GAAG,CAAC,+OAAG,CAAC,SAAS,EAAE,IAAI,CAAC,CAAC;qBACtC;iBACF;gBAED,uBAAuB;gBACvB,0DAA0D;gBAC1D,wCAAwC;gBACxC,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,cAAc,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;oBACnD,IAAI,cAAsB,CAAC;oBAE3B,IAAI,IAAI,CAAC,OAAO,CAAC,MAAM,GAAG,CAAC,IAAI,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE;wBACtD,cAAc,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC;qBAChC,MAAM;wBACL,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;wBACzC,MAAM,WAAW,GAAG,IAAI,CAAC,cAAc,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;wBAC9C,cAAc,GACV,GAAG,CAAC,iPAAI,CAAC,MAAM,CAAC,OAAO,CAAC,WAAW,CAAC,EAAE,OAAO,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;qBAClE;oBAED,GAAG,CAAC,6OAAI,CAAC,cAAc,CAAC,CAAC;oBACzB,yDAAyD;oBACzD,aAAa,CAAC,IAAI,CAAC,cAAc,CAAC,CAAC;iBACpC;gBAED,SAAS,GAAG,GAAG,CAAC,iPAAI,CAAC,SAAS,CAAC,CAAC;gBAEhC,6BAA6B;gBAC7B,IAAI,CAAC,eAAe,EAAE,CAAC,OAAO,EAAC,eAAe,CAAC,EAAE;oBAC/C,SAAS,GAAG,GAAG,CAAC,+OAAG,CAAC,SAAS,EAAE,eAAe,CAAC,CAAC;gBAClD,CAAC,CAAC,CAAC;gBAEH,OAAO,SAAmB,CAAC;YAC7B,CAAC,CAAC;YAEF,MAAM,SAAS,GAAG,IAAI,CAAC,yBAAyB,CAAC,GAAG,EAChD,KAAK,CAAC,EAAE,AAAC,KAAK,CAAC,IAAI,EAAkB,CAAC,CAAC;YAC3C,MAAM,UAAU,GAAG,IAAI,CAAC;YACxB,MAAM,cAAc,GAChB,IAAI,CAAC,UAAU,CAAC,QAAQ,CAAC,iBAAiB,EAAE,UAAU,EAAE,SAAS,CAAC,CAAC;YAEvE,OAAO;gBAAC,cAAc;aAAC,CAAC,MAAM,CAAC,aAAa,CAAC,CAAC;QAChD,CAAC,CAAC;IACJ,CAAC;IAED;;;;OAIG,CACK,gBAAgB,GAAA;QACtB,IAAI,CAAC,YAAY,GAAG,CAAC,IAAc,EAAE,EAAE;YACrC,OAAO,GAAG,CAAC,6OAAI,CAAC,GAAG,EAAE;gBACnB,MAAM,UAAU,GAAa,EAAE,CAAC;gBAChC,IAAI,SAAiB,CAAC;gBACtB,MAAM,MAAM,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE,IAAI,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;gBACjD,MAAM,OAAO,GAAG,IAAI,CAAC,KAAK,CACtB,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,IAAI,CAAC,MAAM,CAAC,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC;gBAClE,MAAM,KAAK,GAAG,EAAE,CAAC;gBACjB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;oBAC3C,KAAK,CAAC,IAAI,CAAC;wBAAC,GAAG,EAAE,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC;wBAAE,KAAK,EAAE,MAAM,CAAC,CAAC,CAAC;oBAAA,CAAC,CAAC,CAAC;iBACrD;gBACD,MAAM,QAAQ,GAAG,IAAI,mTAAQ,CAAC,KAAK,CAAC,CAAC;gBACrC,MAAM,OAAO,OAAG,kTAAO,EAAC,IAAI,CAAC,OAAO,EAAE,QAAQ,CAAa,CAAC;gBAC5D,sBAAsB;gBACtB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;oBAClD,MAAM,YAAY,GAAG,IAAI,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;oBAC3C,0DAA0D;oBAC1D,aAAa;oBACb,MAAM,IAAI,GAAW,GAAG,CAAC,iPAAI,CAAC,YAAY,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;oBACpE,IAAI,CAAC,KAAK,CAAC,EAAE;wBACX,SAAS,GAAG,IAAI,CAAC;qBAClB,MAAM;wBACL,SAAS,GAAG,GAAG,CAAC,+OAAG,CAAC,SAAS,EAAE,IAAI,CAAC,CAAC;qBACtC;oBACD,UAAU,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;iBAC5B;gBACD,uBAAuB;gBACvB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,cAAc,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;oBACnD,MAAM,MAAM,GAAG,IAAI,CAAC,cAAc,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;oBACzC,MAAM,WAAW,GAAG,IAAI,CAAC,cAAc,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;oBAC9C,iEAAiE;oBACjE,MAAM,UAAU,GACZ,GAAG,CAAC,iPAAI,CAAC,MAAM,CAAC,OAAO,CAAC,WAAW,CAAC,EAAE,OAAO,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC;oBACjE,UAAU,CAAC,IAAI,CAAC,UAAoB,CAAC,CAAC;iBACvC;gBACD,OAAO,UAAU,CAAC;YACpB,CAAC,CAAC,CAAC;QACL,CAAC,CAAC;IACJ,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OAiCG,CACH,KAAK,CAAC,GAAG,CACL,CAAgD,EAChD,CAAgD,EAChD,OAAqB,CAAA,CAAE,EAAA;QACzB,IAAI,IAAI,CAAC,UAAU,EAAE;YACnB,MAAM,IAAI,KAAK,CACX,8DAA8D,CAAC,CAAC;SACrE;QACD,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC;QACvB,IAAI,MAAgB,CAAC;QACrB,IAAI,OAAiB,CAAC;QACtB,IAAI,cAAwB,CAAC;QAC7B,IAAI,eAAyB,CAAC;QAC9B,IAAI,SAA0B,CAAC;QAC/B,IAAI,SAA0B,CAAC;QAC/B,IAAI,IAAqB,CAAC;QAC1B,IAAI,IAAqB,CAAC;QAC1B,IAAI,aAAuB,CAAC;QAC5B,IAAI;YACF,MAAM,SAAS,GAAG,IAAI,CAAC,SAAS,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,SAAS,CAAC;gBAC/D,iUAAc,EAAC,SAAS,CAAC,CAAC;YAE1B,sBAAsB;YACtB,oCAAoC;YACpC,MAAM,cAAc,GAAG,KAAK,CAAC;YAC7B,MAAM,gBAAgB,GAClB,MAAM,IAAI,CAAC,mBAAmB,CAC1B,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC,YAAY,EAAE,IAAI,CAAC,WAAW,EAAE,cAAc,EACzD,SAAS,CAAmC,CAAC;YACrD,MAAM,GAAG,gBAAgB,CAAC,CAAC,CAAC,CAAC;YAC7B,OAAO,GAAG,gBAAgB,CAAC,CAAC,CAAC,CAAC;YAC9B,aAAa,GAAG,gBAAgB,CAAC,CAAC,CAAC,CAAC;YAEpC,2BAA2B;YAC3B,IAAI,YAAY,GAAG,KAAK,CAAC;YACzB,IAAI,MAAgB,CAAC;YACrB,IAAI,IAAI,CAAC,cAAc,IAAI,IAAI,IAAI,IAAI,CAAC,cAAc,CAAC,MAAM,GAAG,CAAC,EAAE;gBACjE,YAAY,GAAG,IAAI,CAAC;gBACpB,IAAI,IAAI,CAAC,cAAc,CAAC,MAAM,KAAK,CAAC,EAAE;oBACpC,mDAAmD;oBACnD,SAAS,GAAG,IAAI,CAAC,cAAc,CAAC,CAAC,CAAC,CAAC;oBACnC,SAAS,GAAG,IAAI,CAAC,cAAc,CAAC,CAAC,CAAC,CAAC;iBACpC,MAAM,IAAI,IAAI,CAAC,cAAc,CAAC,MAAM,KAAK,CAAC,EAAE;oBAC3C,MAAM,IAAI,kTAAmB,CACzB,+DAA+D,CAAC,CAAC;iBACtE,MAAM;oBACL,MAAM,IAAI,ySAAU,CAChB,CAAA,6DAAA,CAA+D,GAC/D,CAAA,0CAAA,CAA4C,GAC5C,GAAG,IAAI,CAAC,cAAc,CAAA,YAAA,CAAc,CAAC,CAAC;iBAC3C;gBAED,MAAM,cAAc,GAAG,IAAI,CAAC;gBAC5B,MAAM,eAAe,GACjB,MAAM,IAAI,CAAC,mBAAmB,CAC1B,SAAS,EAAE,SAAS,EAAE,IAAI,EAAE,2BAAA,EAA6B,CACzD,IAAI,EAAwB,0BAAA,EAA4B,CACxD,cAAc,EAAE,SAAS,CAAmC,CAAC;gBACrE,IAAI,GAAG,eAAe,CAAC,CAAC,CAAC,CAAC;gBAC1B,IAAI,GAAG,eAAe,CAAC,CAAC,CAAC,CAAC;gBAC1B,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC;YAC3B,kDAAkD;aACnD,MAAM,IACH,IAAI,CAAC,eAAe,IAAI,IAAI,IAAI,IAAI,CAAC,eAAe,GAAG,CAAC,IACxD,IAAI,CAAC,eAAe,GAAG,CAAC,EAAE;gBAC5B,YAAY,GAAG,IAAI,CAAC;gBACpB,8DAA8D;gBAC9D,MAAM,OAAO,GACT,IAAI,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC,eAAe,CAAC,CAAC,CAAC;gBAChE,MAAM,iBAAiB,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;gBAC7C,IAAI,OAAG,8TAAW,EAAC,MAAM,EAAE,OAAO,EAAE,iBAAiB,CAAa,CAAC;gBACnE,cAAc,GAAG,MAAM,CAAC;gBACxB,MAAM,OAAG,8TAAW,EAAC,MAAM,EAAE,CAAC,EAAE,OAAO,CAAa,CAAC;gBACrD,IAAI,OAAG,8TAAW,EAAC,OAAO,EAAE,OAAO,EAAE,iBAAiB,CAAa,CAAC;gBACpE,eAAe,GAAG,OAAO,CAAC;gBAC1B,OAAO,OAAG,8TAAW,EAAC,OAAO,EAAE,CAAC,EAAE,OAAO,CAAa,CAAC;gBACvD,oEAAoE;gBACpE,sBAAsB;gBACtB,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC;YAE3B,kDAAkD;aACnD,MAAM,IAAI,IAAI,CAAC,eAAe,IAAI,IAAI,EAAE;gBACvC,YAAY,GAAG,IAAI,CAAC;YACpB,oCAAoC;aACrC;YAED,MAAM,GAAG,GAAG,MAAM,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC,MAAM,CAAC,aAAa,CAAC,CAAC;YAEzD,IAAI,CAAC,gCAAgC,EAAE,CAAC;YAExC,4DAA4D;YAE5D,gEAAgE;YAChE,SAAS;YACT,qEAAqE;YACrE,iEAAiE;YACjE,qEAAqE;YACrE,sEAAsE;YACtE,mEAAmE;YACnE,mEAAmE;YACnE,iDAAiD;YACjD,2BAA2B;YAC3B,MAAM,aAAa,GAAG,IAAI,CAAC,iBAAiB,EAAE,CAAC;YAC/C,MAAM,SAAS,GAAG,IAAI,CAAC,sBAAsB,EAAE,CAAC;YAEhD,IAAI,WAAyC,CAAC;YAC9C,IAAI,eAAyB,CAAC;YAC9B,IAAI,YAAY,EAAE;gBAChB,IAAI,CAAC,gBAAgB,EAAE,CAAC;gBACxB,WAAW,GAAG,IAAI,CAAC,YAAY,CAAC;gBAChC,eAAe,GACX,SAAS,CAAC,KAAK,EAAE,CAAC,MAAM,CAAC,SAAS,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;aAC9D,MAAM;gBACL,WAAW,GAAG,IAAI,CAAC;gBACnB,MAAM,GAAG,EAAE,CAAC;gBACZ,eAAe,GAAG,SAAS,CAAC,KAAK,EAAE,CAAC;aACrC;YAED,MAAM,SAAS,OAAG,2TAAoB,EAAC,IAAI,CAAC,SAAS,EAAE,IAAI,CAAC,UAAU,CAAC,CAAC;YACxE,MAAM,GAAG,GAAG,MAAM,IAAI,CAAC,OAAO,CAC1B,aAAa,EAAE,GAAG,EAAE,SAAS,EAAE,SAAS,EAAE,IAAI,CAAC,MAAM,EACrD,IAAI,CAAC,OAAO,EAAE,SAAS,EAAE,WAAW,EAAE,MAAM,EAAE,IAAI,CAAC,OAAO,EAC1D,eAAe,EAAE,IAAI,CAAC,YAAY,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;YACpD,OAAO,GAAG,CAAC;SACZ,QAAS;YACR,IAAI,CAAC,UAAU,GAAG,KAAK,CAAC;YACxB,mBAAmB;gBACnB,oUAAiB,EAAC,MAAM,EAAE,CAAC,CAAC,CAAC;gBAC7B,oUAAiB,EAAC,OAAO,EAAE,CAAC,CAAC,CAAC;gBAC9B,oUAAiB,EAAC,cAAc,EAAE,CAAC,CAAC,CAAC;gBACrC,oUAAiB,EAAC,eAAe,EAAE,CAAC,CAAC,CAAC;gBACtC,oUAAiB,EAAC,IAAgB,EAAE,SAAS,CAAC,CAAC;gBAC/C,oUAAiB,EAAC,IAAgB,EAAE,SAAS,CAAC,CAAC;YAC/C,IAAI,aAAa,IAAI,IAAI,EAAE;gBACzB,GAAG,CAAC,gPAAO,CAAC,aAAa,CAAC,CAAC;aAC5B;SACF;IACD,sCAAsC;IACxC,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;OA0BG,CACH,KAAK,CAAC,OAAO,CACT,CAA+B,EAAE,GAAa,EAAE,SACxC,EAAE,SAAkB,EAAE,MAAe,EAAE,OAAgB,EAC/D,SAA0B,EAAE,IAAmC,EAAE,MACzD,EAAE,OAAwB,EAAE,eAA0B,EAC9D,YAAqB,EAAE,aAAsB,EAAE,eAAwB,EAAA;QAEzE,IAAI,SAAS,IAAI,IAAI,EAAE;YACrB,SAAS,GAAG,EAAE,CAAC;SAChB;QACD,IAAI,MAAM,IAAI,IAAI,EAAE;YAClB,MAAM,GAAG,CAAC,CAAC;SACZ;QACD,IAAI,OAAO,IAAI,IAAI,EAAE;YACnB,OAAO,GAAG,IAAI,CAAC;SAChB;QACD,IAAI,YAAY,IAAI,IAAI,EAAE;YACxB,YAAY,GAAG,CAAC,CAAC;SAClB;QAED,sEAAsE;QACtE,IAAI,YAAY,GAAG,KAAK,CAAC;QACzB,IAAI,IAAI,IAAI,IAAI,IAAI,MAAM,IAAI,IAAI,EAAE;YAClC,YAAY,GAAG,IAAI,CAAC;QACpB,+BAA+B;SAChC;QACD,IAAI,eAAe,IAAI,IAAI,EAAE;YAC3B,YAAY,GAAG,IAAI,CAAC;YACpB,IAAI,aAAa,IAAI,IAAI,EAAE;gBACzB,MAAM,IAAI,ySAAU,CAChB,gEAAgE,GAChE,oCAAoC,CAAC,CAAC;aAC3C;SACF;QAED,MAAM,eAAe,GACjB,IAAI,CAAC,eAAe,CAAC,GAAG,EAAE,SAAS,EAAE,aAAa,EAAE,iBAAiB,CAAC,CAAC;QAC3E,IAAI,UAAoB,CAAC;QACzB,IAAI,eAAe,IAAI,IAAI,EAAE;YAC3B,UAAU,OAAG,iTAAK,EAAC,CAAC,EAAE,eAAe,CAAC,CAAC;SACxC;QAED,IAAI,OAAO,IAAI,IAAI,EAAE;YACnB,OAAO,GAAG,CAAC,CAAC;SACb;QAED,MAAM,EAAC,YAAY,EAAE,OAAO,EAAC,OAAG,yTAAkB,EAC9C,SAAS,EAAE,OAAO,EAAE,MAAM,EAAE,YAAY,EAAE,eAAe,EACzD,aAAa,EAAE,SAAS,EAAE,YAAY,EAAE,eAAe,CAAC,CAAC;QAC7D,YAAY,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC;QAC5B,IAAI,CAAC,OAAO,GAAG,OAAO,CAAC;QACvB,MAAM,YAAY,CAAC,YAAY,EAAE,CAAC;QAClC,IAAI,CAAC,aAAa,GAAG,KAAK,CAAC;QAC3B,oEAAoE;QACpE,+DAA+D;QAE/D,IAAK,IAAI,KAAK,GAAG,YAAY,EAAE,KAAK,GAAG,MAAM,EAAE,EAAE,KAAK,CAAE;YACtD,MAAM,YAAY,CAAC,YAAY,CAAC,KAAK,CAAC,CAAC;YACvC,MAAM,SAAS,GAAmB,CAAA,CAAE,CAAC;YACrC,IAAI,aAAa,IAAI,IAAI,EAAE;gBACzB,MAAM,IAAI,kTAAmB,CACzB,4CAA4C,CAAC,CAAC;aACnD,MAAM;gBACL,IAAI,OAAO,KAAK,OAAO,EAAE;oBACvB,MAAM,IAAI,kTAAmB,CAAC,oCAAoC,GAClC,MAAM,CAAC,CAAC;iBACzC,MAAM,IAAI,OAAO,EAAE;oBAClB,8QAAI,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC;iBAC1B;gBACD,qEAAqE;gBACrE,kDAAkD;gBAClD,MAAM,iBAAiB,OAAG,6PAAQ,EAAC,UAAU,CAAC,CAAC;gBAE/C,MAAM,OAAO,OAAG,8TAAW,EAAC,eAAe,EAAE,SAAS,CAAC,CAAC;gBACxD,IAAK,IAAI,UAAU,GAAG,CAAC,EAAE,UAAU,GAAG,OAAO,CAAC,MAAM,EAAE,EAAE,UAAU,CAAE;oBAClE,MAAM,SAAS,GAAmB,CAAA,CAAE,CAAC;oBACrC,MAAM,YAAY,CAAC,YAAY,CAAC,UAAU,EAAE,SAAS,CAAC,CAAC;oBAEvD,GAAG,CAAC,6OAAI,CAAC,GAAG,EAAE;wBACZ,MAAM,UAAU,GAAG,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;wBAC1C,MAAM,QAAQ,GAAG,OAAO,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;wBACxC,MAAM,QAAQ,GAAG,CAAC,CAAC,iUAAmB,CACjB,iBAAiB,EAAE,UAAU,EAC7B,QAAQ,GAAG,UAAU,CAAa,CAAC;wBACxD,SAAS,CAAC,OAAO,CAAC,GAAG,UAAU,CAAC;wBAChC,SAAS,CAAC,MAAM,CAAC,GAAG,QAAQ,GAAG,UAAU,CAAC;wBAE1C,gEAAgE;wBAChE,sDAAsD;wBACtD,MAAM,QAAQ,OAAG,uUAAoB,EAAC,GAAG,EAAE,QAAQ,CAAa,CAAC;wBACjE,MAAM,IAAI,GAAG,CAAC,CAAC,QAAQ,CAAC,CAAC;wBACzB,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;4BACzC,MAAM,KAAK,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;4BAC3B,MAAM,GAAG,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;4BACpB,SAAS,CAAC,KAAK,CAAC,GAAG,GAAG,CAAC;4BACvB,GAAG,CAAC,6OAAI,CAAC,GAAG,CAAC,CAAC;wBACd,8CAA8C;yBAC/C;wBAED,IAAI,UAAU,KAAK,OAAO,CAAC,MAAM,GAAG,CAAC,EAAE,EAAG,cAAc;4BACtD,IAAI,YAAY,EAAE;gCAChB,MAAM,OAAO,GAAG,IAAI,CAAC,QAAQ,CAAC,IAAI,EAAE,MAAM,EAAE,SAAS,CAAC,CAAC;gCACvD,6DAA6D;gCAC7D,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;oCACzC,MAAM,KAAK,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC;oCAC3B,MAAM,GAAG,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;oCACvB,GAAG,CAAC,6OAAI,CAAC,GAAG,CAAC,CAAC;oCACd,8CAA8C;oCAC9C,SAAS,CAAC,MAAM,GAAG,KAAK,CAAC,GAAG,GAAG,CAAC;iCACjC;6BACF;yBACF;oBACH,CAAC,CAAC,CAAC;oBAEH,MAAM,YAAY,CAAC,UAAU,CAAC,UAAU,EAAE,SAAS,CAAC,CAAC;wBACrD,iTAAoB,EAAC,SAAS,CAAC,CAAC;oBAEhC,IAAI,IAAI,CAAC,aAAa,EAAE;wBACtB,MAAM;qBACP;gBACD,6CAA6C;iBAC9C;gBAED,iBAAiB,CAAC,OAAO,EAAE,CAAC;aAC7B;YACD,sDAAsD;YACtD,MAAM,YAAY,CAAC,UAAU,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC;YAChD,IAAI,IAAI,CAAC,aAAa,EAAE;gBACtB,MAAM;aACP;SACF;QACD,MAAM,YAAY,CAAC,UAAU,EAAE,CAAC;QAEhC,MAAM,IAAI,CAAC,OAAO,CAAC,QAAQ,EAAE,CAAC;QAC9B,OAAO,IAAI,CAAC,OAAO,CAAC;IACtB,CAAC;IAED,uEAAuE;IACvE,4BAA4B;IAC5B;;;;;;;;;;;;;;;;;;;;OAoBG,CACH,KAAK,CAAC,UAAU,CAAI,OAAmB,EAAE,IAA4B,EAAA;QAEnE,WAAO,6TAAU,EAAC,IAAI,EAAE,OAAO,EAAE,IAAI,CAAC,CAAC;IACzC,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;OAsBG,CACH,KAAK,CAAC,YAAY,CACd,CAAgD,EAChD,CAC6B,EAAA;QAC/B,oDAAoD;QACpD,uCAAuC;QACvC,MAAM,cAAc,GAAG,MAAM,IAAI,CAAC,mBAAmB,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAC5D,MAAM,MAAM,GAAG,cAAc,CAAC,CAAC,CAAC,CAAC;QACjC,MAAM,OAAO,GAAG,cAAc,CAAC,CAAC,CAAC,CAAC;QAClC,MAAM,aAAa,GAAG,IAAI,CAAC,iBAAiB,EAAE,CAAC;QAC/C,MAAM,MAAM,GAAG,aAAa,CAAC,MAAM,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC,CAAC;QACrD,MAAM,UAAU,GAAa,EAAE,CAAC;QAChC,KAAK,MAAM,IAAI,IAAI,MAAM,CAAE;YACzB,MAAM,CAAC,GAAG,MAAM,IAAI,CAAC,IAAI,EAAE,CAAC;YAC5B,UAAU,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;SACvB;QACD,GAAG,CAAC,gPAAO,CAAC,MAAM,CAAC,CAAC;YACpB,oUAAiB,EAAC,cAAc,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YACxC,oUAAiB,EAAC,cAAc,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QACxC,WAAO,+TAAgB,EAAC,UAAU,CAAC,CAAC;IACtC,CAAC;IAED;;;;;;;;OAQG,CACO,eAAe,CAAC,MAAsB,EAAA;QAC9C,MAAM,YAAY,GAAkB,EAAE,CAAC;QAEvC,MAAM,aAAa,GAAG,MAAM,IAAI,IAAI,IAAI,MAAM,CAAC,aAAa,CAAC;QAC7D,MAAM,OAAO,GAAG,aAAa,CAAC,CAAC,CAAC,IAAI,CAAC,gBAAgB,CAAC,CAAC,CAAC,IAAI,CAAC,OAAO,CAAC;QACrE,MAAM,YAAY,GAAG,IAAI,CAAC,UAAU,CAAC,aAAa,CAAC,CAAC;QACpD,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,OAAO,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YACvC,IAAI,aAAa,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,SAAS,EAAE;gBAE1C,SAAS;aACV;YACD,YAAY,CAAC,IAAI,CACb;gBAAC,IAAI,EAAE,OAAO,CAAC,CAAC,CAAC,CAAC,YAAY;gBAAE,MAAM,EAAE,YAAY,CAAC,CAAC,CAAC;YAAA,CAAC,CAAC,CAAC;SAC/D;QACD,OAAO,YAAY,CAAC;IACtB,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OA6BG,CACH,IAAI,YAAY,CAAC,IAAa,EAAA;QAC5B,IAAI,CAAC,aAAa,GAAG,IAAI,CAAC;IAC5B,CAAC;IAED,IAAI,YAAY,GAAA;QACd,OAAO,IAAI,CAAC,aAAa,CAAC;IAC5B,CAAC;IAED,IAAI,SAAS,GAAA;QACX,OAAO,IAAI,CAAC,UAAU,CAAC;IACzB,CAAC;IAED,IAAI,SAAS,CAAC,SAAoB,EAAA;QAChC,IAAI,IAAI,CAAC,UAAU,KAAK,SAAS,EAAE;YACjC,IAAI,CAAC,UAAU,GAAG,SAAS,CAAC;YAC5B,IAAI,CAAC,gBAAgB,GAAG,KAAK,CAAC;SAC/B;IACH,CAAC;IAEQ,OAAO,GAAA;QACd,MAAM,MAAM,GAAG,KAAK,CAAC,OAAO,EAAE,CAAC;QAC/B,IAAI,MAAM,CAAC,oBAAoB,KAAK,CAAC,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,IAC3D,IAAI,CAAC,gBAAgB,EAAE;YACzB,MAAM,gCAAgC,GAAG,GAAG,CAAC,+OAAM,EAAE,CAAC,UAAU,CAAC;YACjE,IAAI,CAAC,UAAU,CAAC,OAAO,EAAE,CAAC;YAC1B,MAAM,CAAC,oBAAoB,IACvB,gCAAgC,GAAG,GAAG,CAAC,+OAAM,EAAE,CAAC,UAAU,CAAC;SAChE;QACD,OAAO,MAAM,CAAC;IAChB,CAAC;IAEO,kBAAkB,GAAA;QAExB,IAAI,SACsC,CAAC;QAC3C,IAAI,OAAO,IAAI,CAAC,IAAI,KAAK,QAAQ,EAAE;YACjC,SAAS,OAAG,0TAAW,EAAC,IAAI,CAAC,IAAI,CAAmB,CAAC;SACtD,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE;YACnC,KAAK,MAAM,IAAI,IAAI,IAAI,CAAC,IAAI,CAAE;gBAC5B,IAAI,OAAO,IAAI,KAAK,QAAQ,EAAE;oBAC5B,MAAM,IAAI,KAAK,CAAC,oDAAoD,CAAC,CAAC;iBACvE;aACF;YACD,SAAS,GAAI,IAAI,CAAC,IAAiB,CAAC,GAAG,EAAC,IAAI,CAAC,EAAE,IAAC,0TAAW,EAAC,IAAI,CAAC,CAC7C,CAAC;SACtB,MAAM;YACL,MAAM,WAAW,GAAG,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YAC3C,SAAS,GAAG,CAAA,CAA4C,CAAC;YACzD,MAAM,MAAM,GACR,IAAI,CAAC,IAAuD,CAAC;YACjE,KAAK,MAAM,UAAU,IAAI,WAAW,CAAE;gBACpC,IAAI,OAAO,MAAM,CAAC,UAAU,CAAC,KAAK,QAAQ,EAAE;oBAC1C,SAAS,CAAC,UAAU,CAAC,OACjB,0TAAW,EAAC,MAAM,CAAC,UAAU,CAAW,CAAmB,CAAC;iBACjE,MAAM;oBACL,MAAM,IAAI,KAAK,CAAC,oDAAoD,CAAC,CAAC;iBACvE;aACF;SACF;QACD,OAAO,SAAS,CAAC;IACnB,CAAC;IAEO,oBAAoB,GAAA;QAE1B,IAAI,OAAO,IAAI,CAAC,OAAO,KAAK,QAAQ,IAChC,OAAO,IAAI,CAAC,OAAO,KAAK,UAAU,EAAE;YACtC,OAAO;oBAAC,0TAAW,EAAC,OAAO,CAAC,2SAAmB,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;aAAC,CAAC;SACjE,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,OAAO,CAAC,EAAE;YACtC,OAAO,IAAI,CAAC,OAAO,CAAC,GAAG,EACnB,MAAM,CAAC,EAAE,IAAC,0TAAW,EAAC,OAAO,CAAC,2SAAmB,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;SACjE,MAAM;YACL,MAAM,kBAAkB,GAAuC,CAAA,CAAE,CAAC;YAClE,IAAK,MAAM,GAAG,IAAI,IAAI,CAAC,OAAO,CAAE;gBAC9B,kBAAkB,CAAC,GAAG,CAAC,OACnB,0TAAW,EAAC,OAAO,CAAC,2SAAmB,CAAC,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;aACjE;YACD,OAAO,kBAAkB,CAAC;SAC3B;IACH,CAAC;IAES,iBAAiB,GAAA;QACzB,OAAO;YACL,IAAI,EAAE,IAAI,CAAC,kBAAkB,EAAE;YAC/B,OAAO,EAAE,IAAI,CAAC,oBAAoB,EAAE;YACpC,gBAAgB,EAAE;gBAChB,UAAU,EAAE,IAAI,CAAC,SAAS,CAAC,YAAY,EAAE;gBACzC,MAAM,EAAE,IAAI,CAAC,SAAS,CAAC,SAAS,EAAE;aACT;SAC5B,CAAC;IACF,0DAA0D;IAC1D,0DAA0D;IAC1D,oDAAoD;IACtD,CAAC;IAED,kBAAkB,CAAC,cAA8B,EAAA;QAC/C,IAAI,cAAc,CAAC,gBAAgB,IAAI,IAAI,EAAE;YAC3C,MAAM,IAAI,KAAK,CAAC,8CAA8C,CAAC,CAAC;SACjE;QACD,IAAI,cAAc,CAAC,YAAY,IAAI,IAAI,EAAE;YACvC,MAAM,IAAI,KAAK,CAAC,4CAA4C,CAAC,CAAC;SAC/D;QACD,IAAI,cAAc,CAAC,kBAAkB,IAAI,IAAI,EAAE;YAC7C,MAAM,IAAI,KAAK,CAAC,kDAAkD,CAAC,CAAC;SACrE;QAED,MAAM,QAAQ,OAAG,wUAAmB,EAAC,cAAc,CAAC,gBAAgB,CACxC,CAAC;QAC7B,MAAM,SAAS,OAAG,2TAAW,EAAC,QAAQ,CAAc,CAAC;QAErD,IAAI,IAAI,CAAC;QACT,IAAI,OAAO,cAAc,CAAC,IAAI,KAAK,QAAQ,EAAE;YAC3C,IAAI,OAAG,0TAAW,EAAC,cAAc,CAAC,IAAI,CAAC,CAAC;SACzC,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,cAAc,CAAC,IAAI,CAAC,EAAE;YAC7C,IAAI,GAAG,cAAc,CAAC,IAAI,CAAC,GAAG,EAAC,SAAS,CAAC,EAAE,IAAC,0TAAW,EAAC,SAAS,CAAC,CAAC,CAAC;SACrE,MAAM,IAAI,cAAc,CAAC,IAAI,IAAI,IAAI,EAAE;YACtC,IAAI,GAAG,CAAA,CAA4C,CAAC;YACpD,IAAK,MAAM,GAAG,IAAI,cAAc,CAAC,IAAI,CAAE;gBACrC,IAAI,CAAC,GAAG,CAAC,OAAG,0TAAW,EAAC,cAAc,CAAC,IAAI,CAAC,GAAG,CAAC,CAAmB,CAAC;aACrE;SACF;QAED,IAAI,OAAO,CAAC;QACZ,IAAI,KAAK,CAAC,OAAO,CAAC,cAAc,CAAC,OAAO,CAAC,EAAE;YACzC,OAAO,GAAG,cAAc,CAAC,OAAO,CAAC,GAAG,EAAC,MAAM,CAAC,EAAE,IAAC,0TAAW,EAAC,MAAM,CAAC,CAAC,CAAC;SACrE,MAAM,IAAI,cAAc,CAAC,OAAO,IAAI,IAAI,EAAE;YACzC,OAAO,GAAG,CAAA,CAA+C,CAAC;YAC1D,IAAK,MAAM,GAAG,IAAI,cAAc,CAAC,OAAO,CAAE;gBACxC,OAAO,CAAC,GAAG,CAAC,OAAG,0TAAW,EAAC,cAAc,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC;aACzD;SACF;QAED,IAAI,CAAC,OAAO,CAAC;YAAC,IAAI;YAAE,OAAO;YAAE,SAAS;QAAA,CAAC,CAAC,CAAC;IAC3C,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OAgFG,CACH,KAAK,CAAC,IAAI,CAAC,YAAiC,EAAE,MAAsB,EAAA;QAElE,IAAI,OAAO,YAAY,KAAK,QAAQ,EAAE;YACpC,MAAM,QAAQ,GAAG,8QAAE,CAAC,eAAe,CAAC,YAAY,CAAC,CAAC;YAClD,IAAI,QAAQ,CAAC,MAAM,KAAK,CAAC,EAAE;gBACzB,MAAM,IAAI,ySAAU,CAChB,CAAA,uCAAA,EAA0C,YAAY,CAAA,CAAA,CAAG,CAAC,CAAC;aAChE,MAAM,IAAI,QAAQ,CAAC,MAAM,GAAG,CAAC,EAAE;gBAC9B,MAAM,IAAI,ySAAU,CAChB,CAAA,qBAAA,EAAwB,QAAQ,CAAC,MAAM,CAAA,oBAAA,CAAsB,GAC7D,CAAA,KAAA,EAAQ,YAAY,CAAA,CAAA,CAAG,CAAC,CAAC;aAC9B;YACD,YAAY,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC;SAC5B;QACD,IAAI,YAAY,CAAC,IAAI,IAAI,IAAI,EAAE;YAC7B,MAAM,IAAI,ySAAU,CAChB,0DAA0D,GAC1D,sDAAsD,CAAC,CAAC;SAC7D;QAED,MAAM,kBAAkB,GACpB,MAAM,8QAAE,CAAC,aAAa,CAAC,IAAI,CAAC,eAAe,CAAC,MAAM,CAAC,CAAC,CAAC;QAEzD,MAAM,YAAY,GAAG,KAAK,CAAC;QAC3B,MAAM,SAAS,GAAO,IAAI,CAAC;QAC3B,MAAM,WAAW,GAAG,IAAI,CAAC,MAAM,CAAC,SAAS,EAAE,YAAY,CAAC,CAAC;QACzD,MAAM,cAAc,GAAsB;YACxC,aAAa,EAAE,WAAW;YAC1B,MAAM,EAAE,wBAAwB;YAChC,WAAW,EAAE,CAAA,2BAAA,EAA8B,uSAAO,EAAE;YACpD,WAAW,EAAE,IAAI;SAClB,CAAC;QAEF,MAAM,gBAAgB,GAAG,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,MAAM,CAAC,gBAAgB,CAAC;QAC1E,IAAI,gBAAgB,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;YAC9C,cAAc,CAAC,cAAc,GAAG,IAAI,CAAC,iBAAiB,EAAE,CAAC;YACzD,MAAM,UAAU,GAAG,WAAW,CAAC;YAC/B,MAAM,EAAC,IAAI,EAAE,mBAAmB,EAAE,KAAK,EAAE,oBAAoB,EAAC,GAC1D,MAAM,8QAAE,CAAC,aAAa,CAAC,MAAM,IAAI,CAAC,SAAS,CAAC,UAAU,EAAE,EAAE,UAAU,CAAC,CAAC;YAC1E,kBAAkB,CAAC,KAAK,CAAC,IAAI,CAAC,GAAG,oBAAoB,CAAC,CAAC;YACvD,kBAAkB,CAAC,IAAI,GAAG,8QAAE,CAAC,uBAAuB,CAChD;gBAAC,kBAAkB,CAAC,IAAI;gBAAE,mBAAmB;aAAC,CAAC,CAAC;SACrD;QAED,IAAI,IAAI,CAAC,mBAAmB,IAAI,IAAI,EAAE;YACpC,kDAAkD;YAClD,MAAM,SAAS,GAAG,IAAI,CAAC;gBACvB,sUAAwB,EAAC,IAAI,CAAC,mBAAmB,EAAE,IAAI,CAAC,IAAI,EAAE,SAAS,CAAC,CAAC;YACzE,cAAc,CAAC,mBAAmB,GAAG,IAAI,CAAC,mBAAmB,CAAC;SAC/D;QAED,cAAc,CAAC,UAAU,GAAG,kBAAkB,CAAC,IAAI,CAAC;QACpD,cAAc,CAAC,WAAW,GAAG,kBAAkB,CAAC,KAAK,CAAC;QACtD,OAAO,YAAY,CAAC,IAAI,CAAC,cAAc,CAAC,CAAC;IAC3C,CAAC;IAED;;;;;;;OAOG,CACH,sBAAsB,CAAC,mBAAuB,EAAA;YAC5C,sUAAwB,EAAC,mBAAmB,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;QACzD,IAAI,CAAC,mBAAmB,GAAG,mBAAmB,CAAC;IACjD,CAAC;IAED;;;;;;;;;;OAUG,CACH,sBAAsB,GAAA;QACpB,OAAO,IAAI,CAAC,mBAAmB,CAAC;IAClC,CAAC;;AAtrDD,oEAAoE;AACpE,4EAA4E;AAC5E,gBAAA,EAAkB,CACX,YAAA,SAAS,GAAG,OAAO,CAAC;;AAqrD7B,ySAAa,CAAC,aAAa,CAAC,WAAW,CAAC,CAAC;AAEzC;;;;;GAKG,CACH,oDAAA,EAAsD,CACtD,MAAa,UAAW,SAAQ,WAAW;;AACzB,WAAA,SAAS,GAAG,YAAY,CAAC;;AAE3C,ySAAa,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC"}},
    {"offset": {"line": 5268, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/.pnpm/@tensorflow+tfjs-layers@4.22.0_@tensorflow+tfjs-core@4.22.0/node_modules/@tensorflow/tfjs-layers/dist/engine/base_random_layer.js","sources":["file:///Users/nishant/Documents/software/blog/node_modules/.pnpm/tfjs-layers/src/engine/base_random_layer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2023 CodeSmith LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport { LayerArgs, Layer } from './topology';\nimport { RandomSeed } from '../backend/random_seed';\nimport { serialization } from '@tensorflow/tfjs-core';\n\nexport declare interface BaseRandomLayerArgs extends LayerArgs {\n  seed?: number;\n}\n\nexport abstract class BaseRandomLayer extends Layer {\n  // A layer handle the random number creation and savemodel behavior.\n  /** @nocollapse */\n  static className = 'BaseRandomLayer';\n  protected randomGenerator: RandomSeed;\n\n  constructor(args: BaseRandomLayerArgs) {\n    super(args);\n    this.randomGenerator = new RandomSeed(args.seed);\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'seed': this.randomGenerator.seed\n    };\n\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n"],"names":[],"mappings":";;;;AAAA;;;;;;;;GAQG,CAEH,OAAO,EAAa,KAAK,EAAE,MAAM,YAAY,CAAC;AAC9C,OAAO,EAAE,UAAU,EAAE,MAAM,wBAAwB,CAAC;;;AAOpD,MAAsB,eAAgB,SAAQ,gTAAK;IAMjD,YAAY,IAAyB,CAAA;QACnC,KAAK,CAAC,IAAI,CAAC,CAAC;QACZ,IAAI,CAAC,eAAe,GAAG,IAAI,yTAAU,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;IACnD,CAAC;IAEQ,SAAS,GAAA;QAChB,MAAM,MAAM,GAA6B;YACvC,MAAM,EAAE,IAAI,CAAC,eAAe,CAAC,IAAI;SAClC,CAAC;QAEF,MAAM,UAAU,GAAG,KAAK,CAAC,SAAS,EAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;;AAlBD,oEAAoE;AACpE,gBAAA,EAAkB,CACX,gBAAA,SAAS,GAAG,iBAAiB,CAAC"}}]
}